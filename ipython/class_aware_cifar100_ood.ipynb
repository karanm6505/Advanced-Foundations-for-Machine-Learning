{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec7ea42",
   "metadata": {},
   "source": [
    "# Class-Aware OOD with CIFAR-100\n",
    "This walkthrough configures MetaLoRA for class-aware sampling on CIFAR-100 and evaluates out-of-distribution robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84dabe1",
   "metadata": {},
   "source": [
    "- Configure paths, dependencies, and experiment settings.\n",
    "3- Train on all CIFAR-100 classes with a class-aware sampler.\n",
    "- Use SVHN as a truly OOD benchmark while reusing the CIFAR-trained head.\n",
    "- Report both in-distribution accuracy and SVHN OOD metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78698751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100 as TorchvisionCIFAR100, SVHN as TorchvisionSVHN\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path, marker: str = \"main.py\", max_depth: int = 10) -> Path:\n",
    "    \"\"\"Locate the repository root when running locally or inside Colab.\"\"\"\n",
    "    env_root = os.environ.get(\"METALORA_ROOT\") or os.environ.get(\"REPO_ROOT\")\n",
    "    if env_root:\n",
    "        candidate = Path(env_root).expanduser().resolve()\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    current = start.resolve()\n",
    "    for _ in range(max_depth):\n",
    "        if (current / marker).exists():\n",
    "            return current\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    colab_candidate = Path(\"/content/metalora\").resolve()\n",
    "    if (colab_candidate / marker).exists():\n",
    "        return colab_candidate\n",
    "    if Path(\"/content\").exists():\n",
    "        print(\"Repository not found; attempting to clone into /content/metalora ...\")\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"https://github.com/doem97/metalora.git\",\n",
    "                str(colab_candidate),\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        if (colab_candidate / marker).exists():\n",
    "            return colab_candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate {marker}. Set METALORA_ROOT to the repo path or clone it under /content/metalora.\"\n",
    "    )\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "if \"datasets\" in sys.modules:\n",
    "    del sys.modules[\"datasets\"]\n",
    "os.chdir(REPO_ROOT)\n",
    "\n",
    "import datasets\n",
    "from trainer import (\n",
    "    CLASS_MEAN_FNAME,\n",
    "    TEXT_FEAT_FNAME,\n",
    "    Trainer,\n",
    "    load_clip_to_cpu,\n",
    "    load_vit_to_cpu,\n",
    "    )\n",
    "from models import PeftModelFromCLIP, PeftModelFromViT, ZeroShotCLIP\n",
    "from models.satmae_vit import MAEViTAdapter\n",
    "from utils.config_omega import cfg as base_cfg\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.logger import logger\n",
    "from utils.samplers import ClassAwareSampler, DownSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc52e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_config(repo_root, dataset_name, model_name, tuner_name=None, overrides=None):\n",
    "    config = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n",
    "    config_paths = [\n",
    "        repo_root / \"configs\" / \"data\" / f\"{dataset_name}.yaml\",\n",
    "        repo_root / \"configs\" / \"model\" / f\"{model_name}.yaml\",\n",
    "    ]\n",
    "    if tuner_name:\n",
    "        config_paths.append(repo_root / \"configs\" / \"tuner\" / f\"{tuner_name}.yaml\")\n",
    "    for path in config_paths:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(path)\n",
    "        config = OmegaConf.merge(config, OmegaConf.load(path))\n",
    "    if overrides:\n",
    "        config = OmegaConf.merge(config, OmegaConf.create(overrides))\n",
    "    return config\n",
    "\n",
    "\n",
    "def make_cifar_subset(dataset_cls, root, train, transform, class_indices, remap=True):\n",
    "    keep = sorted(class_indices)\n",
    "    dataset = dataset_cls(root, train=train, transform=transform)\n",
    "    targets = list(dataset.targets)\n",
    "    indices = [idx for idx, label in enumerate(targets) if label in keep]\n",
    "    if len(indices) == 0:\n",
    "        raise ValueError(\"No samples found for the provided classes.\")\n",
    "    index_array = np.array(indices, dtype=np.int64)\n",
    "    dataset.data = dataset.data[index_array]\n",
    "    selected_targets = [targets[idx] for idx in indices]\n",
    "    dataset.original_targets = selected_targets.copy()\n",
    "    subset_classnames = [dataset.classes[idx] for idx in keep]\n",
    "    if remap:\n",
    "        label_map = {orig: new_idx for new_idx, orig in enumerate(keep)}\n",
    "        remapped_targets = [label_map[label] for label in selected_targets]\n",
    "        dataset.targets = remapped_targets\n",
    "        dataset.labels = remapped_targets\n",
    "        dataset.classes = subset_classnames\n",
    "        dataset.classnames = subset_classnames\n",
    "        dataset.class_to_idx = {name: idx for idx, name in enumerate(subset_classnames)}\n",
    "        dataset.label_map = label_map\n",
    "        dataset.inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    else:\n",
    "        dataset.targets = selected_targets\n",
    "        dataset.labels = selected_targets\n",
    "        dataset.classnames = subset_classnames\n",
    "        dataset.label_map = None\n",
    "        dataset.inverse_label_map = None\n",
    "    if hasattr(dataset, \"get_cls_num_list\"):\n",
    "        dataset.cls_num_list = dataset.get_cls_num_list()\n",
    "        dataset.num_classes = len(dataset.cls_num_list)\n",
    "    else:\n",
    "        dataset.num_classes = len(subset_classnames)\n",
    "        dataset.cls_num_list = [dataset.targets.count(i) for i in range(dataset.num_classes)]\n",
    "    dataset.keep_classes = keep\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_ood_metrics(id_scores, ood_scores, tpr=0.95):\n",
    "    id_scores = np.asarray(id_scores, dtype=np.float32)\n",
    "    ood_scores = np.asarray(ood_scores, dtype=np.float32)\n",
    "    if id_scores.size == 0 or ood_scores.size == 0:\n",
    "        raise ValueError(\"Need non-empty ID and OOD score arrays.\")\n",
    "    labels = np.concatenate([np.ones_like(id_scores), np.zeros_like(ood_scores)])\n",
    "    scores = np.concatenate([id_scores, ood_scores])\n",
    "    threshold = np.percentile(id_scores, (1.0 - tpr) * 100.0)\n",
    "    metrics = {\n",
    "        \"auroc\": float(roc_auc_score(labels, scores)),\n",
    "        \"aupr\": float(average_precision_score(labels, scores)),\n",
    "        \"fpr@95tpr\": float(np.mean(ood_scores >= threshold)),\n",
    "        \"threshold@95tpr\": float(threshold),\n",
    "        \"id_mean\": float(id_scores.mean()),\n",
    "        \"id_std\": float(id_scores.std()),\n",
    "        \"ood_mean\": float(ood_scores.mean()),\n",
    "        \"ood_std\": float(ood_scores.std()),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788c16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAwareOODTrainer(Trainer):\n",
    "    def __init__(self, cfg, device, id_classes, ood_classes=None, class_aware_k=4):\n",
    "        self.id_classes = sorted(set(id_classes))\n",
    "        self.ood_classes = sorted(set(ood_classes or []))\n",
    "        self.external_ood_name = getattr(cfg, \"ood_dataset\", None)\n",
    "        self.external_ood_name = (\n",
    "            self.external_ood_name.lower() if self.external_ood_name else None\n",
    "        )\n",
    "        overlap = set(self.id_classes) & set(self.ood_classes)\n",
    "        if overlap and not self.external_ood_name:\n",
    "            raise ValueError(\n",
    "                f\"In-distribution and OOD classes overlap: {sorted(overlap)}\"\n",
    "            )\n",
    "        self.class_aware_k = class_aware_k\n",
    "        super().__init__(cfg, device)\n",
    "        self.local_rank = 0\n",
    "        self.world_size = 1\n",
    "        self.ood_test_loader = None\n",
    "        root_hint = Path(cfg.root or os.environ.get(\"CIFAR100_ROOT\", \"./data\")).expanduser()\n",
    "        class_names = getattr(TorchvisionCIFAR100, \"classes\", None)\n",
    "        if class_names is None:\n",
    "            preview_dataset = TorchvisionCIFAR100(\n",
    "                root=str(root_hint), train=True, download=True\n",
    "            )\n",
    "            class_names = preview_dataset.classes\n",
    "        self.global_classnames = class_names\n",
    "        self.id_classnames = [class_names[idx] for idx in self.id_classes]\n",
    "        if self.external_ood_name:\n",
    "            self.ood_classnames = None\n",
    "        else:\n",
    "            self.ood_classnames = [class_names[idx] for idx in self.ood_classes]\n",
    "        self.last_ood_scores = None\n",
    "\n",
    "    def build_data_loader(self):\n",
    "        cfg = self.cfg\n",
    "        root = cfg.root\n",
    "        resolution = cfg.resolution\n",
    "\n",
    "        if cfg.backbone.startswith(\"CLIP\"):\n",
    "            mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "            std = [0.26862954, 0.26130258, 0.27577711]\n",
    "        else:\n",
    "            mean = [0.5, 0.5, 0.5]\n",
    "            std = [0.5, 0.5, 0.5]\n",
    "\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(resolution),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        transform_plain = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resolution),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resolution * 8 // 7),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.Lambda(\n",
    "                    lambda crop: torch.stack([transforms.ToTensor()(crop)])\n",
    "                ),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        dataset_cls = getattr(datasets, cfg.dataset)\n",
    "        train_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_train, self.id_classes, remap=True\n",
    "        )\n",
    "        train_init_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_plain, self.id_classes, remap=True\n",
    "        )\n",
    "        train_test_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_test, self.id_classes, remap=True\n",
    "        )\n",
    "        id_test_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, False, transform_test, self.id_classes, remap=True\n",
    "        )\n",
    "\n",
    "        if self.external_ood_name:\n",
    "            ood_test_dataset = self._build_external_ood_dataset(transform_test)\n",
    "        else:\n",
    "            if not self.ood_classes:\n",
    "                raise ValueError(\n",
    "                    \"No OOD classes specified and external OOD dataset not provided.\"\n",
    "                )\n",
    "            ood_test_dataset = make_cifar_subset(\n",
    "                dataset_cls, root, False, transform_test, self.ood_classes, remap=False\n",
    "            )\n",
    "\n",
    "        self.num_classes = train_dataset.num_classes\n",
    "        self.cls_num_list = train_dataset.cls_num_list\n",
    "        self.classnames = train_dataset.classnames\n",
    "\n",
    "        freq = np.array(self.cls_num_list)\n",
    "        self.many_idxs = np.where(freq > 100)[0]\n",
    "        self.med_idxs = np.where((freq >= 20) & (freq <= 100))[0]\n",
    "        self.few_idxs = np.where(freq < 20)[0]\n",
    "\n",
    "        if cfg.init_head == \"1_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=1)\n",
    "        elif cfg.init_head == \"10_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=10)\n",
    "        elif cfg.init_head == \"100_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=100)\n",
    "        else:\n",
    "            init_sampler = None\n",
    "\n",
    "        self.accum_step = cfg.accum_step or 1\n",
    "        self.eff_batch_size = cfg.batch_size\n",
    "        denom = self.accum_step * self.world_size\n",
    "        if self.eff_batch_size % denom != 0:\n",
    "            raise ValueError(\n",
    "                f\"batch_size ({cfg.batch_size}) must be divisible by accum_step ({self.accum_step}).\"\n",
    "            )\n",
    "        self.per_gpu_batch_size = self.eff_batch_size // denom\n",
    "\n",
    "        train_sampler = ClassAwareSampler(train_dataset, num_samples_cls=self.class_aware_k)\n",
    "\n",
    "        pin = self.device.type == \"cuda\"\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.per_gpu_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.train_init_loader = DataLoader(\n",
    "            train_init_dataset,\n",
    "            batch_size=min(64, len(train_init_dataset)),\n",
    "            sampler=init_sampler,\n",
    "            shuffle=init_sampler is None,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.train_test_loader = DataLoader(\n",
    "            train_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            id_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.ood_test_loader = DataLoader(\n",
    "            ood_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        ood_desc = (\n",
    "            f\"OOD dataset ({self.external_ood_name.upper()}): {len(ood_test_dataset)} samples\"\n",
    "            if self.external_ood_name\n",
    "            else f\"OOD samples: {len(ood_test_dataset)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train samples: {len(train_dataset)} | ID classes: {len(self.id_classes)} | {ood_desc}\"\n",
    "        )\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.classnames\n",
    "        num_classes = len(classnames)\n",
    "\n",
    "        if cfg.backbone.startswith(\"CLIP\"):\n",
    "            clip_model = load_clip_to_cpu(cfg.backbone, cfg.prec)\n",
    "            if cfg.zero_shot:\n",
    "                self.model = ZeroShotCLIP(clip_model)\n",
    "                self.model.to(self.device)\n",
    "                self.tuner = None\n",
    "                self.head = None\n",
    "                template = \"a photo of a {}.\"\n",
    "                prompts = self.get_tokenized_prompts(classnames, template)\n",
    "                self.model.init_text_features(prompts)\n",
    "                return\n",
    "            self.model = PeftModelFromCLIP(cfg, clip_model, num_classes)\n",
    "        elif cfg.backbone.startswith(\"IN21K-ViT\"):\n",
    "            vit_model = load_vit_to_cpu(cfg.backbone, cfg.prec)\n",
    "            self.model = PeftModelFromViT(cfg, vit_model, num_classes)\n",
    "        elif cfg.backbone.startswith(\"SatMAE-ViT\"):\n",
    "            vit_model = load_vit_to_cpu(cfg.backbone, cfg.prec)\n",
    "            self.model = PeftModelFromViT(cfg, vit_model, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {cfg.backbone}\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.tuner = getattr(self.model, \"tuner\", None)\n",
    "        self.head = getattr(self.model, \"head\", None)\n",
    "\n",
    "        if cfg.init_head == \"text_feat\":\n",
    "            if not cfg.backbone.startswith(\"CLIP\"):\n",
    "                print(\"text_feat head init is only available for CLIP backbones.\")\n",
    "            else:\n",
    "                text_feat_fname = TEXT_FEAT_FNAME.get(cfg.backbone)\n",
    "                if text_feat_fname is None:\n",
    "                    raise ValueError(\n",
    "                        f\"No text feature file registered for {cfg.backbone}\"\n",
    "                    )\n",
    "                if cfg.head_init_folder is None:\n",
    "                    raise ValueError(\n",
    "                        \"head_init_folder must be set for text feature initialization.\"\n",
    "                    )\n",
    "                text_feat_path = os.path.join(cfg.head_init_folder, text_feat_fname)\n",
    "                self.init_head_text_feat(text_feat_path)\n",
    "        elif cfg.init_head in [\"class_mean\", \"1_shot\", \"10_shot\", \"100_shot\"]:\n",
    "            class_mean_fname = CLASS_MEAN_FNAME.get(cfg.backbone)\n",
    "            if class_mean_fname is None:\n",
    "                raise ValueError(\n",
    "                    f\"No class mean file registered for {cfg.backbone}\"\n",
    "                )\n",
    "            if cfg.head_init_folder is None:\n",
    "                raise ValueError(\n",
    "                    \"head_init_folder must be set for class mean initialization.\"\n",
    "                )\n",
    "            class_mean_path = os.path.join(cfg.head_init_folder, class_mean_fname)\n",
    "            self.init_head_class_mean(class_mean_path)\n",
    "        elif cfg.init_head == \"linear_probe\":\n",
    "            self.init_head_linear_probe()\n",
    "\n",
    "        if not (cfg.zero_shot or cfg.test_train or cfg.test_only):\n",
    "            self.build_optimizer()\n",
    "            self.build_criterion()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    def _build_external_ood_dataset(self, transform):\n",
    "        dataset_name = self.external_ood_name\n",
    "        if dataset_name == \"svhn\":\n",
    "            root = getattr(self.cfg, \"ood_root\", None)\n",
    "            if root is None:\n",
    "                root = Path(self.cfg.root).expanduser() / \"svhn\"\n",
    "            root = Path(root).expanduser()\n",
    "            split = getattr(self.cfg, \"ood_split\", \"test\")\n",
    "            return TorchvisionSVHN(\n",
    "                root=str(root),\n",
    "                split=split,\n",
    "                download=True,\n",
    "                transform=transform,\n",
    "            )\n",
    "        raise ValueError(\n",
    "            f\"Unsupported external OOD dataset: {self.external_ood_name}\"\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_ood(self):\n",
    "        if self.ood_test_loader is None:\n",
    "            raise RuntimeError(\"OOD loader not initialized.\")\n",
    "        self.model.eval()\n",
    "        if self.tuner is not None:\n",
    "            self.tuner.eval()\n",
    "        if self.head is not None:\n",
    "            self.head.eval()\n",
    "\n",
    "        amp_enabled = self.cfg.prec == \"amp\" and self.device.type == \"cuda\"\n",
    "\n",
    "        def collect_scores(loader):\n",
    "            scores = []\n",
    "            for images, _ in loader:\n",
    "                images = images.to(self.device)\n",
    "                batch_size, ncrops, c, h, w = images.size()\n",
    "                images = images.view(batch_size * ncrops, c, h, w)\n",
    "                with autocast(enabled=amp_enabled):\n",
    "                    logits = self.model(images)\n",
    "                logits = logits.view(batch_size, ncrops, -1).mean(dim=1)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                scores.extend(probs.max(dim=1)[0].cpu().numpy())\n",
    "            return scores\n",
    "\n",
    "        id_scores = collect_scores(self.test_loader)\n",
    "        ood_scores = collect_scores(self.ood_test_loader)\n",
    "        metrics = compute_ood_metrics(id_scores, ood_scores)\n",
    "        self.last_ood_scores = {\"id\": id_scores, \"ood\": ood_scores, \"metrics\": metrics}\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34450219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mTraining on all 100 CIFAR-100 classes (first 10): ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle']\u001b[0m\n",
      "\u001b[37mUsing SVHN at /content/metalora/data/svhn as the OOD dataset.\u001b[0m\n",
      "\u001b[37mUsing SVHN at /content/metalora/data/svhn as the OOD dataset.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'CIFAR100', 'root': '/content/metalora/data', 'imb_factor': None, 'head_init_folder': '/content/metalora/output/notebooks/cifar100_class_aware_svhn', 'backbone': 'CLIP-ViT-B/16', 'resolution': 224, 'output_dir': '/content/metalora/output/notebooks/cifar100_class_aware_svhn', 'print_freq': 20, 'seed': 0, 'deterministic': True, 'num_workers': 2, 'prec': 'amp', 'num_epochs': 5, 'batch_size': 64, 'accum_step': 1, 'lr': 0.01, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 0.0005, 'momentum': 0.9, 'loss_type': 'CE', 'classifier': 'CosineClassifier', 'scale': 25, 'fine_tuning': False, 'head_only': False, 'full_tuning': False, 'bias_tuning': False, 'ln_tuning': False, 'bn_tuning': False, 'vpt_shallow': False, 'vpt_deep': False, 'adapter': False, 'adaptformer': False, 'lora': False, 'lora_mlp': False, 'scale_alpha': 1, 'ssf_attn': False, 'ssf_mlp': False, 'ssf_ln': False, 'mask': False, 'partial': None, 'vpt_len': None, 'adapter_dim': None, 'adaptformer_scale': 'learnable', 'mask_ratio': None, 'mask_seed': None, 'init_head': 'text_feat', 'prompt': 'default', 'tte': False, 'expand': 24, 'tte_mode': 'fivecrop', 'randaug_times': 1, 'zero_shot': False, 'test_only': False, 'test_train': False, 'model_dir': None, 'use_flora': False, 'flora': {'arch': {'modules': ['q', 'v', 'k', 'out', 'mlp1', 'mlp2'], 'layers': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'rank': 4, 'alpha': None}, 'optimizer': {'default_lr': 0.02, 'lr_config': {'layers': {}, 'modules': {}, 'specific': {}}}}, 'use_meta': True, 'meta_data_ratio': 0.1, 'meta_lr': 0.001, 'meta_update_freq': 1, 'meta_inner_steps': 5, 'ood_dataset': 'svhn', 'ood_root': '/content/metalora/data/svhn', 'ood_split': 'test'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_CLASSES = list(range(100))\n",
    "\n",
    "default_data_root = Path(\n",
    "    os.environ.get(\"CIFAR100_ROOT\") or (REPO_ROOT / \"data\")\n",
    ").expanduser().resolve()\n",
    "svhn_root = Path(\n",
    "    os.environ.get(\"SVHN_ROOT\") or (default_data_root / \"svhn\")\n",
    ").expanduser().resolve()\n",
    "\n",
    "class_names = getattr(TorchvisionCIFAR100, \"classes\", None)\n",
    "if class_names is None:\n",
    "    preview_dataset = TorchvisionCIFAR100(\n",
    "        root=str(default_data_root), train=True, download=True\n",
    "    )\n",
    "    class_names = preview_dataset.classes\n",
    "\n",
    "id_names_preview = [class_names[idx] for idx in ID_CLASSES[:10]]\n",
    "print(\n",
    "    f\"Training on all {len(ID_CLASSES)} CIFAR-100 classes (first 10): {id_names_preview}\"\n",
    ")\n",
    "print(f\"Using SVHN at {svhn_root} as the OOD dataset.\")\n",
    "\n",
    "cfg = load_experiment_config(\n",
    "    REPO_ROOT,\n",
    "    dataset_name=\"cifar100\",\n",
    "    model_name=\"clip_vit_b16\",\n",
    "    tuner_name=None,\n",
    " )\n",
    "\n",
    "cfg.use_meta = True\n",
    "cfg.output_dir = str(REPO_ROOT / \"output\" / \"notebooks\" / \"cifar100_class_aware_svhn\")\n",
    "cfg.root = str(default_data_root)\n",
    "cfg.num_epochs = 5\n",
    "cfg.batch_size = 64\n",
    "cfg.accum_step = 1\n",
    "cfg.loss_type = \"CE\"\n",
    "cfg.head_only = False\n",
    "cfg.init_head = \"text_feat\"\n",
    "cfg.tte = False\n",
    "cfg.lr = 0.01\n",
    "cfg.print_freq = 20\n",
    "cfg.seed = 0\n",
    "cfg.deterministic = True\n",
    "cfg.num_workers = min(4, os.cpu_count() or 4)\n",
    "cfg.prec = \"amp\" if device.type == \"cuda\" else \"fp32\"\n",
    "cfg.head_init_folder = cfg.output_dir\n",
    "cfg.ood_dataset = \"svhn\"\n",
    "cfg.ood_root = str(svhn_root)\n",
    "cfg.ood_split = \"test\"\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "if cfg.seed is not None:\n",
    "    random.seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(cfg.seed)\n",
    "        torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "if cfg.deterministic and torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "logger.init(cfg.output_dir)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c8d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mTrain samples: 50000 | ID classes: 100 | OOD dataset (SVHN): 26032 samples\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mLoading text features from /content/metalora/output/notebooks/cifar100_class_aware_svhn/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mTuner mode: Only tuning the tuner and head\u001b[0m\n",
      "\u001b[37mTurning off gradients in the model\u001b[0m\n",
      "\u001b[37mTurning on gradients in the tuner and head\u001b[0m\n",
      "\u001b[37mLoading text features from /content/metalora/output/notebooks/cifar100_class_aware_svhn/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mTuner mode: Only tuning the tuner and head\u001b[0m\n",
      "\u001b[37mTurning off gradients in the model\u001b[0m\n",
      "\u001b[37mTurning on gradients in the tuner and head\u001b[0m\n",
      "\u001b[37mTotal params: 149697536\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37mHead params: 76800\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:453: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if cfg.prec == \"amp\" else None\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 50000\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37mTotal params: 149697536\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37mHead params: 76800\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:453: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if cfg.prec == \"amp\" else None\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 50000\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_class_aware_svhn/tensorboard)\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_class_aware_svhn/tensorboard)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = ClassAwareOODTrainer(\n",
    "    cfg, device, ID_CLASSES, ood_classes=None, class_aware_k=4\n",
    ")\n",
    "trainer.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa29caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\n",
      "================================================================================\n",
      "                                 Training model                                 \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/782] time 0.294 (0.317) data 0.000 (0.022) loss 3.3974 (3.7254) acc 39.0625 (29.0672) (mean 28.7039 many 28.7039 med nan few nan) lr 1.0000e-02 elapsed 0:00:06 eta 0:20:32\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/782] time 0.294 (0.317) data 0.000 (0.022) loss 3.3974 (3.7254) acc 39.0625 (29.0672) (mean 28.7039 many 28.7039 med nan few nan) lr 1.0000e-02 elapsed 0:00:06 eta 0:20:32\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/782] time 0.683 (0.343) data 0.000 (0.011) loss 3.2723 (3.4661) acc 42.1875 (31.6744) (mean 31.5586 many 31.5586 med nan few nan) lr 1.0000e-02 elapsed 0:00:13 eta 0:22:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/782] time 0.683 (0.343) data 0.000 (0.011) loss 3.2723 (3.4661) acc 42.1875 (31.6744) (mean 31.5586 many 31.5586 med nan few nan) lr 1.0000e-02 elapsed 0:00:13 eta 0:22:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/782] time 0.299 (0.357) data 0.000 (0.007) loss 3.2230 (3.2456) acc 29.6875 (31.3642) (mean 32.6315 many 32.6315 med nan few nan) lr 1.0000e-02 elapsed 0:00:21 eta 0:22:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/782] time 0.299 (0.357) data 0.000 (0.007) loss 3.2230 (3.2456) acc 29.6875 (31.3642) (mean 32.6315 many 32.6315 med nan few nan) lr 1.0000e-02 elapsed 0:00:21 eta 0:22:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/782] time 0.284 (0.341) data 0.000 (0.006) loss 3.1201 (3.0376) acc 26.5625 (34.6765) (mean 35.4723 many 35.4723 med nan few nan) lr 1.0000e-02 elapsed 0:00:27 eta 0:21:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/782] time 0.284 (0.341) data 0.000 (0.006) loss 3.1201 (3.0376) acc 26.5625 (34.6765) (mean 35.4723 many 35.4723 med nan few nan) lr 1.0000e-02 elapsed 0:00:27 eta 0:21:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/782] time 0.283 (0.330) data 0.000 (0.005) loss 3.0403 (2.9447) acc 39.0625 (36.8742) (mean 37.3809 many 37.3809 med nan few nan) lr 1.0000e-02 elapsed 0:00:32 eta 0:20:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/782] time 0.283 (0.330) data 0.000 (0.005) loss 3.0403 (2.9447) acc 39.0625 (36.8742) (mean 37.3809 many 37.3809 med nan few nan) lr 1.0000e-02 elapsed 0:00:32 eta 0:20:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/782] time 0.279 (0.321) data 0.000 (0.004) loss 2.7926 (2.8563) acc 37.5000 (36.8292) (mean 38.4648 many 38.4648 med nan few nan) lr 1.0000e-02 elapsed 0:00:38 eta 0:20:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/782] time 0.279 (0.321) data 0.000 (0.004) loss 2.7926 (2.8563) acc 37.5000 (36.8292) (mean 38.4648 many 38.4648 med nan few nan) lr 1.0000e-02 elapsed 0:00:38 eta 0:20:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/782] time 0.262 (0.314) data 0.000 (0.004) loss 2.8684 (2.6549) acc 35.9375 (41.2268) (mean 41.4154 many 41.4154 med nan few nan) lr 1.0000e-02 elapsed 0:00:43 eta 0:19:45\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/782] time 0.262 (0.314) data 0.000 (0.004) loss 2.8684 (2.6549) acc 35.9375 (41.2268) (mean 41.4154 many 41.4154 med nan few nan) lr 1.0000e-02 elapsed 0:00:43 eta 0:19:45\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/782] time 0.268 (0.309) data 0.000 (0.003) loss 2.3594 (2.6059) acc 50.0000 (40.7807) (mean 39.6797 many 39.6797 med nan few nan) lr 1.0000e-02 elapsed 0:00:49 eta 0:19:18\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/782] time 0.268 (0.309) data 0.000 (0.003) loss 2.3594 (2.6059) acc 50.0000 (40.7807) (mean 39.6797 many 39.6797 med nan few nan) lr 1.0000e-02 elapsed 0:00:49 eta 0:19:18\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/782] time 0.266 (0.305) data 0.000 (0.003) loss 2.4024 (2.5104) acc 40.6250 (42.7594) (mean 42.0789 many 42.0789 med nan few nan) lr 1.0000e-02 elapsed 0:00:54 eta 0:18:56\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/782] time 0.266 (0.305) data 0.000 (0.003) loss 2.4024 (2.5104) acc 40.6250 (42.7594) (mean 42.0789 many 42.0789 med nan few nan) lr 1.0000e-02 elapsed 0:00:54 eta 0:18:56\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/782] time 0.270 (0.301) data 0.000 (0.003) loss 2.6293 (2.5702) acc 39.0625 (41.3276) (mean 42.3902 many 42.3902 med nan few nan) lr 1.0000e-02 elapsed 0:01:00 eta 0:18:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/782] time 0.270 (0.301) data 0.000 (0.003) loss 2.6293 (2.5702) acc 39.0625 (41.3276) (mean 42.3902 many 42.3902 med nan few nan) lr 1.0000e-02 elapsed 0:01:00 eta 0:18:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/782] time 0.272 (0.298) data 0.002 (0.002) loss 2.8317 (2.5390) acc 34.3750 (41.8802) (mean 42.8711 many 42.8711 med nan few nan) lr 1.0000e-02 elapsed 0:01:05 eta 0:18:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/782] time 0.272 (0.298) data 0.002 (0.002) loss 2.8317 (2.5390) acc 34.3750 (41.8802) (mean 42.8711 many 42.8711 med nan few nan) lr 1.0000e-02 elapsed 0:01:05 eta 0:18:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/782] time 0.271 (0.296) data 0.000 (0.002) loss 2.4856 (2.4036) acc 48.4375 (45.6796) (mean 45.6374 many 45.6374 med nan few nan) lr 1.0000e-02 elapsed 0:01:11 eta 0:18:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/782] time 0.271 (0.296) data 0.000 (0.002) loss 2.4856 (2.4036) acc 48.4375 (45.6796) (mean 45.6374 many 45.6374 med nan few nan) lr 1.0000e-02 elapsed 0:01:11 eta 0:18:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/782] time 0.279 (0.294) data 0.000 (0.002) loss 2.5494 (2.3318) acc 40.6250 (46.3850) (mean 45.3522 many 45.3522 med nan few nan) lr 1.0000e-02 elapsed 0:01:16 eta 0:17:54\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/782] time 0.279 (0.294) data 0.000 (0.002) loss 2.5494 (2.3318) acc 40.6250 (46.3850) (mean 45.3522 many 45.3522 med nan few nan) lr 1.0000e-02 elapsed 0:01:16 eta 0:17:54\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/782] time 0.286 (0.293) data 0.000 (0.002) loss 2.4843 (2.4191) acc 43.7500 (42.6993) (mean 43.9881 many 43.9881 med nan few nan) lr 1.0000e-02 elapsed 0:01:22 eta 0:17:44\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/782] time 0.286 (0.293) data 0.000 (0.002) loss 2.4843 (2.4191) acc 43.7500 (42.6993) (mean 43.9881 many 43.9881 med nan few nan) lr 1.0000e-02 elapsed 0:01:22 eta 0:17:44\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/782] time 0.278 (0.293) data 0.000 (0.002) loss 2.8344 (2.3954) acc 29.6875 (44.4354) (mean 45.1131 many 45.1131 med nan few nan) lr 1.0000e-02 elapsed 0:01:27 eta 0:17:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/782] time 0.278 (0.293) data 0.000 (0.002) loss 2.8344 (2.3954) acc 29.6875 (44.4354) (mean 45.1131 many 45.1131 med nan few nan) lr 1.0000e-02 elapsed 0:01:27 eta 0:17:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/782] time 0.284 (0.292) data 0.000 (0.002) loss 2.4506 (2.3678) acc 40.6250 (43.2159) (mean 44.7117 many 44.7117 med nan few nan) lr 1.0000e-02 elapsed 0:01:33 eta 0:17:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/782] time 0.284 (0.292) data 0.000 (0.002) loss 2.4506 (2.3678) acc 40.6250 (43.2159) (mean 44.7117 many 44.7117 med nan few nan) lr 1.0000e-02 elapsed 0:01:33 eta 0:17:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/782] time 0.288 (0.292) data 0.002 (0.002) loss 2.3487 (2.3106) acc 48.4375 (48.2206) (mean 47.8118 many 47.8118 med nan few nan) lr 1.0000e-02 elapsed 0:01:39 eta 0:17:23\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/782] time 0.288 (0.292) data 0.002 (0.002) loss 2.3487 (2.3106) acc 48.4375 (48.2206) (mean 47.8118 many 47.8118 med nan few nan) lr 1.0000e-02 elapsed 0:01:39 eta 0:17:23\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/782] time 0.285 (0.292) data 0.000 (0.002) loss 1.8831 (2.2659) acc 60.9375 (48.4303) (mean 47.9963 many 47.9963 med nan few nan) lr 1.0000e-02 elapsed 0:01:45 eta 0:17:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/782] time 0.285 (0.292) data 0.000 (0.002) loss 1.8831 (2.2659) acc 60.9375 (48.4303) (mean 47.9963 many 47.9963 med nan few nan) lr 1.0000e-02 elapsed 0:01:45 eta 0:17:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/782] time 0.282 (0.292) data 0.000 (0.002) loss 2.6277 (2.2279) acc 37.5000 (47.5420) (mean 48.1381 many 48.1381 med nan few nan) lr 1.0000e-02 elapsed 0:01:50 eta 0:17:09\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/782] time 0.282 (0.292) data 0.000 (0.002) loss 2.6277 (2.2279) acc 37.5000 (47.5420) (mean 48.1381 many 48.1381 med nan few nan) lr 1.0000e-02 elapsed 0:01:50 eta 0:17:09\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/782] time 0.287 (0.291) data 0.002 (0.002) loss 2.1543 (2.2327) acc 57.8125 (47.7550) (mean 47.7658 many 47.7658 med nan few nan) lr 1.0000e-02 elapsed 0:01:56 eta 0:17:02\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/782] time 0.287 (0.291) data 0.002 (0.002) loss 2.1543 (2.2327) acc 57.8125 (47.7550) (mean 47.7658 many 47.7658 med nan few nan) lr 1.0000e-02 elapsed 0:01:56 eta 0:17:02\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/782] time 0.277 (0.291) data 0.000 (0.002) loss 2.0007 (2.2096) acc 56.2500 (47.2850) (mean 47.4943 many 47.4943 med nan few nan) lr 1.0000e-02 elapsed 0:02:02 eta 0:16:54\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/782] time 0.277 (0.291) data 0.000 (0.002) loss 2.0007 (2.2096) acc 56.2500 (47.2850) (mean 47.4943 many 47.4943 med nan few nan) lr 1.0000e-02 elapsed 0:02:02 eta 0:16:54\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/782] time 0.272 (0.290) data 0.000 (0.001) loss 2.1917 (2.2454) acc 48.4375 (47.6994) (mean 47.0483 many 47.0483 med nan few nan) lr 1.0000e-02 elapsed 0:02:07 eta 0:16:46\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/782] time 0.272 (0.290) data 0.000 (0.001) loss 2.1917 (2.2454) acc 48.4375 (47.6994) (mean 47.0483 many 47.0483 med nan few nan) lr 1.0000e-02 elapsed 0:02:07 eta 0:16:46\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/782] time 0.271 (0.289) data 0.000 (0.001) loss 2.7594 (2.2443) acc 32.8125 (46.6828) (mean 48.2474 many 48.2474 med nan few nan) lr 1.0000e-02 elapsed 0:02:13 eta 0:16:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/782] time 0.271 (0.289) data 0.000 (0.001) loss 2.7594 (2.2443) acc 32.8125 (46.6828) (mean 48.2474 many 48.2474 med nan few nan) lr 1.0000e-02 elapsed 0:02:13 eta 0:16:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/782] time 0.278 (0.289) data 0.000 (0.001) loss 2.2568 (2.2544) acc 46.8750 (46.7986) (mean 46.8852 many 46.8852 med nan few nan) lr 1.0000e-02 elapsed 0:02:18 eta 0:16:30\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/782] time 0.278 (0.289) data 0.000 (0.001) loss 2.2568 (2.2544) acc 46.8750 (46.7986) (mean 46.8852 many 46.8852 med nan few nan) lr 1.0000e-02 elapsed 0:02:18 eta 0:16:30\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/782] time 0.269 (0.288) data 0.000 (0.001) loss 2.2064 (2.2495) acc 42.1875 (45.8780) (mean 46.2696 many 46.2696 med nan few nan) lr 1.0000e-02 elapsed 0:02:24 eta 0:16:23\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/782] time 0.269 (0.288) data 0.000 (0.001) loss 2.2064 (2.2495) acc 42.1875 (45.8780) (mean 46.2696 many 46.2696 med nan few nan) lr 1.0000e-02 elapsed 0:02:24 eta 0:16:23\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/782] time 0.273 (0.288) data 0.000 (0.001) loss 2.0650 (2.2224) acc 56.2500 (47.9808) (mean 47.2730 many 47.2730 med nan few nan) lr 1.0000e-02 elapsed 0:02:29 eta 0:16:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/782] time 0.273 (0.288) data 0.000 (0.001) loss 2.0650 (2.2224) acc 56.2500 (47.9808) (mean 47.2730 many 47.2730 med nan few nan) lr 1.0000e-02 elapsed 0:02:29 eta 0:16:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/782] time 0.284 (0.288) data 0.000 (0.001) loss 2.3843 (2.1690) acc 45.3125 (47.8219) (mean 47.5176 many 47.5176 med nan few nan) lr 1.0000e-02 elapsed 0:02:35 eta 0:16:08\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/782] time 0.284 (0.288) data 0.000 (0.001) loss 2.3843 (2.1690) acc 45.3125 (47.8219) (mean 47.5176 many 47.5176 med nan few nan) lr 1.0000e-02 elapsed 0:02:35 eta 0:16:08\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/782] time 0.282 (0.287) data 0.000 (0.001) loss 2.6479 (2.2606) acc 35.9375 (45.0350) (mean 46.0904 many 46.0904 med nan few nan) lr 1.0000e-02 elapsed 0:02:40 eta 0:16:02\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/782] time 0.282 (0.287) data 0.000 (0.001) loss 2.6479 (2.2606) acc 35.9375 (45.0350) (mean 46.0904 many 46.0904 med nan few nan) lr 1.0000e-02 elapsed 0:02:40 eta 0:16:02\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/782] time 0.285 (0.287) data 0.000 (0.001) loss 2.1979 (2.2078) acc 45.3125 (46.2440) (mean 46.9473 many 46.9473 med nan few nan) lr 1.0000e-02 elapsed 0:02:46 eta 0:15:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/782] time 0.285 (0.287) data 0.000 (0.001) loss 2.1979 (2.2078) acc 45.3125 (46.2440) (mean 46.9473 many 46.9473 med nan few nan) lr 1.0000e-02 elapsed 0:02:46 eta 0:15:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/782] time 0.278 (0.287) data 0.000 (0.001) loss 2.1140 (2.2123) acc 54.6875 (46.1595) (mean 46.5684 many 46.5684 med nan few nan) lr 1.0000e-02 elapsed 0:02:52 eta 0:15:49\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/782] time 0.278 (0.287) data 0.000 (0.001) loss 2.1140 (2.2123) acc 54.6875 (46.1595) (mean 46.5684 many 46.5684 med nan few nan) lr 1.0000e-02 elapsed 0:02:52 eta 0:15:49\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/782] time 0.285 (0.287) data 0.000 (0.001) loss 1.5713 (2.1440) acc 64.0625 (48.6099) (mean 48.0717 many 48.0717 med nan few nan) lr 1.0000e-02 elapsed 0:02:57 eta 0:15:43\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/782] time 0.285 (0.287) data 0.000 (0.001) loss 1.5713 (2.1440) acc 64.0625 (48.6099) (mean 48.0717 many 48.0717 med nan few nan) lr 1.0000e-02 elapsed 0:02:57 eta 0:15:43\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [640/782] time 0.286 (0.287) data 0.000 (0.001) loss 2.3151 (2.1861) acc 45.3125 (48.3108) (mean 47.8420 many 47.8420 med nan few nan) lr 1.0000e-02 elapsed 0:03:03 eta 0:15:37\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [640/782] time 0.286 (0.287) data 0.000 (0.001) loss 2.3151 (2.1861) acc 45.3125 (48.3108) (mean 47.8420 many 47.8420 med nan few nan) lr 1.0000e-02 elapsed 0:03:03 eta 0:15:37\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [660/782] time 0.275 (0.287) data 0.000 (0.001) loss 1.9536 (2.1268) acc 53.1250 (50.2729) (mean 49.5318 many 49.5318 med nan few nan) lr 1.0000e-02 elapsed 0:03:09 eta 0:15:31\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [660/782] time 0.275 (0.287) data 0.000 (0.001) loss 1.9536 (2.1268) acc 53.1250 (50.2729) (mean 49.5318 many 49.5318 med nan few nan) lr 1.0000e-02 elapsed 0:03:09 eta 0:15:31\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [680/782] time 0.286 (0.286) data 0.000 (0.001) loss 2.3000 (2.1671) acc 35.9375 (46.6312) (mean 47.8443 many 47.8443 med nan few nan) lr 1.0000e-02 elapsed 0:03:14 eta 0:15:25\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [680/782] time 0.286 (0.286) data 0.000 (0.001) loss 2.3000 (2.1671) acc 35.9375 (46.6312) (mean 47.8443 many 47.8443 med nan few nan) lr 1.0000e-02 elapsed 0:03:14 eta 0:15:25\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [700/782] time 0.281 (0.286) data 0.000 (0.001) loss 1.8446 (2.2211) acc 62.5000 (45.0795) (mean 45.9955 many 45.9955 med nan few nan) lr 1.0000e-02 elapsed 0:03:20 eta 0:15:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [700/782] time 0.281 (0.286) data 0.000 (0.001) loss 1.8446 (2.2211) acc 62.5000 (45.0795) (mean 45.9955 many 45.9955 med nan few nan) lr 1.0000e-02 elapsed 0:03:20 eta 0:15:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [720/782] time 0.283 (0.286) data 0.005 (0.001) loss 2.1366 (2.1221) acc 50.0000 (49.5815) (mean 48.9136 many 48.9136 med nan few nan) lr 1.0000e-02 elapsed 0:03:26 eta 0:15:13\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [720/782] time 0.283 (0.286) data 0.005 (0.001) loss 2.1366 (2.1221) acc 50.0000 (49.5815) (mean 48.9136 many 48.9136 med nan few nan) lr 1.0000e-02 elapsed 0:03:26 eta 0:15:13\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [740/782] time 0.278 (0.286) data 0.000 (0.001) loss 2.1753 (2.1736) acc 48.4375 (47.9208) (mean 49.1168 many 49.1168 med nan few nan) lr 1.0000e-02 elapsed 0:03:31 eta 0:15:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [740/782] time 0.278 (0.286) data 0.000 (0.001) loss 2.1753 (2.1736) acc 48.4375 (47.9208) (mean 49.1168 many 49.1168 med nan few nan) lr 1.0000e-02 elapsed 0:03:31 eta 0:15:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [760/782] time 0.278 (0.286) data 0.000 (0.001) loss 1.8553 (2.1053) acc 53.1250 (48.5406) (mean 47.8928 many 47.8928 med nan few nan) lr 1.0000e-02 elapsed 0:03:37 eta 0:15:00\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [760/782] time 0.278 (0.286) data 0.000 (0.001) loss 1.8553 (2.1053) acc 53.1250 (48.5406) (mean 47.8928 many 47.8928 med nan few nan) lr 1.0000e-02 elapsed 0:03:37 eta 0:15:00\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [780/782] time 0.281 (0.286) data 0.000 (0.001) loss 2.0860 (2.1397) acc 48.4375 (47.9613) (mean 47.8315 many 47.8315 med nan few nan) lr 1.0000e-02 elapsed 0:03:42 eta 0:14:54\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [780/782] time 0.281 (0.286) data 0.000 (0.001) loss 2.0860 (2.1397) acc 48.4375 (47.9613) (mean 47.8315 many 47.8315 med nan few nan) lr 1.0000e-02 elapsed 0:03:42 eta 0:14:54\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [782/782] time 0.273 (0.286) data 0.000 (0.001) loss 2.2002 (2.1445) acc 50.0000 (47.5049) (mean 47.4967 many 47.4967 med nan few nan) lr 1.0000e-02 elapsed 0:03:43 eta 0:14:53\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [782/782] time 0.273 (0.286) data 0.000 (0.001) loss 2.2002 (2.1445) acc 50.0000 (47.5049) (mean 47.4967 many 47.4967 med nan few nan) lr 1.0000e-02 elapsed 0:03:43 eta 0:14:53\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/782] time 0.280 (0.286) data 0.000 (0.002) loss 2.1019 (2.1232) acc 40.6250 (46.9648) (mean 47.5711 many 47.5711 med nan few nan) lr 9.0451e-03 elapsed 0:03:49 eta 0:14:47\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/782] time 0.280 (0.286) data 0.000 (0.002) loss 2.1019 (2.1232) acc 40.6250 (46.9648) (mean 47.5711 many 47.5711 med nan few nan) lr 9.0451e-03 elapsed 0:03:49 eta 0:14:47\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/782] time 0.281 (0.286) data 0.000 (0.001) loss 1.8503 (2.0351) acc 60.9375 (50.3366) (mean 49.6463 many 49.6463 med nan few nan) lr 9.0451e-03 elapsed 0:03:54 eta 0:14:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/782] time 0.281 (0.286) data 0.000 (0.001) loss 1.8503 (2.0351) acc 60.9375 (50.3366) (mean 49.6463 many 49.6463 med nan few nan) lr 9.0451e-03 elapsed 0:03:54 eta 0:14:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/782] time 0.280 (0.285) data 0.000 (0.001) loss 2.3557 (2.0883) acc 45.3125 (50.0158) (mean 50.3877 many 50.3877 med nan few nan) lr 9.0451e-03 elapsed 0:04:00 eta 0:14:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/782] time 0.280 (0.285) data 0.000 (0.001) loss 2.3557 (2.0883) acc 45.3125 (50.0158) (mean 50.3877 many 50.3877 med nan few nan) lr 9.0451e-03 elapsed 0:04:00 eta 0:14:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/782] time 0.282 (0.285) data 0.000 (0.001) loss 1.9672 (2.0320) acc 57.8125 (51.8333) (mean 51.7399 many 51.7399 med nan few nan) lr 9.0451e-03 elapsed 0:04:06 eta 0:14:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/782] time 0.282 (0.285) data 0.000 (0.001) loss 1.9672 (2.0320) acc 57.8125 (51.8333) (mean 51.7399 many 51.7399 med nan few nan) lr 9.0451e-03 elapsed 0:04:06 eta 0:14:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/782] time 0.284 (0.285) data 0.000 (0.001) loss 2.4227 (2.1414) acc 34.3750 (48.1584) (mean 49.6207 many 49.6207 med nan few nan) lr 9.0451e-03 elapsed 0:04:11 eta 0:14:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/782] time 0.284 (0.285) data 0.000 (0.001) loss 2.4227 (2.1414) acc 34.3750 (48.1584) (mean 49.6207 many 49.6207 med nan few nan) lr 9.0451e-03 elapsed 0:04:11 eta 0:14:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/782] time 0.282 (0.285) data 0.000 (0.001) loss 2.1497 (2.1161) acc 40.6250 (48.9557) (mean 49.4891 many 49.4891 med nan few nan) lr 9.0451e-03 elapsed 0:04:17 eta 0:14:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/782] time 0.282 (0.285) data 0.000 (0.001) loss 2.1497 (2.1161) acc 40.6250 (48.9557) (mean 49.4891 many 49.4891 med nan few nan) lr 9.0451e-03 elapsed 0:04:17 eta 0:14:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/782] time 0.284 (0.285) data 0.000 (0.001) loss 1.7352 (1.9949) acc 59.3750 (50.2026) (mean 49.8723 many 49.8723 med nan few nan) lr 9.0451e-03 elapsed 0:04:23 eta 0:14:12\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/782] time 0.284 (0.285) data 0.000 (0.001) loss 1.7352 (1.9949) acc 59.3750 (50.2026) (mean 49.8723 many 49.8723 med nan few nan) lr 9.0451e-03 elapsed 0:04:23 eta 0:14:12\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/782] time 0.287 (0.285) data 0.000 (0.001) loss 1.9152 (2.0754) acc 57.8125 (49.7222) (mean 49.3803 many 49.3803 med nan few nan) lr 9.0451e-03 elapsed 0:04:28 eta 0:14:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/782] time 0.287 (0.285) data 0.000 (0.001) loss 1.9152 (2.0754) acc 57.8125 (49.7222) (mean 49.3803 many 49.3803 med nan few nan) lr 9.0451e-03 elapsed 0:04:28 eta 0:14:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/782] time 0.281 (0.285) data 0.000 (0.001) loss 1.8382 (2.0126) acc 57.8125 (51.3128) (mean 50.4585 many 50.4585 med nan few nan) lr 9.0451e-03 elapsed 0:04:34 eta 0:14:00\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/782] time 0.281 (0.285) data 0.000 (0.001) loss 1.8382 (2.0126) acc 57.8125 (51.3128) (mean 50.4585 many 50.4585 med nan few nan) lr 9.0451e-03 elapsed 0:04:34 eta 0:14:00\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/782] time 0.289 (0.285) data 0.000 (0.001) loss 2.0812 (2.1233) acc 48.4375 (48.6836) (mean 49.4290 many 49.4290 med nan few nan) lr 9.0451e-03 elapsed 0:04:39 eta 0:13:54\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/782] time 0.289 (0.285) data 0.000 (0.001) loss 2.0812 (2.1233) acc 48.4375 (48.6836) (mean 49.4290 many 49.4290 med nan few nan) lr 9.0451e-03 elapsed 0:04:39 eta 0:13:54\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/782] time 0.279 (0.285) data 0.000 (0.001) loss 2.0346 (2.0890) acc 51.5625 (49.8559) (mean 50.2258 many 50.2258 med nan few nan) lr 9.0451e-03 elapsed 0:04:45 eta 0:13:48\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/782] time 0.279 (0.285) data 0.000 (0.001) loss 2.0346 (2.0890) acc 51.5625 (49.8559) (mean 50.2258 many 50.2258 med nan few nan) lr 9.0451e-03 elapsed 0:04:45 eta 0:13:48\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/782] time 0.281 (0.285) data 0.000 (0.001) loss 1.8140 (2.0443) acc 56.2500 (51.8668) (mean 51.9542 many 51.9542 med nan few nan) lr 9.0451e-03 elapsed 0:04:51 eta 0:13:42\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/782] time 0.281 (0.285) data 0.000 (0.001) loss 1.8140 (2.0443) acc 56.2500 (51.8668) (mean 51.9542 many 51.9542 med nan few nan) lr 9.0451e-03 elapsed 0:04:51 eta 0:13:42\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/782] time 0.281 (0.285) data 0.000 (0.001) loss 2.3709 (2.0386) acc 35.9375 (52.1702) (mean 52.4392 many 52.4392 med nan few nan) lr 9.0451e-03 elapsed 0:04:56 eta 0:13:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/782] time 0.281 (0.285) data 0.000 (0.001) loss 2.3709 (2.0386) acc 35.9375 (52.1702) (mean 52.4392 many 52.4392 med nan few nan) lr 9.0451e-03 elapsed 0:04:56 eta 0:13:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/782] time 0.281 (0.285) data 0.000 (0.001) loss 2.0554 (2.0503) acc 48.4375 (49.3862) (mean 49.7027 many 49.7027 med nan few nan) lr 9.0451e-03 elapsed 0:05:02 eta 0:13:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/782] time 0.281 (0.285) data 0.000 (0.001) loss 2.0554 (2.0503) acc 48.4375 (49.3862) (mean 49.7027 many 49.7027 med nan few nan) lr 9.0451e-03 elapsed 0:05:02 eta 0:13:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/782] time 0.280 (0.285) data 0.000 (0.001) loss 2.2110 (2.0985) acc 40.6250 (49.1098) (mean 49.9413 many 49.9413 med nan few nan) lr 9.0451e-03 elapsed 0:05:08 eta 0:13:25\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/782] time 0.280 (0.285) data 0.000 (0.001) loss 2.2110 (2.0985) acc 40.6250 (49.1098) (mean 49.9413 many 49.9413 med nan few nan) lr 9.0451e-03 elapsed 0:05:08 eta 0:13:25\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/782] time 0.277 (0.285) data 0.000 (0.001) loss 1.9900 (2.0422) acc 48.4375 (51.4084) (mean 51.8612 many 51.8612 med nan few nan) lr 9.0451e-03 elapsed 0:05:13 eta 0:13:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/782] time 0.277 (0.285) data 0.000 (0.001) loss 1.9900 (2.0422) acc 48.4375 (51.4084) (mean 51.8612 many 51.8612 med nan few nan) lr 9.0451e-03 elapsed 0:05:13 eta 0:13:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/782] time 0.277 (0.285) data 0.003 (0.001) loss 2.1947 (2.0306) acc 51.5625 (51.6616) (mean 52.2556 many 52.2556 med nan few nan) lr 9.0451e-03 elapsed 0:05:19 eta 0:13:13\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/782] time 0.277 (0.285) data 0.003 (0.001) loss 2.1947 (2.0306) acc 51.5625 (51.6616) (mean 52.2556 many 52.2556 med nan few nan) lr 9.0451e-03 elapsed 0:05:19 eta 0:13:13\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/782] time 0.281 (0.284) data 0.000 (0.001) loss 1.6973 (1.9346) acc 59.3750 (53.3100) (mean 51.7337 many 51.7337 med nan few nan) lr 9.0451e-03 elapsed 0:05:24 eta 0:13:07\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/782] time 0.281 (0.284) data 0.000 (0.001) loss 1.6973 (1.9346) acc 59.3750 (53.3100) (mean 51.7337 many 51.7337 med nan few nan) lr 9.0451e-03 elapsed 0:05:24 eta 0:13:07\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/782] time 0.277 (0.284) data 0.000 (0.001) loss 2.4875 (2.0299) acc 37.5000 (51.2157) (mean 51.7047 many 51.7047 med nan few nan) lr 9.0451e-03 elapsed 0:05:30 eta 0:13:01\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/782] time 0.277 (0.284) data 0.000 (0.001) loss 2.4875 (2.0299) acc 37.5000 (51.2157) (mean 51.7047 many 51.7047 med nan few nan) lr 9.0451e-03 elapsed 0:05:30 eta 0:13:01\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/782] time 0.276 (0.284) data 0.000 (0.001) loss 1.8097 (2.0071) acc 56.2500 (51.7096) (mean 51.8538 many 51.8538 med nan few nan) lr 9.0451e-03 elapsed 0:05:36 eta 0:12:55\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/782] time 0.276 (0.284) data 0.000 (0.001) loss 1.8097 (2.0071) acc 56.2500 (51.7096) (mean 51.8538 many 51.8538 med nan few nan) lr 9.0451e-03 elapsed 0:05:36 eta 0:12:55\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/782] time 0.284 (0.284) data 0.000 (0.001) loss 2.0167 (2.0842) acc 46.8750 (49.9579) (mean 49.9864 many 49.9864 med nan few nan) lr 9.0451e-03 elapsed 0:05:41 eta 0:12:49\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/782] time 0.284 (0.284) data 0.000 (0.001) loss 2.0167 (2.0842) acc 46.8750 (49.9579) (mean 49.9864 many 49.9864 med nan few nan) lr 9.0451e-03 elapsed 0:05:41 eta 0:12:49\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/782] time 0.283 (0.284) data 0.000 (0.001) loss 2.0501 (2.1129) acc 50.0000 (47.2861) (mean 48.0860 many 48.0860 med nan few nan) lr 9.0451e-03 elapsed 0:05:47 eta 0:12:43\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/782] time 0.283 (0.284) data 0.000 (0.001) loss 2.0501 (2.1129) acc 50.0000 (47.2861) (mean 48.0860 many 48.0860 med nan few nan) lr 9.0451e-03 elapsed 0:05:47 eta 0:12:43\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/782] time 0.281 (0.284) data 0.000 (0.001) loss 1.9407 (2.0590) acc 54.6875 (50.7465) (mean 49.9174 many 49.9174 med nan few nan) lr 9.0451e-03 elapsed 0:05:52 eta 0:12:38\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/782] time 0.281 (0.284) data 0.000 (0.001) loss 1.9407 (2.0590) acc 54.6875 (50.7465) (mean 49.9174 many 49.9174 med nan few nan) lr 9.0451e-03 elapsed 0:05:52 eta 0:12:38\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/782] time 0.281 (0.284) data 0.003 (0.001) loss 2.2937 (2.0335) acc 43.7500 (50.9314) (mean 50.8472 many 50.8472 med nan few nan) lr 9.0451e-03 elapsed 0:05:58 eta 0:12:32\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/782] time 0.281 (0.284) data 0.003 (0.001) loss 2.2937 (2.0335) acc 43.7500 (50.9314) (mean 50.8472 many 50.8472 med nan few nan) lr 9.0451e-03 elapsed 0:05:58 eta 0:12:32\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/782] time 0.284 (0.284) data 0.000 (0.001) loss 2.0321 (2.0901) acc 53.1250 (48.8376) (mean 49.7104 many 49.7104 med nan few nan) lr 9.0451e-03 elapsed 0:06:04 eta 0:12:26\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/782] time 0.284 (0.284) data 0.000 (0.001) loss 2.0321 (2.0901) acc 53.1250 (48.8376) (mean 49.7104 many 49.7104 med nan few nan) lr 9.0451e-03 elapsed 0:06:04 eta 0:12:26\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/782] time 0.282 (0.284) data 0.005 (0.001) loss 1.9216 (2.0185) acc 57.8125 (52.7840) (mean 52.3651 many 52.3651 med nan few nan) lr 9.0451e-03 elapsed 0:06:09 eta 0:12:20\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/782] time 0.282 (0.284) data 0.005 (0.001) loss 1.9216 (2.0185) acc 57.8125 (52.7840) (mean 52.3651 many 52.3651 med nan few nan) lr 9.0451e-03 elapsed 0:06:09 eta 0:12:20\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/782] time 0.274 (0.284) data 0.000 (0.001) loss 1.8665 (1.9284) acc 51.5625 (52.0876) (mean 50.9915 many 50.9915 med nan few nan) lr 9.0451e-03 elapsed 0:06:15 eta 0:12:14\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/782] time 0.274 (0.284) data 0.000 (0.001) loss 1.8665 (1.9284) acc 51.5625 (52.0876) (mean 50.9915 many 50.9915 med nan few nan) lr 9.0451e-03 elapsed 0:06:15 eta 0:12:14\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/782] time 0.279 (0.284) data 0.000 (0.001) loss 2.1302 (2.0816) acc 51.5625 (50.8001) (mean 50.9329 many 50.9329 med nan few nan) lr 9.0451e-03 elapsed 0:06:21 eta 0:12:08\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/782] time 0.279 (0.284) data 0.000 (0.001) loss 2.1302 (2.0816) acc 51.5625 (50.8001) (mean 50.9329 many 50.9329 med nan few nan) lr 9.0451e-03 elapsed 0:06:21 eta 0:12:08\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/782] time 0.280 (0.284) data 0.000 (0.001) loss 1.9053 (2.0676) acc 53.1250 (48.0638) (mean 48.8954 many 48.8954 med nan few nan) lr 9.0451e-03 elapsed 0:06:26 eta 0:12:03\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/782] time 0.280 (0.284) data 0.000 (0.001) loss 1.9053 (2.0676) acc 53.1250 (48.0638) (mean 48.8954 many 48.8954 med nan few nan) lr 9.0451e-03 elapsed 0:06:26 eta 0:12:03\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [600/782] time 0.277 (0.284) data 0.000 (0.001) loss 2.7198 (2.1549) acc 34.3750 (47.0209) (mean 47.9131 many 47.9131 med nan few nan) lr 9.0451e-03 elapsed 0:06:32 eta 0:11:57\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [600/782] time 0.277 (0.284) data 0.000 (0.001) loss 2.7198 (2.1549) acc 34.3750 (47.0209) (mean 47.9131 many 47.9131 med nan few nan) lr 9.0451e-03 elapsed 0:06:32 eta 0:11:57\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/782] time 0.285 (0.284) data 0.000 (0.001) loss 1.8142 (2.0281) acc 60.9375 (52.0799) (mean 50.8462 many 50.8462 med nan few nan) lr 9.0451e-03 elapsed 0:06:37 eta 0:11:51\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/782] time 0.285 (0.284) data 0.000 (0.001) loss 1.8142 (2.0281) acc 60.9375 (52.0799) (mean 50.8462 many 50.8462 med nan few nan) lr 9.0451e-03 elapsed 0:06:37 eta 0:11:51\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [640/782] time 0.281 (0.284) data 0.000 (0.001) loss 2.1363 (1.9967) acc 50.0000 (49.7084) (mean 49.8213 many 49.8213 med nan few nan) lr 9.0451e-03 elapsed 0:06:43 eta 0:11:45\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [640/782] time 0.281 (0.284) data 0.000 (0.001) loss 2.1363 (1.9967) acc 50.0000 (49.7084) (mean 49.8213 many 49.8213 med nan few nan) lr 9.0451e-03 elapsed 0:06:43 eta 0:11:45\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [660/782] time 0.284 (0.284) data 0.001 (0.001) loss 1.9863 (2.0344) acc 50.0000 (48.7136) (mean 48.9288 many 48.9288 med nan few nan) lr 9.0451e-03 elapsed 0:06:49 eta 0:11:40\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [660/782] time 0.284 (0.284) data 0.001 (0.001) loss 1.9863 (2.0344) acc 50.0000 (48.7136) (mean 48.9288 many 48.9288 med nan few nan) lr 9.0451e-03 elapsed 0:06:49 eta 0:11:40\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [680/782] time 0.280 (0.284) data 0.000 (0.001) loss 2.2478 (2.0177) acc 50.0000 (51.4892) (mean 51.4333 many 51.4333 med nan few nan) lr 9.0451e-03 elapsed 0:06:54 eta 0:11:34\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [680/782] time 0.280 (0.284) data 0.000 (0.001) loss 2.2478 (2.0177) acc 50.0000 (51.4892) (mean 51.4333 many 51.4333 med nan few nan) lr 9.0451e-03 elapsed 0:06:54 eta 0:11:34\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [700/782] time 0.287 (0.284) data 0.001 (0.001) loss 2.1496 (2.0439) acc 45.3125 (49.7503) (mean 50.6315 many 50.6315 med nan few nan) lr 9.0451e-03 elapsed 0:07:00 eta 0:11:28\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [700/782] time 0.287 (0.284) data 0.001 (0.001) loss 2.1496 (2.0439) acc 45.3125 (49.7503) (mean 50.6315 many 50.6315 med nan few nan) lr 9.0451e-03 elapsed 0:07:00 eta 0:11:28\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [720/782] time 0.281 (0.284) data 0.000 (0.001) loss 2.1224 (1.9337) acc 51.5625 (53.7254) (mean 53.4429 many 53.4429 med nan few nan) lr 9.0451e-03 elapsed 0:07:06 eta 0:11:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [720/782] time 0.281 (0.284) data 0.000 (0.001) loss 2.1224 (1.9337) acc 51.5625 (53.7254) (mean 53.4429 many 53.4429 med nan few nan) lr 9.0451e-03 elapsed 0:07:06 eta 0:11:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [740/782] time 0.279 (0.284) data 0.000 (0.001) loss 1.7181 (1.9999) acc 62.5000 (51.4950) (mean 51.1812 many 51.1812 med nan few nan) lr 9.0451e-03 elapsed 0:07:11 eta 0:11:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [740/782] time 0.279 (0.284) data 0.000 (0.001) loss 1.7181 (1.9999) acc 62.5000 (51.4950) (mean 51.1812 many 51.1812 med nan few nan) lr 9.0451e-03 elapsed 0:07:11 eta 0:11:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [760/782] time 0.286 (0.284) data 0.000 (0.001) loss 1.8857 (2.0291) acc 60.9375 (51.1330) (mean 51.6421 many 51.6421 med nan few nan) lr 9.0451e-03 elapsed 0:07:17 eta 0:11:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [760/782] time 0.286 (0.284) data 0.000 (0.001) loss 1.8857 (2.0291) acc 60.9375 (51.1330) (mean 51.6421 many 51.6421 med nan few nan) lr 9.0451e-03 elapsed 0:07:17 eta 0:11:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [780/782] time 0.280 (0.284) data 0.000 (0.001) loss 1.8279 (1.9342) acc 59.3750 (53.1190) (mean 52.6061 many 52.6061 med nan few nan) lr 9.0451e-03 elapsed 0:07:22 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [780/782] time 0.280 (0.284) data 0.000 (0.001) loss 1.8279 (1.9342) acc 59.3750 (53.1190) (mean 52.6061 many 52.6061 med nan few nan) lr 9.0451e-03 elapsed 0:07:22 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [782/782] time 0.276 (0.284) data 0.000 (0.001) loss 2.8269 (2.0215) acc 37.5000 (50.8545) (mean 52.1263 many 52.1263 med nan few nan) lr 9.0451e-03 elapsed 0:07:23 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [782/782] time 0.276 (0.284) data 0.000 (0.001) loss 2.8269 (2.0215) acc 37.5000 (50.8545) (mean 52.1263 many 52.1263 med nan few nan) lr 9.0451e-03 elapsed 0:07:23 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/782] time 0.282 (0.284) data 0.000 (0.001) loss 1.7427 (1.9823) acc 60.9375 (52.8163) (mean 52.3099 many 52.3099 med nan few nan) lr 6.5451e-03 elapsed 0:07:29 eta 0:10:59\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/782] time 0.282 (0.284) data 0.000 (0.001) loss 1.7427 (1.9823) acc 60.9375 (52.8163) (mean 52.3099 many 52.3099 med nan few nan) lr 6.5451e-03 elapsed 0:07:29 eta 0:10:59\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/782] time 0.275 (0.283) data 0.000 (0.001) loss 1.5614 (1.9313) acc 60.9375 (54.4717) (mean 53.1729 many 53.1729 med nan few nan) lr 6.5451e-03 elapsed 0:07:34 eta 0:10:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/782] time 0.275 (0.283) data 0.000 (0.001) loss 1.5614 (1.9313) acc 60.9375 (54.4717) (mean 53.1729 many 53.1729 med nan few nan) lr 6.5451e-03 elapsed 0:07:34 eta 0:10:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/782] time 0.277 (0.283) data 0.000 (0.001) loss 2.2887 (1.9776) acc 45.3125 (52.6618) (mean 53.1752 many 53.1752 med nan few nan) lr 6.5451e-03 elapsed 0:07:40 eta 0:10:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/782] time 0.277 (0.283) data 0.000 (0.001) loss 2.2887 (1.9776) acc 45.3125 (52.6618) (mean 53.1752 many 53.1752 med nan few nan) lr 6.5451e-03 elapsed 0:07:40 eta 0:10:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/782] time 0.280 (0.283) data 0.000 (0.001) loss 2.2890 (2.0286) acc 42.1875 (51.2426) (mean 52.3597 many 52.3597 med nan few nan) lr 6.5451e-03 elapsed 0:07:45 eta 0:10:42\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/782] time 0.280 (0.283) data 0.000 (0.001) loss 2.2890 (2.0286) acc 42.1875 (51.2426) (mean 52.3597 many 52.3597 med nan few nan) lr 6.5451e-03 elapsed 0:07:45 eta 0:10:42\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/782] time 0.285 (0.283) data 0.005 (0.001) loss 2.1143 (2.0514) acc 50.0000 (50.9307) (mean 51.6086 many 51.6086 med nan few nan) lr 6.5451e-03 elapsed 0:07:51 eta 0:10:36\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/782] time 0.285 (0.283) data 0.005 (0.001) loss 2.1143 (2.0514) acc 50.0000 (50.9307) (mean 51.6086 many 51.6086 med nan few nan) lr 6.5451e-03 elapsed 0:07:51 eta 0:10:36\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.9936 (1.9226) acc 60.9375 (54.7789) (mean 54.1249 many 54.1249 med nan few nan) lr 6.5451e-03 elapsed 0:07:57 eta 0:10:30\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.9936 (1.9226) acc 60.9375 (54.7789) (mean 54.1249 many 54.1249 med nan few nan) lr 6.5451e-03 elapsed 0:07:57 eta 0:10:30\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/782] time 0.284 (0.283) data 0.001 (0.001) loss 2.0874 (1.9809) acc 45.3125 (52.6377) (mean 53.4221 many 53.4221 med nan few nan) lr 6.5451e-03 elapsed 0:08:02 eta 0:10:24\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/782] time 0.284 (0.283) data 0.001 (0.001) loss 2.0874 (1.9809) acc 45.3125 (52.6377) (mean 53.4221 many 53.4221 med nan few nan) lr 6.5451e-03 elapsed 0:08:02 eta 0:10:24\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9776 (1.9325) acc 53.1250 (53.0225) (mean 53.1437 many 53.1437 med nan few nan) lr 6.5451e-03 elapsed 0:08:08 eta 0:10:19\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9776 (1.9325) acc 53.1250 (53.0225) (mean 53.1437 many 53.1437 med nan few nan) lr 6.5451e-03 elapsed 0:08:08 eta 0:10:19\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/782] time 0.288 (0.283) data 0.000 (0.001) loss 1.5958 (1.9189) acc 59.3750 (52.6142) (mean 52.1615 many 52.1615 med nan few nan) lr 6.5451e-03 elapsed 0:08:14 eta 0:10:13\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/782] time 0.288 (0.283) data 0.000 (0.001) loss 1.5958 (1.9189) acc 59.3750 (52.6142) (mean 52.1615 many 52.1615 med nan few nan) lr 6.5451e-03 elapsed 0:08:14 eta 0:10:13\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/782] time 0.276 (0.283) data 0.000 (0.001) loss 1.8724 (1.9551) acc 56.2500 (52.9700) (mean 53.0323 many 53.0323 med nan few nan) lr 6.5451e-03 elapsed 0:08:19 eta 0:10:07\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/782] time 0.276 (0.283) data 0.000 (0.001) loss 1.8724 (1.9551) acc 56.2500 (52.9700) (mean 53.0323 many 53.0323 med nan few nan) lr 6.5451e-03 elapsed 0:08:19 eta 0:10:07\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/782] time 0.286 (0.283) data 0.000 (0.001) loss 1.8938 (2.0546) acc 51.5625 (49.9585) (mean 50.8762 many 50.8762 med nan few nan) lr 6.5451e-03 elapsed 0:08:25 eta 0:10:02\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/782] time 0.286 (0.283) data 0.000 (0.001) loss 1.8938 (2.0546) acc 51.5625 (49.9585) (mean 50.8762 many 50.8762 med nan few nan) lr 6.5451e-03 elapsed 0:08:25 eta 0:10:02\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/782] time 0.277 (0.283) data 0.000 (0.001) loss 2.0226 (2.0016) acc 43.7500 (50.3245) (mean 50.8885 many 50.8885 med nan few nan) lr 6.5451e-03 elapsed 0:08:31 eta 0:09:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/782] time 0.277 (0.283) data 0.000 (0.001) loss 2.0226 (2.0016) acc 43.7500 (50.3245) (mean 50.8885 many 50.8885 med nan few nan) lr 6.5451e-03 elapsed 0:08:31 eta 0:09:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9598 (1.9878) acc 48.4375 (52.1992) (mean 51.8493 many 51.8493 med nan few nan) lr 6.5451e-03 elapsed 0:08:36 eta 0:09:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9598 (1.9878) acc 48.4375 (52.1992) (mean 51.8493 many 51.8493 med nan few nan) lr 6.5451e-03 elapsed 0:08:36 eta 0:09:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/782] time 0.283 (0.283) data 0.000 (0.001) loss 2.2214 (1.9850) acc 43.7500 (52.3967) (mean 52.2095 many 52.2095 med nan few nan) lr 6.5451e-03 elapsed 0:08:42 eta 0:09:44\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/782] time 0.283 (0.283) data 0.000 (0.001) loss 2.2214 (1.9850) acc 43.7500 (52.3967) (mean 52.2095 many 52.2095 med nan few nan) lr 6.5451e-03 elapsed 0:08:42 eta 0:09:44\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.7937 (1.9599) acc 56.2500 (53.5085) (mean 53.3032 many 53.3032 med nan few nan) lr 6.5451e-03 elapsed 0:08:47 eta 0:09:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.7937 (1.9599) acc 56.2500 (53.5085) (mean 53.3032 many 53.3032 med nan few nan) lr 6.5451e-03 elapsed 0:08:47 eta 0:09:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/782] time 0.280 (0.283) data 0.000 (0.001) loss 1.7546 (1.9274) acc 57.8125 (53.3554) (mean 52.9918 many 52.9918 med nan few nan) lr 6.5451e-03 elapsed 0:08:53 eta 0:09:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/782] time 0.280 (0.283) data 0.000 (0.001) loss 1.7546 (1.9274) acc 57.8125 (53.3554) (mean 52.9918 many 52.9918 med nan few nan) lr 6.5451e-03 elapsed 0:08:53 eta 0:09:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/782] time 0.276 (0.283) data 0.000 (0.001) loss 1.9913 (1.9558) acc 54.6875 (53.4087) (mean 52.8821 many 52.8821 med nan few nan) lr 6.5451e-03 elapsed 0:08:58 eta 0:09:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/782] time 0.276 (0.283) data 0.000 (0.001) loss 1.9913 (1.9558) acc 54.6875 (53.4087) (mean 52.8821 many 52.8821 med nan few nan) lr 6.5451e-03 elapsed 0:08:58 eta 0:09:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/782] time 0.279 (0.283) data 0.000 (0.001) loss 2.2797 (1.9644) acc 45.3125 (52.4352) (mean 52.6617 many 52.6617 med nan few nan) lr 6.5451e-03 elapsed 0:09:04 eta 0:09:21\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/782] time 0.279 (0.283) data 0.000 (0.001) loss 2.2797 (1.9644) acc 45.3125 (52.4352) (mean 52.6617 many 52.6617 med nan few nan) lr 6.5451e-03 elapsed 0:09:04 eta 0:09:21\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/782] time 0.278 (0.283) data 0.000 (0.001) loss 2.4188 (1.9987) acc 40.6250 (51.5126) (mean 52.1803 many 52.1803 med nan few nan) lr 6.5451e-03 elapsed 0:09:10 eta 0:09:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/782] time 0.278 (0.283) data 0.000 (0.001) loss 2.4188 (1.9987) acc 40.6250 (51.5126) (mean 52.1803 many 52.1803 med nan few nan) lr 6.5451e-03 elapsed 0:09:10 eta 0:09:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/782] time 0.285 (0.283) data 0.000 (0.001) loss 1.7087 (2.0058) acc 57.8125 (51.5767) (mean 51.7231 many 51.7231 med nan few nan) lr 6.5451e-03 elapsed 0:09:15 eta 0:09:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/782] time 0.285 (0.283) data 0.000 (0.001) loss 1.7087 (2.0058) acc 57.8125 (51.5767) (mean 51.7231 many 51.7231 med nan few nan) lr 6.5451e-03 elapsed 0:09:15 eta 0:09:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/782] time 0.280 (0.283) data 0.001 (0.001) loss 2.1081 (1.9124) acc 50.0000 (53.3717) (mean 53.6276 many 53.6276 med nan few nan) lr 6.5451e-03 elapsed 0:09:21 eta 0:09:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/782] time 0.280 (0.283) data 0.001 (0.001) loss 2.1081 (1.9124) acc 50.0000 (53.3717) (mean 53.6276 many 53.6276 med nan few nan) lr 6.5451e-03 elapsed 0:09:21 eta 0:09:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9430 (1.9880) acc 54.6875 (51.9541) (mean 52.4224 many 52.4224 med nan few nan) lr 6.5451e-03 elapsed 0:09:27 eta 0:08:59\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9430 (1.9880) acc 54.6875 (51.9541) (mean 52.4224 many 52.4224 med nan few nan) lr 6.5451e-03 elapsed 0:09:27 eta 0:08:59\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/782] time 0.290 (0.283) data 0.003 (0.001) loss 1.9764 (2.0068) acc 54.6875 (52.1348) (mean 52.6384 many 52.6384 med nan few nan) lr 6.5451e-03 elapsed 0:09:32 eta 0:08:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/782] time 0.290 (0.283) data 0.003 (0.001) loss 1.9764 (2.0068) acc 54.6875 (52.1348) (mean 52.6384 many 52.6384 med nan few nan) lr 6.5451e-03 elapsed 0:09:32 eta 0:08:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/782] time 0.278 (0.283) data 0.000 (0.001) loss 2.1177 (1.9763) acc 48.4375 (52.1504) (mean 52.5627 many 52.5627 med nan few nan) lr 6.5451e-03 elapsed 0:09:38 eta 0:08:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/782] time 0.278 (0.283) data 0.000 (0.001) loss 2.1177 (1.9763) acc 48.4375 (52.1504) (mean 52.5627 many 52.5627 med nan few nan) lr 6.5451e-03 elapsed 0:09:38 eta 0:08:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/782] time 0.285 (0.283) data 0.000 (0.001) loss 2.2122 (1.9142) acc 50.0000 (52.8336) (mean 53.2953 many 53.2953 med nan few nan) lr 6.5451e-03 elapsed 0:09:44 eta 0:08:42\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/782] time 0.285 (0.283) data 0.000 (0.001) loss 2.2122 (1.9142) acc 50.0000 (52.8336) (mean 53.2953 many 53.2953 med nan few nan) lr 6.5451e-03 elapsed 0:09:44 eta 0:08:42\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/782] time 0.282 (0.283) data 0.000 (0.001) loss 1.8993 (1.9006) acc 48.4375 (52.9867) (mean 53.2513 many 53.2513 med nan few nan) lr 6.5451e-03 elapsed 0:09:49 eta 0:08:36\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/782] time 0.282 (0.283) data 0.000 (0.001) loss 1.8993 (1.9006) acc 48.4375 (52.9867) (mean 53.2513 many 53.2513 med nan few nan) lr 6.5451e-03 elapsed 0:09:49 eta 0:08:36\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/782] time 0.280 (0.283) data 0.000 (0.001) loss 1.6728 (1.9332) acc 64.0625 (54.1846) (mean 53.6575 many 53.6575 med nan few nan) lr 6.5451e-03 elapsed 0:09:55 eta 0:08:30\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/782] time 0.280 (0.283) data 0.000 (0.001) loss 1.6728 (1.9332) acc 64.0625 (54.1846) (mean 53.6575 many 53.6575 med nan few nan) lr 6.5451e-03 elapsed 0:09:55 eta 0:08:30\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/782] time 0.284 (0.283) data 0.000 (0.001) loss 2.4758 (1.9228) acc 31.2500 (53.8075) (mean 54.3742 many 54.3742 med nan few nan) lr 6.5451e-03 elapsed 0:10:00 eta 0:08:25\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/782] time 0.284 (0.283) data 0.000 (0.001) loss 2.4758 (1.9228) acc 31.2500 (53.8075) (mean 54.3742 many 54.3742 med nan few nan) lr 6.5451e-03 elapsed 0:10:00 eta 0:08:25\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.9604 (2.0073) acc 56.2500 (50.7093) (mean 51.3203 many 51.3203 med nan few nan) lr 6.5451e-03 elapsed 0:10:06 eta 0:08:19\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.9604 (2.0073) acc 56.2500 (50.7093) (mean 51.3203 many 51.3203 med nan few nan) lr 6.5451e-03 elapsed 0:10:06 eta 0:08:19\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/782] time 0.274 (0.283) data 0.000 (0.001) loss 1.8178 (1.9726) acc 64.0625 (53.7796) (mean 53.0991 many 53.0991 med nan few nan) lr 6.5451e-03 elapsed 0:10:12 eta 0:08:13\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/782] time 0.274 (0.283) data 0.000 (0.001) loss 1.8178 (1.9726) acc 64.0625 (53.7796) (mean 53.0991 many 53.0991 med nan few nan) lr 6.5451e-03 elapsed 0:10:12 eta 0:08:13\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/782] time 0.283 (0.283) data 0.000 (0.001) loss 2.0935 (1.9894) acc 56.2500 (54.0889) (mean 54.1669 many 54.1669 med nan few nan) lr 6.5451e-03 elapsed 0:10:17 eta 0:08:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/782] time 0.283 (0.283) data 0.000 (0.001) loss 2.0935 (1.9894) acc 56.2500 (54.0889) (mean 54.1669 many 54.1669 med nan few nan) lr 6.5451e-03 elapsed 0:10:17 eta 0:08:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [640/782] time 0.274 (0.283) data 0.002 (0.001) loss 1.9547 (1.9593) acc 54.6875 (53.2800) (mean 54.5090 many 54.5090 med nan few nan) lr 6.5451e-03 elapsed 0:10:23 eta 0:08:02\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [640/782] time 0.274 (0.283) data 0.002 (0.001) loss 1.9547 (1.9593) acc 54.6875 (53.2800) (mean 54.5090 many 54.5090 med nan few nan) lr 6.5451e-03 elapsed 0:10:23 eta 0:08:02\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [660/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9578 (1.9758) acc 54.6875 (50.9000) (mean 51.6987 many 51.6987 med nan few nan) lr 6.5451e-03 elapsed 0:10:28 eta 0:07:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [660/782] time 0.281 (0.283) data 0.000 (0.001) loss 1.9578 (1.9758) acc 54.6875 (50.9000) (mean 51.6987 many 51.6987 med nan few nan) lr 6.5451e-03 elapsed 0:10:28 eta 0:07:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [680/782] time 0.282 (0.283) data 0.000 (0.001) loss 2.0177 (1.9825) acc 51.5625 (52.0631) (mean 52.2255 many 52.2255 med nan few nan) lr 6.5451e-03 elapsed 0:10:34 eta 0:07:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [680/782] time 0.282 (0.283) data 0.000 (0.001) loss 2.0177 (1.9825) acc 51.5625 (52.0631) (mean 52.2255 many 52.2255 med nan few nan) lr 6.5451e-03 elapsed 0:10:34 eta 0:07:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [700/782] time 0.280 (0.283) data 0.000 (0.001) loss 2.1568 (2.0469) acc 43.7500 (49.5179) (mean 50.7212 many 50.7212 med nan few nan) lr 6.5451e-03 elapsed 0:10:40 eta 0:07:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [700/782] time 0.280 (0.283) data 0.000 (0.001) loss 2.1568 (2.0469) acc 43.7500 (49.5179) (mean 50.7212 many 50.7212 med nan few nan) lr 6.5451e-03 elapsed 0:10:40 eta 0:07:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [720/782] time 0.274 (0.283) data 0.000 (0.001) loss 2.0009 (1.9743) acc 60.9375 (53.5692) (mean 53.2495 many 53.2495 med nan few nan) lr 6.5451e-03 elapsed 0:10:45 eta 0:07:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [720/782] time 0.274 (0.283) data 0.000 (0.001) loss 2.0009 (1.9743) acc 60.9375 (53.5692) (mean 53.2495 many 53.2495 med nan few nan) lr 6.5451e-03 elapsed 0:10:45 eta 0:07:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [740/782] time 0.284 (0.283) data 0.000 (0.001) loss 1.9802 (2.0075) acc 53.1250 (52.7336) (mean 53.6973 many 53.6973 med nan few nan) lr 6.5451e-03 elapsed 0:10:51 eta 0:07:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [740/782] time 0.284 (0.283) data 0.000 (0.001) loss 1.9802 (2.0075) acc 53.1250 (52.7336) (mean 53.6973 many 53.6973 med nan few nan) lr 6.5451e-03 elapsed 0:10:51 eta 0:07:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [760/782] time 0.272 (0.283) data 0.000 (0.001) loss 1.5864 (1.9820) acc 54.6875 (51.2010) (mean 50.8872 many 50.8872 med nan few nan) lr 6.5451e-03 elapsed 0:10:56 eta 0:07:28\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [760/782] time 0.272 (0.283) data 0.000 (0.001) loss 1.5864 (1.9820) acc 54.6875 (51.2010) (mean 50.8872 many 50.8872 med nan few nan) lr 6.5451e-03 elapsed 0:10:56 eta 0:07:28\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [780/782] time 0.281 (0.283) data 0.000 (0.001) loss 2.2048 (1.9788) acc 48.4375 (53.0029) (mean 53.0628 many 53.0628 med nan few nan) lr 6.5451e-03 elapsed 0:11:02 eta 0:07:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [780/782] time 0.281 (0.283) data 0.000 (0.001) loss 2.2048 (1.9788) acc 48.4375 (53.0029) (mean 53.0628 many 53.0628 med nan few nan) lr 6.5451e-03 elapsed 0:11:02 eta 0:07:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [782/782] time 0.272 (0.283) data 0.000 (0.001) loss 2.2826 (2.0168) acc 43.7500 (51.3854) (mean 52.5263 many 52.5263 med nan few nan) lr 6.5451e-03 elapsed 0:11:02 eta 0:07:21\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [782/782] time 0.272 (0.283) data 0.000 (0.001) loss 2.2826 (2.0168) acc 43.7500 (51.3854) (mean 52.5263 many 52.5263 med nan few nan) lr 6.5451e-03 elapsed 0:11:02 eta 0:07:21\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/782] time 0.286 (0.283) data 0.000 (0.001) loss 1.8206 (1.9423) acc 53.1250 (52.2084) (mean 52.5505 many 52.5505 med nan few nan) lr 3.4549e-03 elapsed 0:11:08 eta 0:07:16\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/782] time 0.286 (0.283) data 0.000 (0.001) loss 1.8206 (1.9423) acc 53.1250 (52.2084) (mean 52.5505 many 52.5505 med nan few nan) lr 3.4549e-03 elapsed 0:11:08 eta 0:07:16\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/782] time 0.281 (0.283) data 0.002 (0.001) loss 1.8391 (1.9145) acc 57.8125 (53.8819) (mean 53.5461 many 53.5461 med nan few nan) lr 3.4549e-03 elapsed 0:11:14 eta 0:07:10\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/782] time 0.281 (0.283) data 0.002 (0.001) loss 1.8391 (1.9145) acc 57.8125 (53.8819) (mean 53.5461 many 53.5461 med nan few nan) lr 3.4549e-03 elapsed 0:11:14 eta 0:07:10\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/782] time 0.273 (0.283) data 0.000 (0.001) loss 2.1057 (1.9220) acc 53.1250 (53.0102) (mean 52.8283 many 52.8283 med nan few nan) lr 3.4549e-03 elapsed 0:11:20 eta 0:07:04\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/782] time 0.273 (0.283) data 0.000 (0.001) loss 2.1057 (1.9220) acc 53.1250 (53.0102) (mean 52.8283 many 52.8283 med nan few nan) lr 3.4549e-03 elapsed 0:11:20 eta 0:07:04\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/782] time 0.282 (0.283) data 0.000 (0.001) loss 2.0266 (1.8725) acc 46.8750 (54.5986) (mean 54.1023 many 54.1023 med nan few nan) lr 3.4549e-03 elapsed 0:11:25 eta 0:06:59\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/782] time 0.282 (0.283) data 0.000 (0.001) loss 2.0266 (1.8725) acc 46.8750 (54.5986) (mean 54.1023 many 54.1023 med nan few nan) lr 3.4549e-03 elapsed 0:11:25 eta 0:06:59\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.7640 (1.8931) acc 57.8125 (54.1281) (mean 54.3463 many 54.3463 med nan few nan) lr 3.4549e-03 elapsed 0:11:31 eta 0:06:53\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.7640 (1.8931) acc 57.8125 (54.1281) (mean 54.3463 many 54.3463 med nan few nan) lr 3.4549e-03 elapsed 0:11:31 eta 0:06:53\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/782] time 0.284 (0.283) data 0.000 (0.001) loss 1.7725 (1.8653) acc 60.9375 (55.2608) (mean 55.0688 many 55.0688 med nan few nan) lr 3.4549e-03 elapsed 0:11:36 eta 0:06:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/782] time 0.284 (0.283) data 0.000 (0.001) loss 1.7725 (1.8653) acc 60.9375 (55.2608) (mean 55.0688 many 55.0688 med nan few nan) lr 3.4549e-03 elapsed 0:11:36 eta 0:06:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.8347 (1.9315) acc 54.6875 (54.7468) (mean 54.9679 many 54.9679 med nan few nan) lr 3.4549e-03 elapsed 0:11:42 eta 0:06:42\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/782] time 0.278 (0.283) data 0.000 (0.001) loss 1.8347 (1.9315) acc 54.6875 (54.7468) (mean 54.9679 many 54.9679 med nan few nan) lr 3.4549e-03 elapsed 0:11:42 eta 0:06:42\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/782] time 0.288 (0.283) data 0.000 (0.001) loss 1.6227 (1.9666) acc 59.3750 (53.3272) (mean 53.4629 many 53.4629 med nan few nan) lr 3.4549e-03 elapsed 0:11:48 eta 0:06:36\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/782] time 0.288 (0.283) data 0.000 (0.001) loss 1.6227 (1.9666) acc 59.3750 (53.3272) (mean 53.4629 many 53.4629 med nan few nan) lr 3.4549e-03 elapsed 0:11:48 eta 0:06:36\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/782] time 0.276 (0.283) data 0.000 (0.001) loss 1.6981 (1.9217) acc 53.1250 (53.4834) (mean 53.4353 many 53.4353 med nan few nan) lr 3.4549e-03 elapsed 0:11:53 eta 0:06:30\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/782] time 0.276 (0.283) data 0.000 (0.001) loss 1.6981 (1.9217) acc 53.1250 (53.4834) (mean 53.4353 many 53.4353 med nan few nan) lr 3.4549e-03 elapsed 0:11:53 eta 0:06:30\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/782] time 0.286 (0.283) data 0.000 (0.001) loss 1.8271 (1.9342) acc 57.8125 (52.8772) (mean 52.8423 many 52.8423 med nan few nan) lr 3.4549e-03 elapsed 0:11:59 eta 0:06:25\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/782] time 0.286 (0.283) data 0.000 (0.001) loss 1.8271 (1.9342) acc 57.8125 (52.8772) (mean 52.8423 many 52.8423 med nan few nan) lr 3.4549e-03 elapsed 0:11:59 eta 0:06:25\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/782] time 0.282 (0.283) data 0.000 (0.001) loss 2.1953 (1.9779) acc 50.0000 (52.1685) (mean 53.1364 many 53.1364 med nan few nan) lr 3.4549e-03 elapsed 0:12:05 eta 0:06:19\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/782] time 0.282 (0.283) data 0.000 (0.001) loss 2.1953 (1.9779) acc 50.0000 (52.1685) (mean 53.1364 many 53.1364 med nan few nan) lr 3.4549e-03 elapsed 0:12:05 eta 0:06:19\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/782] time 0.277 (0.282) data 0.000 (0.001) loss 2.2250 (1.9388) acc 48.4375 (52.8946) (mean 53.3977 many 53.3977 med nan few nan) lr 3.4549e-03 elapsed 0:12:10 eta 0:06:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/782] time 0.277 (0.282) data 0.000 (0.001) loss 2.2250 (1.9388) acc 48.4375 (52.8946) (mean 53.3977 many 53.3977 med nan few nan) lr 3.4549e-03 elapsed 0:12:10 eta 0:06:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.9188 (1.8700) acc 54.6875 (56.1636) (mean 55.4539 many 55.4539 med nan few nan) lr 3.4549e-03 elapsed 0:12:16 eta 0:06:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.9188 (1.8700) acc 54.6875 (56.1636) (mean 55.4539 many 55.4539 med nan few nan) lr 3.4549e-03 elapsed 0:12:16 eta 0:06:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.7995 (1.9533) acc 62.5000 (52.7592) (mean 53.5056 many 53.5056 med nan few nan) lr 3.4549e-03 elapsed 0:12:22 eta 0:06:02\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.7995 (1.9533) acc 62.5000 (52.7592) (mean 53.5056 many 53.5056 med nan few nan) lr 3.4549e-03 elapsed 0:12:22 eta 0:06:02\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.5073 (1.8549) acc 67.1875 (55.2554) (mean 54.7435 many 54.7435 med nan few nan) lr 3.4549e-03 elapsed 0:12:27 eta 0:05:57\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.5073 (1.8549) acc 67.1875 (55.2554) (mean 54.7435 many 54.7435 med nan few nan) lr 3.4549e-03 elapsed 0:12:27 eta 0:05:57\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/782] time 0.277 (0.282) data 0.000 (0.001) loss 1.5729 (1.8620) acc 62.5000 (55.3331) (mean 54.8823 many 54.8823 med nan few nan) lr 3.4549e-03 elapsed 0:12:33 eta 0:05:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/782] time 0.277 (0.282) data 0.000 (0.001) loss 1.5729 (1.8620) acc 62.5000 (55.3331) (mean 54.8823 many 54.8823 med nan few nan) lr 3.4549e-03 elapsed 0:12:33 eta 0:05:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/782] time 0.285 (0.282) data 0.001 (0.001) loss 2.0208 (1.9740) acc 56.2500 (53.9497) (mean 53.6522 many 53.6522 med nan few nan) lr 3.4549e-03 elapsed 0:12:39 eta 0:05:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/782] time 0.285 (0.282) data 0.001 (0.001) loss 2.0208 (1.9740) acc 56.2500 (53.9497) (mean 53.6522 many 53.6522 med nan few nan) lr 3.4549e-03 elapsed 0:12:39 eta 0:05:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/782] time 0.278 (0.282) data 0.000 (0.001) loss 2.1251 (1.8599) acc 46.8750 (56.5066) (mean 56.7698 many 56.7698 med nan few nan) lr 3.4549e-03 elapsed 0:12:44 eta 0:05:40\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/782] time 0.278 (0.282) data 0.000 (0.001) loss 2.1251 (1.8599) acc 46.8750 (56.5066) (mean 56.7698 many 56.7698 med nan few nan) lr 3.4549e-03 elapsed 0:12:44 eta 0:05:40\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/782] time 0.284 (0.282) data 0.000 (0.001) loss 2.1514 (1.9403) acc 50.0000 (53.7788) (mean 54.5527 many 54.5527 med nan few nan) lr 3.4549e-03 elapsed 0:12:50 eta 0:05:34\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/782] time 0.284 (0.282) data 0.000 (0.001) loss 2.1514 (1.9403) acc 50.0000 (53.7788) (mean 54.5527 many 54.5527 med nan few nan) lr 3.4549e-03 elapsed 0:12:50 eta 0:05:34\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/782] time 0.279 (0.282) data 0.004 (0.001) loss 1.4682 (1.8239) acc 64.0625 (56.7613) (mean 56.0452 many 56.0452 med nan few nan) lr 3.4549e-03 elapsed 0:12:55 eta 0:05:28\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/782] time 0.279 (0.282) data 0.004 (0.001) loss 1.4682 (1.8239) acc 64.0625 (56.7613) (mean 56.0452 many 56.0452 med nan few nan) lr 3.4549e-03 elapsed 0:12:55 eta 0:05:28\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/782] time 0.282 (0.282) data 0.001 (0.001) loss 1.8376 (1.9495) acc 53.1250 (53.4286) (mean 54.4873 many 54.4873 med nan few nan) lr 3.4549e-03 elapsed 0:13:01 eta 0:05:23\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/782] time 0.282 (0.282) data 0.001 (0.001) loss 1.8376 (1.9495) acc 53.1250 (53.4286) (mean 54.4873 many 54.4873 med nan few nan) lr 3.4549e-03 elapsed 0:13:01 eta 0:05:23\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/782] time 0.272 (0.282) data 0.000 (0.001) loss 1.8041 (1.8299) acc 62.5000 (57.6308) (mean 57.2679 many 57.2679 med nan few nan) lr 3.4549e-03 elapsed 0:13:07 eta 0:05:17\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/782] time 0.272 (0.282) data 0.000 (0.001) loss 1.8041 (1.8299) acc 62.5000 (57.6308) (mean 57.2679 many 57.2679 med nan few nan) lr 3.4549e-03 elapsed 0:13:07 eta 0:05:17\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.8470 (1.9011) acc 51.5625 (55.3232) (mean 56.0283 many 56.0283 med nan few nan) lr 3.4549e-03 elapsed 0:13:12 eta 0:05:11\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.8470 (1.9011) acc 51.5625 (55.3232) (mean 56.0283 many 56.0283 med nan few nan) lr 3.4549e-03 elapsed 0:13:12 eta 0:05:11\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/782] time 0.279 (0.282) data 0.000 (0.001) loss 1.8786 (1.9289) acc 60.9375 (55.5240) (mean 55.1211 many 55.1211 med nan few nan) lr 3.4549e-03 elapsed 0:13:18 eta 0:05:06\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/782] time 0.279 (0.282) data 0.000 (0.001) loss 1.8786 (1.9289) acc 60.9375 (55.5240) (mean 55.1211 many 55.1211 med nan few nan) lr 3.4549e-03 elapsed 0:13:18 eta 0:05:06\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/782] time 0.281 (0.282) data 0.000 (0.001) loss 2.1338 (1.9115) acc 46.8750 (54.1249) (mean 53.8416 many 53.8416 med nan few nan) lr 3.4549e-03 elapsed 0:13:24 eta 0:05:00\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/782] time 0.281 (0.282) data 0.000 (0.001) loss 2.1338 (1.9115) acc 46.8750 (54.1249) (mean 53.8416 many 53.8416 med nan few nan) lr 3.4549e-03 elapsed 0:13:24 eta 0:05:00\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/782] time 0.277 (0.282) data 0.000 (0.001) loss 1.4698 (1.8979) acc 68.7500 (55.5128) (mean 54.3379 many 54.3379 med nan few nan) lr 3.4549e-03 elapsed 0:13:29 eta 0:04:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/782] time 0.277 (0.282) data 0.000 (0.001) loss 1.4698 (1.8979) acc 68.7500 (55.5128) (mean 54.3379 many 54.3379 med nan few nan) lr 3.4549e-03 elapsed 0:13:29 eta 0:04:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.6790 (1.9162) acc 60.9375 (54.6036) (mean 54.7415 many 54.7415 med nan few nan) lr 3.4549e-03 elapsed 0:13:35 eta 0:04:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.6790 (1.9162) acc 60.9375 (54.6036) (mean 54.7415 many 54.7415 med nan few nan) lr 3.4549e-03 elapsed 0:13:35 eta 0:04:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/782] time 0.281 (0.282) data 0.000 (0.001) loss 2.0548 (1.9392) acc 48.4375 (53.3374) (mean 53.2860 many 53.2860 med nan few nan) lr 3.4549e-03 elapsed 0:13:40 eta 0:04:43\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/782] time 0.281 (0.282) data 0.000 (0.001) loss 2.0548 (1.9392) acc 48.4375 (53.3374) (mean 53.2860 many 53.2860 med nan few nan) lr 3.4549e-03 elapsed 0:13:40 eta 0:04:43\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/782] time 0.282 (0.282) data 0.001 (0.001) loss 1.8770 (1.9502) acc 54.6875 (53.8070) (mean 53.8716 many 53.8716 med nan few nan) lr 3.4549e-03 elapsed 0:13:46 eta 0:04:37\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/782] time 0.282 (0.282) data 0.001 (0.001) loss 1.8770 (1.9502) acc 54.6875 (53.8070) (mean 53.8716 many 53.8716 med nan few nan) lr 3.4549e-03 elapsed 0:13:46 eta 0:04:37\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/782] time 0.280 (0.282) data 0.000 (0.001) loss 2.1166 (1.8991) acc 56.2500 (55.9925) (mean 55.0246 many 55.0246 med nan few nan) lr 3.4549e-03 elapsed 0:13:52 eta 0:04:32\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/782] time 0.280 (0.282) data 0.000 (0.001) loss 2.1166 (1.8991) acc 56.2500 (55.9925) (mean 55.0246 many 55.0246 med nan few nan) lr 3.4549e-03 elapsed 0:13:52 eta 0:04:32\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/782] time 0.285 (0.282) data 0.000 (0.001) loss 2.0508 (1.9522) acc 51.5625 (53.5402) (mean 54.4993 many 54.4993 med nan few nan) lr 3.4549e-03 elapsed 0:13:57 eta 0:04:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/782] time 0.285 (0.282) data 0.000 (0.001) loss 2.0508 (1.9522) acc 51.5625 (53.5402) (mean 54.4993 many 54.4993 med nan few nan) lr 3.4549e-03 elapsed 0:13:57 eta 0:04:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [640/782] time 0.276 (0.282) data 0.000 (0.001) loss 1.8236 (1.8662) acc 56.2500 (56.1290) (mean 56.3121 many 56.3121 med nan few nan) lr 3.4549e-03 elapsed 0:14:03 eta 0:04:20\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [640/782] time 0.276 (0.282) data 0.000 (0.001) loss 1.8236 (1.8662) acc 56.2500 (56.1290) (mean 56.3121 many 56.3121 med nan few nan) lr 3.4549e-03 elapsed 0:14:03 eta 0:04:20\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [660/782] time 0.281 (0.282) data 0.000 (0.001) loss 2.1864 (1.9076) acc 51.5625 (54.7292) (mean 55.2210 many 55.2210 med nan few nan) lr 3.4549e-03 elapsed 0:14:08 eta 0:04:15\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [660/782] time 0.281 (0.282) data 0.000 (0.001) loss 2.1864 (1.9076) acc 51.5625 (54.7292) (mean 55.2210 many 55.2210 med nan few nan) lr 3.4549e-03 elapsed 0:14:08 eta 0:04:15\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [680/782] time 0.275 (0.282) data 0.000 (0.001) loss 1.7869 (1.9060) acc 53.1250 (53.2059) (mean 53.8176 many 53.8176 med nan few nan) lr 3.4549e-03 elapsed 0:14:14 eta 0:04:09\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [680/782] time 0.275 (0.282) data 0.000 (0.001) loss 1.7869 (1.9060) acc 53.1250 (53.2059) (mean 53.8176 many 53.8176 med nan few nan) lr 3.4549e-03 elapsed 0:14:14 eta 0:04:09\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [700/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.5904 (1.8955) acc 59.3750 (55.3333) (mean 55.2444 many 55.2444 med nan few nan) lr 3.4549e-03 elapsed 0:14:20 eta 0:04:03\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [700/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.5904 (1.8955) acc 59.3750 (55.3333) (mean 55.2444 many 55.2444 med nan few nan) lr 3.4549e-03 elapsed 0:14:20 eta 0:04:03\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [720/782] time 0.278 (0.282) data 0.002 (0.001) loss 1.9986 (1.9263) acc 45.3125 (53.0785) (mean 53.5629 many 53.5629 med nan few nan) lr 3.4549e-03 elapsed 0:14:25 eta 0:03:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [720/782] time 0.278 (0.282) data 0.002 (0.001) loss 1.9986 (1.9263) acc 45.3125 (53.0785) (mean 53.5629 many 53.5629 med nan few nan) lr 3.4549e-03 elapsed 0:14:25 eta 0:03:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [740/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.6787 (1.9164) acc 62.5000 (54.6026) (mean 54.4981 many 54.4981 med nan few nan) lr 3.4549e-03 elapsed 0:14:31 eta 0:03:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [740/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.6787 (1.9164) acc 62.5000 (54.6026) (mean 54.4981 many 54.4981 med nan few nan) lr 3.4549e-03 elapsed 0:14:31 eta 0:03:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [760/782] time 0.285 (0.282) data 0.003 (0.001) loss 1.9378 (1.8827) acc 59.3750 (56.7281) (mean 56.6183 many 56.6183 med nan few nan) lr 3.4549e-03 elapsed 0:14:36 eta 0:03:46\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [760/782] time 0.285 (0.282) data 0.003 (0.001) loss 1.9378 (1.8827) acc 59.3750 (56.7281) (mean 56.6183 many 56.6183 med nan few nan) lr 3.4549e-03 elapsed 0:14:36 eta 0:03:46\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [780/782] time 0.275 (0.282) data 0.000 (0.001) loss 2.0838 (1.9231) acc 46.8750 (53.7864) (mean 54.1986 many 54.1986 med nan few nan) lr 3.4549e-03 elapsed 0:14:42 eta 0:03:41\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [780/782] time 0.275 (0.282) data 0.000 (0.001) loss 2.0838 (1.9231) acc 46.8750 (53.7864) (mean 54.1986 many 54.1986 med nan few nan) lr 3.4549e-03 elapsed 0:14:42 eta 0:03:41\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [782/782] time 0.274 (0.282) data 0.000 (0.001) loss 3.0214 (2.0783) acc 6.2500 (48.1295) (mean 53.2118 many 53.2118 med nan few nan) lr 3.4549e-03 elapsed 0:14:42 eta 0:03:40\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [782/782] time 0.274 (0.282) data 0.000 (0.001) loss 3.0214 (2.0783) acc 6.2500 (48.1295) (mean 53.2118 many 53.2118 med nan few nan) lr 3.4549e-03 elapsed 0:14:42 eta 0:03:40\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.9928 (1.9229) acc 57.8125 (54.1014) (mean 54.4490 many 54.4490 med nan few nan) lr 9.5492e-04 elapsed 0:14:48 eta 0:03:35\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.9928 (1.9229) acc 57.8125 (54.1014) (mean 54.4490 many 54.4490 med nan few nan) lr 9.5492e-04 elapsed 0:14:48 eta 0:03:35\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/782] time 0.279 (0.282) data 0.000 (0.001) loss 1.8117 (1.8903) acc 57.8125 (53.7419) (mean 53.9815 many 53.9815 med nan few nan) lr 9.5492e-04 elapsed 0:14:54 eta 0:03:29\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/782] time 0.279 (0.282) data 0.000 (0.001) loss 1.8117 (1.8903) acc 57.8125 (53.7419) (mean 53.9815 many 53.9815 med nan few nan) lr 9.5492e-04 elapsed 0:14:54 eta 0:03:29\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.8960 (1.8910) acc 46.8750 (54.2552) (mean 54.8223 many 54.8223 med nan few nan) lr 9.5492e-04 elapsed 0:15:00 eta 0:03:23\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.8960 (1.8910) acc 46.8750 (54.2552) (mean 54.8223 many 54.8223 med nan few nan) lr 9.5492e-04 elapsed 0:15:00 eta 0:03:23\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/782] time 0.269 (0.282) data 0.000 (0.001) loss 1.8209 (1.9596) acc 51.5625 (53.5738) (mean 54.8031 many 54.8031 med nan few nan) lr 9.5492e-04 elapsed 0:15:05 eta 0:03:18\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/782] time 0.269 (0.282) data 0.000 (0.001) loss 1.8209 (1.9596) acc 51.5625 (53.5738) (mean 54.8031 many 54.8031 med nan few nan) lr 9.5492e-04 elapsed 0:15:05 eta 0:03:18\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.7820 (1.8951) acc 56.2500 (53.3689) (mean 53.7962 many 53.7962 med nan few nan) lr 9.5492e-04 elapsed 0:15:11 eta 0:03:12\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/782] time 0.282 (0.282) data 0.000 (0.001) loss 1.7820 (1.8951) acc 56.2500 (53.3689) (mean 53.7962 many 53.7962 med nan few nan) lr 9.5492e-04 elapsed 0:15:11 eta 0:03:12\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/782] time 0.278 (0.282) data 0.000 (0.001) loss 2.2268 (1.9205) acc 42.1875 (54.9125) (mean 55.1196 many 55.1196 med nan few nan) lr 9.5492e-04 elapsed 0:15:16 eta 0:03:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/782] time 0.278 (0.282) data 0.000 (0.001) loss 2.2268 (1.9205) acc 42.1875 (54.9125) (mean 55.1196 many 55.1196 med nan few nan) lr 9.5492e-04 elapsed 0:15:16 eta 0:03:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/782] time 0.280 (0.282) data 0.000 (0.001) loss 2.0084 (1.9346) acc 50.0000 (53.0059) (mean 54.5380 many 54.5380 med nan few nan) lr 9.5492e-04 elapsed 0:15:22 eta 0:03:01\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/782] time 0.280 (0.282) data 0.000 (0.001) loss 2.0084 (1.9346) acc 50.0000 (53.0059) (mean 54.5380 many 54.5380 med nan few nan) lr 9.5492e-04 elapsed 0:15:22 eta 0:03:01\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/782] time 0.276 (0.282) data 0.000 (0.001) loss 1.5763 (1.9134) acc 62.5000 (53.7744) (mean 54.5411 many 54.5411 med nan few nan) lr 9.5492e-04 elapsed 0:15:28 eta 0:02:55\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/782] time 0.276 (0.282) data 0.000 (0.001) loss 1.5763 (1.9134) acc 62.5000 (53.7744) (mean 54.5411 many 54.5411 med nan few nan) lr 9.5492e-04 elapsed 0:15:28 eta 0:02:55\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.6181 (1.8671) acc 68.7500 (55.5711) (mean 55.1605 many 55.1605 med nan few nan) lr 9.5492e-04 elapsed 0:15:33 eta 0:02:49\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.6181 (1.8671) acc 68.7500 (55.5711) (mean 55.1605 many 55.1605 med nan few nan) lr 9.5492e-04 elapsed 0:15:33 eta 0:02:49\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/782] time 0.286 (0.282) data 0.000 (0.001) loss 1.9424 (1.8806) acc 48.4375 (55.1237) (mean 54.9162 many 54.9162 med nan few nan) lr 9.5492e-04 elapsed 0:15:39 eta 0:02:44\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/782] time 0.286 (0.282) data 0.000 (0.001) loss 1.9424 (1.8806) acc 48.4375 (55.1237) (mean 54.9162 many 54.9162 med nan few nan) lr 9.5492e-04 elapsed 0:15:39 eta 0:02:44\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.5643 (1.8831) acc 64.0625 (54.9422) (mean 53.8544 many 53.8544 med nan few nan) lr 9.5492e-04 elapsed 0:15:45 eta 0:02:38\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.5643 (1.8831) acc 64.0625 (54.9422) (mean 53.8544 many 53.8544 med nan few nan) lr 9.5492e-04 elapsed 0:15:45 eta 0:02:38\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [240/782] time 0.276 (0.282) data 0.000 (0.001) loss 1.8047 (1.9261) acc 56.2500 (53.7413) (mean 53.6412 many 53.6412 med nan few nan) lr 9.5492e-04 elapsed 0:15:50 eta 0:02:32\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [240/782] time 0.276 (0.282) data 0.000 (0.001) loss 1.8047 (1.9261) acc 56.2500 (53.7413) (mean 53.6412 many 53.6412 med nan few nan) lr 9.5492e-04 elapsed 0:15:50 eta 0:02:32\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [260/782] time 0.288 (0.282) data 0.000 (0.001) loss 1.8530 (1.8503) acc 51.5625 (56.7892) (mean 56.6780 many 56.6780 med nan few nan) lr 9.5492e-04 elapsed 0:15:56 eta 0:02:27\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [260/782] time 0.288 (0.282) data 0.000 (0.001) loss 1.8530 (1.8503) acc 51.5625 (56.7892) (mean 56.6780 many 56.6780 med nan few nan) lr 9.5492e-04 elapsed 0:15:56 eta 0:02:27\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [280/782] time 0.279 (0.282) data 0.000 (0.001) loss 2.0536 (1.8871) acc 45.3125 (54.1444) (mean 54.7834 many 54.7834 med nan few nan) lr 9.5492e-04 elapsed 0:16:01 eta 0:02:21\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [280/782] time 0.279 (0.282) data 0.000 (0.001) loss 2.0536 (1.8871) acc 45.3125 (54.1444) (mean 54.7834 many 54.7834 med nan few nan) lr 9.5492e-04 elapsed 0:16:01 eta 0:02:21\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [300/782] time 0.288 (0.282) data 0.000 (0.001) loss 1.8279 (1.8484) acc 57.8125 (55.4525) (mean 55.5657 many 55.5657 med nan few nan) lr 9.5492e-04 elapsed 0:16:07 eta 0:02:16\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [300/782] time 0.288 (0.282) data 0.000 (0.001) loss 1.8279 (1.8484) acc 57.8125 (55.4525) (mean 55.5657 many 55.5657 med nan few nan) lr 9.5492e-04 elapsed 0:16:07 eta 0:02:16\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [320/782] time 0.279 (0.282) data 0.000 (0.001) loss 1.8480 (1.8809) acc 56.2500 (54.5738) (mean 55.1754 many 55.1754 med nan few nan) lr 9.5492e-04 elapsed 0:16:13 eta 0:02:10\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [320/782] time 0.279 (0.282) data 0.000 (0.001) loss 1.8480 (1.8809) acc 56.2500 (54.5738) (mean 55.1754 many 55.1754 med nan few nan) lr 9.5492e-04 elapsed 0:16:13 eta 0:02:10\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [340/782] time 0.287 (0.282) data 0.000 (0.001) loss 1.8064 (1.8743) acc 59.3750 (55.1841) (mean 55.4672 many 55.4672 med nan few nan) lr 9.5492e-04 elapsed 0:16:18 eta 0:02:04\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [340/782] time 0.287 (0.282) data 0.000 (0.001) loss 1.8064 (1.8743) acc 59.3750 (55.1841) (mean 55.4672 many 55.4672 med nan few nan) lr 9.5492e-04 elapsed 0:16:18 eta 0:02:04\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [360/782] time 0.284 (0.282) data 0.000 (0.001) loss 2.0358 (1.9492) acc 56.2500 (53.5125) (mean 54.5977 many 54.5977 med nan few nan) lr 9.5492e-04 elapsed 0:16:24 eta 0:01:59\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [360/782] time 0.284 (0.282) data 0.000 (0.001) loss 2.0358 (1.9492) acc 56.2500 (53.5125) (mean 54.5977 many 54.5977 med nan few nan) lr 9.5492e-04 elapsed 0:16:24 eta 0:01:59\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [380/782] time 0.274 (0.282) data 0.000 (0.001) loss 2.0747 (1.8865) acc 43.7500 (55.0598) (mean 55.1348 many 55.1348 med nan few nan) lr 9.5492e-04 elapsed 0:16:30 eta 0:01:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [380/782] time 0.274 (0.282) data 0.000 (0.001) loss 2.0747 (1.8865) acc 43.7500 (55.0598) (mean 55.1348 many 55.1348 med nan few nan) lr 9.5492e-04 elapsed 0:16:30 eta 0:01:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [400/782] time 0.286 (0.282) data 0.000 (0.001) loss 1.6445 (1.8382) acc 62.5000 (56.7532) (mean 56.4546 many 56.4546 med nan few nan) lr 9.5492e-04 elapsed 0:16:35 eta 0:01:47\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [400/782] time 0.286 (0.282) data 0.000 (0.001) loss 1.6445 (1.8382) acc 62.5000 (56.7532) (mean 56.4546 many 56.4546 med nan few nan) lr 9.5492e-04 elapsed 0:16:35 eta 0:01:47\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [420/782] time 0.283 (0.282) data 0.000 (0.001) loss 1.8736 (1.8724) acc 56.2500 (56.2469) (mean 55.8207 many 55.8207 med nan few nan) lr 9.5492e-04 elapsed 0:16:41 eta 0:01:42\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [420/782] time 0.283 (0.282) data 0.000 (0.001) loss 1.8736 (1.8724) acc 56.2500 (56.2469) (mean 55.8207 many 55.8207 med nan few nan) lr 9.5492e-04 elapsed 0:16:41 eta 0:01:42\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [440/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.8855 (1.9392) acc 53.1250 (53.6299) (mean 53.9144 many 53.9144 med nan few nan) lr 9.5492e-04 elapsed 0:16:47 eta 0:01:36\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [440/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.8855 (1.9392) acc 53.1250 (53.6299) (mean 53.9144 many 53.9144 med nan few nan) lr 9.5492e-04 elapsed 0:16:47 eta 0:01:36\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [460/782] time 0.277 (0.282) data 0.000 (0.001) loss 2.0107 (1.8865) acc 54.6875 (56.5020) (mean 56.6306 many 56.6306 med nan few nan) lr 9.5492e-04 elapsed 0:16:52 eta 0:01:30\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [460/782] time 0.277 (0.282) data 0.000 (0.001) loss 2.0107 (1.8865) acc 54.6875 (56.5020) (mean 56.6306 many 56.6306 med nan few nan) lr 9.5492e-04 elapsed 0:16:52 eta 0:01:30\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [480/782] time 0.283 (0.282) data 0.003 (0.001) loss 1.5621 (1.8740) acc 68.7500 (56.9256) (mean 56.4355 many 56.4355 med nan few nan) lr 9.5492e-04 elapsed 0:16:58 eta 0:01:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [480/782] time 0.283 (0.282) data 0.003 (0.001) loss 1.5621 (1.8740) acc 68.7500 (56.9256) (mean 56.4355 many 56.4355 med nan few nan) lr 9.5492e-04 elapsed 0:16:58 eta 0:01:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [500/782] time 0.273 (0.282) data 0.000 (0.001) loss 1.9301 (1.8974) acc 59.3750 (54.6369) (mean 55.3775 many 55.3775 med nan few nan) lr 9.5492e-04 elapsed 0:17:04 eta 0:01:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [500/782] time 0.273 (0.282) data 0.000 (0.001) loss 1.9301 (1.8974) acc 59.3750 (54.6369) (mean 55.3775 many 55.3775 med nan few nan) lr 9.5492e-04 elapsed 0:17:04 eta 0:01:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [520/782] time 0.286 (0.282) data 0.000 (0.001) loss 1.5907 (1.9410) acc 65.6250 (53.0663) (mean 53.9922 many 53.9922 med nan few nan) lr 9.5492e-04 elapsed 0:17:09 eta 0:01:13\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [520/782] time 0.286 (0.282) data 0.000 (0.001) loss 1.5907 (1.9410) acc 65.6250 (53.0663) (mean 53.9922 many 53.9922 med nan few nan) lr 9.5492e-04 elapsed 0:17:09 eta 0:01:13\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [540/782] time 0.283 (0.282) data 0.000 (0.001) loss 1.8030 (1.9140) acc 54.6875 (54.6749) (mean 55.1723 many 55.1723 med nan few nan) lr 9.5492e-04 elapsed 0:17:15 eta 0:01:08\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [540/782] time 0.283 (0.282) data 0.000 (0.001) loss 1.8030 (1.9140) acc 54.6875 (54.6749) (mean 55.1723 many 55.1723 med nan few nan) lr 9.5492e-04 elapsed 0:17:15 eta 0:01:08\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [560/782] time 0.278 (0.282) data 0.000 (0.001) loss 1.7034 (1.8366) acc 57.8125 (56.3116) (mean 56.2146 many 56.2146 med nan few nan) lr 9.5492e-04 elapsed 0:17:20 eta 0:01:02\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [560/782] time 0.278 (0.282) data 0.000 (0.001) loss 1.7034 (1.8366) acc 57.8125 (56.3116) (mean 56.2146 many 56.2146 med nan few nan) lr 9.5492e-04 elapsed 0:17:20 eta 0:01:02\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [580/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.6494 (1.8881) acc 60.9375 (54.2876) (mean 54.0998 many 54.0998 med nan few nan) lr 9.5492e-04 elapsed 0:17:26 eta 0:00:56\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [580/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.6494 (1.8881) acc 60.9375 (54.2876) (mean 54.0998 many 54.0998 med nan few nan) lr 9.5492e-04 elapsed 0:17:26 eta 0:00:56\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [600/782] time 0.283 (0.282) data 0.000 (0.001) loss 1.9130 (1.9257) acc 51.5625 (53.7849) (mean 54.1080 many 54.1080 med nan few nan) lr 9.5492e-04 elapsed 0:17:32 eta 0:00:51\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [600/782] time 0.283 (0.282) data 0.000 (0.001) loss 1.9130 (1.9257) acc 51.5625 (53.7849) (mean 54.1080 many 54.1080 med nan few nan) lr 9.5492e-04 elapsed 0:17:32 eta 0:00:51\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [620/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.6789 (1.7778) acc 67.1875 (58.6602) (mean 57.7530 many 57.7530 med nan few nan) lr 9.5492e-04 elapsed 0:17:37 eta 0:00:45\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [620/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.6789 (1.7778) acc 67.1875 (58.6602) (mean 57.7530 many 57.7530 med nan few nan) lr 9.5492e-04 elapsed 0:17:37 eta 0:00:45\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [640/782] time 0.278 (0.282) data 0.000 (0.001) loss 1.8276 (1.8486) acc 64.0625 (56.1721) (mean 56.7825 many 56.7825 med nan few nan) lr 9.5492e-04 elapsed 0:17:43 eta 0:00:40\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [640/782] time 0.278 (0.282) data 0.000 (0.001) loss 1.8276 (1.8486) acc 64.0625 (56.1721) (mean 56.7825 many 56.7825 med nan few nan) lr 9.5492e-04 elapsed 0:17:43 eta 0:00:40\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [660/782] time 0.282 (0.282) data 0.004 (0.001) loss 1.9279 (1.8775) acc 57.8125 (57.5713) (mean 57.2153 many 57.2153 med nan few nan) lr 9.5492e-04 elapsed 0:17:49 eta 0:00:34\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [660/782] time 0.282 (0.282) data 0.004 (0.001) loss 1.9279 (1.8775) acc 57.8125 (57.5713) (mean 57.2153 many 57.2153 med nan few nan) lr 9.5492e-04 elapsed 0:17:49 eta 0:00:34\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [680/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.8390 (1.8089) acc 59.3750 (58.0375) (mean 57.7189 many 57.7189 med nan few nan) lr 9.5492e-04 elapsed 0:17:54 eta 0:00:28\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [680/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.8390 (1.8089) acc 59.3750 (58.0375) (mean 57.7189 many 57.7189 med nan few nan) lr 9.5492e-04 elapsed 0:17:54 eta 0:00:28\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [700/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.6600 (1.8560) acc 56.2500 (55.2609) (mean 55.6865 many 55.6865 med nan few nan) lr 9.5492e-04 elapsed 0:18:00 eta 0:00:23\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [700/782] time 0.280 (0.282) data 0.000 (0.001) loss 1.6600 (1.8560) acc 56.2500 (55.2609) (mean 55.6865 many 55.6865 med nan few nan) lr 9.5492e-04 elapsed 0:18:00 eta 0:00:23\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [720/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.6041 (1.9241) acc 59.3750 (53.8889) (mean 54.5485 many 54.5485 med nan few nan) lr 9.5492e-04 elapsed 0:18:05 eta 0:00:17\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [720/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.6041 (1.9241) acc 59.3750 (53.8889) (mean 54.5485 many 54.5485 med nan few nan) lr 9.5492e-04 elapsed 0:18:05 eta 0:00:17\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [740/782] time 0.278 (0.282) data 0.000 (0.001) loss 2.2171 (1.8444) acc 43.7500 (55.4157) (mean 55.4623 many 55.4623 med nan few nan) lr 9.5492e-04 elapsed 0:18:11 eta 0:00:11\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [740/782] time 0.278 (0.282) data 0.000 (0.001) loss 2.2171 (1.8444) acc 43.7500 (55.4157) (mean 55.4623 many 55.4623 med nan few nan) lr 9.5492e-04 elapsed 0:18:11 eta 0:00:11\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [760/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.4910 (1.8637) acc 59.3750 (55.8854) (mean 55.7336 many 55.7336 med nan few nan) lr 9.5492e-04 elapsed 0:18:17 eta 0:00:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [760/782] time 0.281 (0.282) data 0.000 (0.001) loss 1.4910 (1.8637) acc 59.3750 (55.8854) (mean 55.7336 many 55.7336 med nan few nan) lr 9.5492e-04 elapsed 0:18:17 eta 0:00:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [780/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.6122 (1.7804) acc 64.0625 (58.4397) (mean 57.9357 many 57.9357 med nan few nan) lr 9.5492e-04 elapsed 0:18:22 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [780/782] time 0.284 (0.282) data 0.000 (0.001) loss 1.6122 (1.7804) acc 64.0625 (58.4397) (mean 57.9357 many 57.9357 med nan few nan) lr 9.5492e-04 elapsed 0:18:22 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [782/782] time 0.277 (0.282) data 0.000 (0.001) loss 1.1471 (1.7597) acc 81.2500 (58.9768) (mean 57.3717 many 57.3717 med nan few nan) lr 9.5492e-04 elapsed 0:18:23 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [782/782] time 0.277 (0.282) data 0.000 (0.001) loss 1.1471 (1.7597) acc 81.2500 (58.9768) (mean 57.3717 many 57.3717 med nan few nan) lr 9.5492e-04 elapsed 0:18:23 eta 0:00:00\u001b[0m\n",
      "\u001b[37mFinished training\u001b[0m\n",
      "\u001b[37mNote: Printed training accuracy is approximate. Use test_train=True for precise evaluation.\u001b[0m\n",
      "\u001b[37mTotal training time: 0:18:23\u001b[0m\n",
      "\u001b[37mCheckpoint saved to /content/metalora/output/notebooks/cifar100_class_aware_svhn/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      "\u001b[37mFinished training\u001b[0m\n",
      "\u001b[37mNote: Printed training accuracy is approximate. Use test_train=True for precise evaluation.\u001b[0m\n",
      "\u001b[37mTotal training time: 0:18:23\u001b[0m\n",
      "\u001b[37mCheckpoint saved to /content/metalora/output/notebooks/cifar100_class_aware_svhn/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      " 10%|9         | 15/157 [00:11<01:51]\u001b[0m\n",
      " 10%|9         | 15/157 [00:11<01:51]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:23<01:37]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:23<01:37]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:34<01:25]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:34<01:25]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:46<01:14]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:46<01:14]\u001b[0m\n",
      " 48%|####7     | 75/157 [00:57<01:02]\u001b[0m\n",
      " 48%|####7     | 75/157 [00:57<01:02]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:08<00:51]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:08<00:51]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:20<00:39]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:20<00:39]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:31<00:28]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:31<00:28]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:42<00:16]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:42<00:16]\u001b[0m\n",
      " 96%|#########5| 150/157 [01:54<00:05]\u001b[0m\n",
      " 96%|#########5| 150/157 [01:54<00:05]\u001b[0m\n",
      "100%|##########| 157/157 [01:58<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,537\n",
      "* accuracy: 75.4%\n",
      "* error: 24.6%\n",
      "* macro_f1: 75.1%\u001b[0m\n",
      "\u001b[37m* class acc: [93. 90. 85. 71. 34. 75. 74. 75. 85. 97. 58. 70. 84. 79. 86. 86. 81. 93.\n",
      " 68. 86. 89. 88. 85. 75. 84. 65. 61. 48. 83. 76. 70. 79. 58. 60. 74. 73.\n",
      " 79. 87. 68. 88. 70. 84. 56. 85. 68. 37. 88. 56. 96. 89. 59. 76. 77. 98.\n",
      " 84. 39. 88. 85. 94. 61. 89. 78. 78. 51. 65. 86. 69. 56. 98. 87. 84. 92.\n",
      " 48. 67. 34. 77. 86. 75. 69. 78. 60. 70. 94. 79. 73. 83. 82. 92. 63. 93.\n",
      " 81. 77. 78. 54. 83. 65. 48. 73. 86. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 34.0%\n",
      "* hmean_acc: 71.7%\n",
      "* gmean_acc: 73.7%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.4%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.4%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.37, 'error_rate': 24.629999999999995, 'macro_f1': 75.06627794417255, 'worst_case_acc': 34.0, 'hmean_acc': np.float64(71.6529490506214), 'gmean_acc': np.float64(73.6936310278147), 'many_acc': np.float64(75.37), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.37)})\u001b[0m\n",
      "100%|##########| 157/157 [01:58<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,537\n",
      "* accuracy: 75.4%\n",
      "* error: 24.6%\n",
      "* macro_f1: 75.1%\u001b[0m\n",
      "\u001b[37m* class acc: [93. 90. 85. 71. 34. 75. 74. 75. 85. 97. 58. 70. 84. 79. 86. 86. 81. 93.\n",
      " 68. 86. 89. 88. 85. 75. 84. 65. 61. 48. 83. 76. 70. 79. 58. 60. 74. 73.\n",
      " 79. 87. 68. 88. 70. 84. 56. 85. 68. 37. 88. 56. 96. 89. 59. 76. 77. 98.\n",
      " 84. 39. 88. 85. 94. 61. 89. 78. 78. 51. 65. 86. 69. 56. 98. 87. 84. 92.\n",
      " 48. 67. 34. 77. 86. 75. 69. 78. 60. 70. 94. 79. 73. 83. 82. 92. 63. 93.\n",
      " 81. 77. 78. 54. 83. 65. 48. 73. 86. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 34.0%\n",
      "* hmean_acc: 71.7%\n",
      "* gmean_acc: 73.7%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.4%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.4%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.37, 'error_rate': 24.629999999999995, 'macro_f1': 75.06627794417255, 'worst_case_acc': 34.0, 'hmean_acc': np.float64(71.6529490506214), 'gmean_acc': np.float64(73.6936310278147), 'many_acc': np.float64(75.37), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.37)})\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RUN_TRAINING = True\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    trainer.train()\n",
    "else:\n",
    "    print(\"Skipping training; existing weights will be evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7483de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      " 10%|9         | 15/157 [00:11<01:50]\u001b[0m\n",
      " 10%|9         | 15/157 [00:11<01:50]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:23<01:37]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:23<01:37]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:34<01:25]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:34<01:25]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:45<01:13]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:45<01:13]\u001b[0m\n",
      " 48%|####7     | 75/157 [00:57<01:02]\u001b[0m\n",
      " 48%|####7     | 75/157 [00:57<01:02]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:08<00:51]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:08<00:51]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:20<00:39]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:20<00:39]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:31<00:28]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:31<00:28]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:42<00:16]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:42<00:16]\u001b[0m\n",
      " 96%|#########5| 150/157 [01:54<00:05]\u001b[0m\n",
      " 96%|#########5| 150/157 [01:54<00:05]\u001b[0m\n",
      "100%|##########| 157/157 [01:58<00:00]\u001b[0m\n",
      "100%|##########| 157/157 [01:58<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,537\n",
      "* accuracy: 75.4%\n",
      "* error: 24.6%\n",
      "* macro_f1: 75.1%\u001b[0m\n",
      "\u001b[37m* class acc: [93. 90. 85. 71. 34. 75. 74. 75. 85. 97. 58. 70. 84. 79. 86. 86. 81. 93.\n",
      " 68. 86. 89. 88. 85. 75. 84. 65. 61. 48. 83. 76. 70. 79. 58. 60. 74. 73.\n",
      " 79. 87. 68. 88. 70. 84. 56. 85. 68. 37. 88. 56. 96. 89. 59. 76. 77. 98.\n",
      " 84. 39. 88. 85. 94. 61. 89. 78. 78. 51. 65. 86. 69. 56. 98. 87. 84. 92.\n",
      " 48. 67. 34. 77. 86. 75. 69. 78. 60. 70. 94. 79. 73. 83. 82. 92. 63. 93.\n",
      " 81. 77. 78. 54. 83. 65. 48. 73. 86. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 34.0%\n",
      "* hmean_acc: 71.7%\n",
      "* gmean_acc: 73.7%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.4%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.4%\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,537\n",
      "* accuracy: 75.4%\n",
      "* error: 24.6%\n",
      "* macro_f1: 75.1%\u001b[0m\n",
      "\u001b[37m* class acc: [93. 90. 85. 71. 34. 75. 74. 75. 85. 97. 58. 70. 84. 79. 86. 86. 81. 93.\n",
      " 68. 86. 89. 88. 85. 75. 84. 65. 61. 48. 83. 76. 70. 79. 58. 60. 74. 73.\n",
      " 79. 87. 68. 88. 70. 84. 56. 85. 68. 37. 88. 56. 96. 89. 59. 76. 77. 98.\n",
      " 84. 39. 88. 85. 94. 61. 89. 78. 78. 51. 65. 86. 69. 56. 98. 87. 84. 92.\n",
      " 48. 67. 34. 77. 86. 75. 69. 78. 60. 70. 94. 79. 73. 83. 82. 92. 63. 93.\n",
      " 81. 77. 78. 54. 83. 65. 48. 73. 86. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 34.0%\n",
      "* hmean_acc: 71.7%\n",
      "* gmean_acc: 73.7%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.4%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.4%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.37, 'error_rate': 24.629999999999995, 'macro_f1': 75.06627794417255, 'worst_case_acc': 34.0, 'hmean_acc': np.float64(71.6529490506214), 'gmean_acc': np.float64(73.6936310278147), 'many_acc': np.float64(75.37), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.37)})\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.37, 'error_rate': 24.629999999999995, 'macro_f1': 75.06627794417255, 'worst_case_acc': 34.0, 'hmean_acc': np.float64(71.6529490506214), 'gmean_acc': np.float64(73.6936310278147), 'many_acc': np.float64(75.37), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.37)})\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,537\n",
      "* accuracy: 75.4%\n",
      "* error: 24.6%\n",
      "* macro_f1: 75.1%\u001b[0m\n",
      "\u001b[37m* class acc: [93. 90. 85. 71. 34. 75. 74. 75. 85. 97. 58. 70. 84. 79. 86. 86. 81. 93.\n",
      " 68. 86. 89. 88. 85. 75. 84. 65. 61. 48. 83. 76. 70. 79. 58. 60. 74. 73.\n",
      " 79. 87. 68. 88. 70. 84. 56. 85. 68. 37. 88. 56. 96. 89. 59. 76. 77. 98.\n",
      " 84. 39. 88. 85. 94. 61. 89. 78. 78. 51. 65. 86. 69. 56. 98. 87. 84. 92.\n",
      " 48. 67. 34. 77. 86. 75. 69. 78. 60. 70. 94. 79. 73. 83. 82. 92. 63. 93.\n",
      " 81. 77. 78. 54. 83. 65. 48. 73. 86. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 34.0%\n",
      "* hmean_acc: 71.7%\n",
      "* gmean_acc: 73.7%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,537\n",
      "* accuracy: 75.4%\n",
      "* error: 24.6%\n",
      "* macro_f1: 75.1%\u001b[0m\n",
      "\u001b[37m* class acc: [93. 90. 85. 71. 34. 75. 74. 75. 85. 97. 58. 70. 84. 79. 86. 86. 81. 93.\n",
      " 68. 86. 89. 88. 85. 75. 84. 65. 61. 48. 83. 76. 70. 79. 58. 60. 74. 73.\n",
      " 79. 87. 68. 88. 70. 84. 56. 85. 68. 37. 88. 56. 96. 89. 59. 76. 77. 98.\n",
      " 84. 39. 88. 85. 94. 61. 89. 78. 78. 51. 65. 86. 69. 56. 98. 87. 84. 92.\n",
      " 48. 67. 34. 77. 86. 75. 69. 78. 60. 70. 94. 79. 73. 83. 82. 92. 63. 93.\n",
      " 81. 77. 78. 54. 83. 65. 48. 73. 86. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 34.0%\n",
      "* hmean_acc: 71.7%\n",
      "* gmean_acc: 73.7%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.4%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.4%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.4%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.4%\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-531465214.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-531465214.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[37mid_accuracy: 75.3700\u001b[0m\n",
      "\u001b[37mid_accuracy: 75.3700\u001b[0m\n",
      "\u001b[37mmany_acc: 75.3700\u001b[0m\n",
      "\u001b[37mmed_acc: nan\u001b[0m\n",
      "\u001b[37mfew_acc: nan\u001b[0m\n",
      "\u001b[37mauroc: 0.8972\u001b[0m\n",
      "\u001b[37maupr: 0.8271\u001b[0m\n",
      "\u001b[37mfpr@95tpr: 0.5064\u001b[0m\n",
      "\u001b[37mthreshold@95tpr: 0.1396\u001b[0m\n",
      "\u001b[37mid_mean: 0.6023\u001b[0m\n",
      "\u001b[37mid_std: 0.2817\u001b[0m\n",
      "\u001b[37mood_mean: 0.1855\u001b[0m\n",
      "\u001b[37mood_std: 0.1456\u001b[0m\n",
      "\u001b[37mmany_acc: 75.3700\u001b[0m\n",
      "\u001b[37mmed_acc: nan\u001b[0m\n",
      "\u001b[37mfew_acc: nan\u001b[0m\n",
      "\u001b[37mauroc: 0.8972\u001b[0m\n",
      "\u001b[37maupr: 0.8271\u001b[0m\n",
      "\u001b[37mfpr@95tpr: 0.5064\u001b[0m\n",
      "\u001b[37mthreshold@95tpr: 0.1396\u001b[0m\n",
      "\u001b[37mid_mean: 0.6023\u001b[0m\n",
      "\u001b[37mid_std: 0.2817\u001b[0m\n",
      "\u001b[37mood_mean: 0.1855\u001b[0m\n",
      "\u001b[37mood_std: 0.1456\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "id_acc_scalar = float(trainer.test())\n",
    "id_metrics = trainer.evaluator.evaluate()\n",
    "ood_metrics = trainer.evaluate_ood()\n",
    "\n",
    "summary = {\n",
    "    \"id_accuracy\": id_acc_scalar,\n",
    "    \"many_acc\": id_metrics.get(\"many_acc\"),\n",
    "    \"med_acc\": id_metrics.get(\"med_acc\"),\n",
    "    \"few_acc\": id_metrics.get(\"few_acc\"),\n",
    "}\n",
    "summary.update(ood_metrics)\n",
    "\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    elif value is None:\n",
    "        print(f\"{key}: N/A\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1cb96b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mAUPR (ID positive): 0.8272\u001b[0m\n",
      "\u001b[37mAUPR (OOD positive): 0.9531\u001b[0m\n",
      "\u001b[37mAUPR (OOD positive): 0.9531\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "if trainer.last_ood_scores is None:\n",
    "    raise RuntimeError(\"Run trainer.evaluate_ood() before computing precision curves.\")\n",
    "\n",
    "id_scores = np.asarray(trainer.last_ood_scores[\"id\"], dtype=np.float32)\n",
    "ood_scores = np.asarray(trainer.last_ood_scores[\"ood\"], dtype=np.float32)\n",
    "labels_id = np.concatenate([np.ones_like(id_scores), np.zeros_like(ood_scores)])\n",
    "scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "prec_id, rec_id, _ = precision_recall_curve(labels_id, scores)\n",
    "aupr_id = auc(rec_id, prec_id)\n",
    "\n",
    "labels_ood = 1 - labels_id\n",
    "prec_ood, rec_ood, _ = precision_recall_curve(labels_ood, -scores)\n",
    "aupr_ood = auc(rec_ood, prec_ood)\n",
    "\n",
    "print(f\"AUPR (ID positive): {aupr_id:.4f}\")\n",
    "print(f\"AUPR (OOD positive): {aupr_ood:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d373c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
