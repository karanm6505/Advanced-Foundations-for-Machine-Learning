{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec7ea42",
   "metadata": {},
   "source": [
    "# Class-Aware OOD with CIFAR-100\n",
    "This walkthrough configures MetaLoRA for class-aware sampling on CIFAR-100 and evaluates out-of-distribution robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84dabe1",
   "metadata": {},
   "source": [
    "- Configure paths, dependencies, and experiment settings.\n",
    "3- Train on all CIFAR-100 classes with a class-aware sampler.\n",
    "- Use SVHN as a truly OOD benchmark while reusing the CIFAR-trained head.\n",
    "- Report both in-distribution accuracy and SVHN OOD metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78698751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100 as TorchvisionCIFAR100, SVHN as TorchvisionSVHN\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path, marker: str = \"main.py\", max_depth: int = 10) -> Path:\n",
    "    \"\"\"Locate the repository root when running locally or inside Colab.\"\"\"\n",
    "    env_root = os.environ.get(\"METALORA_ROOT\") or os.environ.get(\"REPO_ROOT\")\n",
    "    if env_root:\n",
    "        candidate = Path(env_root).expanduser().resolve()\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    current = start.resolve()\n",
    "    for _ in range(max_depth):\n",
    "        if (current / marker).exists():\n",
    "            return current\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    colab_candidate = Path(\"/content/metalora\").resolve()\n",
    "    if (colab_candidate / marker).exists():\n",
    "        return colab_candidate\n",
    "    if Path(\"/content\").exists():\n",
    "        print(\"Repository not found; attempting to clone into /content/metalora ...\")\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"https://github.com/doem97/metalora.git\",\n",
    "                str(colab_candidate),\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        if (colab_candidate / marker).exists():\n",
    "            return colab_candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate {marker}. Set METALORA_ROOT to the repo path or clone it under /content/metalora.\"\n",
    "    )\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "if \"datasets\" in sys.modules:\n",
    "    del sys.modules[\"datasets\"]\n",
    "os.chdir(REPO_ROOT)\n",
    "\n",
    "import datasets\n",
    "from trainer import (\n",
    "    CLASS_MEAN_FNAME,\n",
    "    TEXT_FEAT_FNAME,\n",
    "    Trainer,\n",
    "    load_clip_to_cpu,\n",
    "    load_vit_to_cpu,\n",
    "    )\n",
    "from models import PeftModelFromCLIP, PeftModelFromViT, ZeroShotCLIP\n",
    "from models.satmae_vit import MAEViTAdapter\n",
    "from utils.config_omega import cfg as base_cfg\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.logger import logger\n",
    "from utils.samplers import ClassAwareSampler, DownSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc52e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_config(repo_root, dataset_name, model_name, tuner_name=None, overrides=None):\n",
    "    config = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n",
    "    config_paths = [\n",
    "        repo_root / \"configs\" / \"data\" / f\"{dataset_name}.yaml\",\n",
    "        repo_root / \"configs\" / \"model\" / f\"{model_name}.yaml\",\n",
    "    ]\n",
    "    if tuner_name:\n",
    "        config_paths.append(repo_root / \"configs\" / \"tuner\" / f\"{tuner_name}.yaml\")\n",
    "    for path in config_paths:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(path)\n",
    "        config = OmegaConf.merge(config, OmegaConf.load(path))\n",
    "    if overrides:\n",
    "        config = OmegaConf.merge(config, OmegaConf.create(overrides))\n",
    "    return config\n",
    "\n",
    "\n",
    "def make_cifar_subset(dataset_cls, root, train, transform, class_indices, remap=True):\n",
    "    keep = sorted(class_indices)\n",
    "    dataset = dataset_cls(root, train=train, transform=transform)\n",
    "    targets = list(dataset.targets)\n",
    "    indices = [idx for idx, label in enumerate(targets) if label in keep]\n",
    "    if len(indices) == 0:\n",
    "        raise ValueError(\"No samples found for the provided classes.\")\n",
    "    index_array = np.array(indices, dtype=np.int64)\n",
    "    dataset.data = dataset.data[index_array]\n",
    "    selected_targets = [targets[idx] for idx in indices]\n",
    "    dataset.original_targets = selected_targets.copy()\n",
    "    subset_classnames = [dataset.classes[idx] for idx in keep]\n",
    "    if remap:\n",
    "        label_map = {orig: new_idx for new_idx, orig in enumerate(keep)}\n",
    "        remapped_targets = [label_map[label] for label in selected_targets]\n",
    "        dataset.targets = remapped_targets\n",
    "        dataset.labels = remapped_targets\n",
    "        dataset.classes = subset_classnames\n",
    "        dataset.classnames = subset_classnames\n",
    "        dataset.class_to_idx = {name: idx for idx, name in enumerate(subset_classnames)}\n",
    "        dataset.label_map = label_map\n",
    "        dataset.inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    else:\n",
    "        dataset.targets = selected_targets\n",
    "        dataset.labels = selected_targets\n",
    "        dataset.classnames = subset_classnames\n",
    "        dataset.label_map = None\n",
    "        dataset.inverse_label_map = None\n",
    "    if hasattr(dataset, \"get_cls_num_list\"):\n",
    "        dataset.cls_num_list = dataset.get_cls_num_list()\n",
    "        dataset.num_classes = len(dataset.cls_num_list)\n",
    "    else:\n",
    "        dataset.num_classes = len(subset_classnames)\n",
    "        dataset.cls_num_list = [dataset.targets.count(i) for i in range(dataset.num_classes)]\n",
    "    dataset.keep_classes = keep\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_ood_metrics(id_scores, ood_scores, tpr=0.95):\n",
    "    id_scores = np.asarray(id_scores, dtype=np.float32)\n",
    "    ood_scores = np.asarray(ood_scores, dtype=np.float32)\n",
    "    if id_scores.size == 0 or ood_scores.size == 0:\n",
    "        raise ValueError(\"Need non-empty ID and OOD score arrays.\")\n",
    "    labels = np.concatenate([np.ones_like(id_scores), np.zeros_like(ood_scores)])\n",
    "    scores = np.concatenate([id_scores, ood_scores])\n",
    "    threshold = np.percentile(id_scores, (1.0 - tpr) * 100.0)\n",
    "    metrics = {\n",
    "        \"auroc\": float(roc_auc_score(labels, scores)),\n",
    "        \"aupr\": float(average_precision_score(labels, scores)),\n",
    "        \"fpr@95tpr\": float(np.mean(ood_scores >= threshold)),\n",
    "        \"threshold@95tpr\": float(threshold),\n",
    "        \"id_mean\": float(id_scores.mean()),\n",
    "        \"id_std\": float(id_scores.std()),\n",
    "        \"ood_mean\": float(ood_scores.mean()),\n",
    "        \"ood_std\": float(ood_scores.std()),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAwareOODTrainer(Trainer):\n",
    "    def __init__(self, cfg, device, id_classes, ood_classes=None, class_aware_k=4):\n",
    "        self.id_classes = sorted(set(id_classes))\n",
    "        self.ood_classes = sorted(set(ood_classes or []))\n",
    "        self.external_ood_name = getattr(cfg, \"ood_dataset\", None)\n",
    "        self.external_ood_name = (\n",
    "            self.external_ood_name.lower() if self.external_ood_name else None\n",
    "        )\n",
    "        overlap = set(self.id_classes) & set(self.ood_classes)\n",
    "        if overlap and not self.external_ood_name:\n",
    "            raise ValueError(\n",
    "                f\"In-distribution and OOD classes overlap: {sorted(overlap)}\"\n",
    "            )\n",
    "        self.class_aware_k = class_aware_k\n",
    "        self.use_class_aware_sampler = class_aware_k is not None and class_aware_k > 0\n",
    "        self.training_history = []\n",
    "        super().__init__(cfg, device)\n",
    "        self.local_rank = 0\n",
    "        self.world_size = 1\n",
    "        self.ood_test_loader = None\n",
    "        root_hint = Path(cfg.root or os.environ.get(\"CIFAR100_ROOT\", \"./data\")).expanduser()\n",
    "        class_names = getattr(TorchvisionCIFAR100, \"classes\", None)\n",
    "        if class_names is None:\n",
    "            preview_dataset = TorchvisionCIFAR100(\n",
    "                root=str(root_hint), train=True, download=True\n",
    "            )\n",
    "            class_names = preview_dataset.classes\n",
    "        self.global_classnames = class_names\n",
    "        self.id_classnames = [class_names[idx] for idx in self.id_classes]\n",
    "        if self.external_ood_name:\n",
    "            self.ood_classnames = None\n",
    "        else:\n",
    "            self.ood_classnames = [class_names[idx] for idx in self.ood_classes]\n",
    "        self.last_ood_scores = None\n",
    "\n",
    "    def build_data_loader(self):\n",
    "        cfg = self.cfg\n",
    "        root = cfg.root\n",
    "        resolution = cfg.resolution\n",
    "\n",
    "        if cfg.backbone.startswith(\"CLIP\"):\n",
    "            mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "            std = [0.26862954, 0.26130258, 0.27577711]\n",
    "        else:\n",
    "            mean = [0.5, 0.5, 0.5]\n",
    "            std = [0.5, 0.5, 0.5]\n",
    "\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(resolution),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        transform_plain = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resolution),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resolution * 8 // 7),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.Lambda(\n",
    "                    lambda crop: torch.stack([transforms.ToTensor()(crop)])\n",
    "                ),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        dataset_cls = getattr(datasets, cfg.dataset)\n",
    "        train_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_train, self.id_classes, remap=True\n",
    "        )\n",
    "        train_init_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_plain, self.id_classes, remap=True\n",
    "        )\n",
    "        train_test_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_test, self.id_classes, remap=True\n",
    "        )\n",
    "        id_test_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, False, transform_test, self.id_classes, remap=True\n",
    "        )\n",
    "\n",
    "        if self.external_ood_name:\n",
    "            ood_test_dataset = self._build_external_ood_dataset(transform_test)\n",
    "        else:\n",
    "            if not self.ood_classes:\n",
    "                raise ValueError(\n",
    "                    \"No OOD classes specified and external OOD dataset not provided.\"\n",
    "                )\n",
    "            ood_test_dataset = make_cifar_subset(\n",
    "                dataset_cls, root, False, transform_test, self.ood_classes, remap=False\n",
    "            )\n",
    "\n",
    "        self.num_classes = train_dataset.num_classes\n",
    "        self.cls_num_list = train_dataset.cls_num_list\n",
    "        self.classnames = train_dataset.classnames\n",
    "\n",
    "        freq = np.array(self.cls_num_list)\n",
    "        self.many_idxs = np.where(freq > 100)[0]\n",
    "        self.med_idxs = np.where((freq >= 20) & (freq <= 100))[0]\n",
    "        self.few_idxs = np.where(freq < 20)[0]\n",
    "\n",
    "        if cfg.init_head == \"1_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=1)\n",
    "        elif cfg.init_head == \"10_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=10)\n",
    "        elif cfg.init_head == \"100_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=100)\n",
    "        else:\n",
    "            init_sampler = None\n",
    "\n",
    "        self.accum_step = cfg.accum_step or 1\n",
    "        self.eff_batch_size = cfg.batch_size\n",
    "        denom = self.accum_step * self.world_size\n",
    "        if self.eff_batch_size % denom != 0:\n",
    "            raise ValueError(\n",
    "                f\"batch_size ({cfg.batch_size}) must be divisible by accum_step ({self.accum_step}).\"\n",
    "            )\n",
    "        self.per_gpu_batch_size = self.eff_batch_size // denom\n",
    "\n",
    "        if self.use_class_aware_sampler:\n",
    "            train_sampler = ClassAwareSampler(\n",
    "                train_dataset, num_samples_cls=self.class_aware_k\n",
    "            )\n",
    "            shuffle_train = False\n",
    "        else:\n",
    "            train_sampler = None\n",
    "            shuffle_train = True\n",
    "        sampler_desc = (\n",
    "            \"class-aware sampler\" if train_sampler else \"standard shuffled batches\"\n",
    "        )\n",
    "        print(f\"Train sampler: {sampler_desc}\")\n",
    "\n",
    "        pin = self.device.type == \"cuda\"\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.per_gpu_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            shuffle=shuffle_train,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.train_init_loader = DataLoader(\n",
    "            train_init_dataset,\n",
    "            batch_size=min(64, len(train_init_dataset)),\n",
    "            sampler=init_sampler,\n",
    "            shuffle=init_sampler is None,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.train_test_loader = DataLoader(\n",
    "            train_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            id_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.ood_test_loader = DataLoader(\n",
    "            ood_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        ood_desc = (\n",
    "            f\"OOD dataset ({self.external_ood_name.upper()}): {len(ood_test_dataset)} samples\"\n",
    "            if self.external_ood_name\n",
    "            else f\"OOD samples: {len(ood_test_dataset)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train samples: {len(train_dataset)} | ID classes: {len(self.id_classes)} | {ood_desc}\"\n",
    "        )\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.classnames\n",
    "        num_classes = len(classnames)\n",
    "\n",
    "        if cfg.backbone.startswith(\"CLIP\"):\n",
    "            clip_model = load_clip_to_cpu(cfg.backbone, cfg.prec)\n",
    "            if cfg.zero_shot:\n",
    "                self.model = ZeroShotCLIP(clip_model)\n",
    "                self.model.to(self.device)\n",
    "                self.tuner = None\n",
    "                self.head = None\n",
    "                template = \"a photo of a {}.\"\n",
    "                prompts = self.get_tokenized_prompts(classnames, template)\n",
    "                self.model.init_text_features(prompts)\n",
    "                return\n",
    "            self.model = PeftModelFromCLIP(cfg, clip_model, num_classes)\n",
    "        elif cfg.backbone.startswith(\"IN21K-ViT\"):\n",
    "            vit_model = load_vit_to_cpu(cfg.backbone, cfg.prec)\n",
    "            self.model = PeftModelFromViT(cfg, vit_model, num_classes)\n",
    "        elif cfg.backbone.startswith(\"SatMAE-ViT\"):\n",
    "            vit_model = load_vit_to_cpu(cfg.backbone, cfg.prec)\n",
    "            self.model = PeftModelFromViT(cfg, vit_model, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {cfg.backbone}\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.tuner = getattr(self.model, \"tuner\", None)\n",
    "        self.head = getattr(self.model, \"head\", None)\n",
    "\n",
    "        if cfg.init_head == \"text_feat\":\n",
    "            if not cfg.backbone.startswith(\"CLIP\"):\n",
    "                print(\"text_feat head init is only available for CLIP backbones.\")\n",
    "            else:\n",
    "                text_feat_fname = TEXT_FEAT_FNAME.get(cfg.backbone)\n",
    "                if text_feat_fname is None:\n",
    "                    raise ValueError(\n",
    "                        f\"No text feature file registered for {cfg.backbone}\"\n",
    "                    )\n",
    "                if cfg.head_init_folder is None:\n",
    "                    raise ValueError(\n",
    "                        \"head_init_folder must be set for text feature initialization.\"\n",
    "                    )\n",
    "                text_feat_path = os.path.join(cfg.head_init_folder, text_feat_fname)\n",
    "                self.init_head_text_feat(text_feat_path)\n",
    "        elif cfg.init_head in [\"class_mean\", \"1_shot\", \"10_shot\", \"100_shot\"]:\n",
    "            class_mean_fname = CLASS_MEAN_FNAME.get(cfg.backbone)\n",
    "            if class_mean_fname is None:\n",
    "                raise ValueError(\n",
    "                    f\"No class mean file registered for {cfg.backbone}\"\n",
    "                )\n",
    "            if cfg.head_init_folder is None:\n",
    "                raise ValueError(\n",
    "                    \"head_init_folder must be set for class mean initialization.\"\n",
    "                )\n",
    "            class_mean_path = os.path.join(cfg.head_init_folder, class_mean_fname)\n",
    "            self.init_head_class_mean(class_mean_path)\n",
    "        elif cfg.init_head == \"linear_probe\":\n",
    "            self.init_head_linear_probe()\n",
    "\n",
    "        if not (cfg.zero_shot or cfg.test_train or cfg.test_only):\n",
    "            self.build_optimizer()\n",
    "            self.build_criterion()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    def _build_external_ood_dataset(self, transform):\n",
    "        dataset_name = self.external_ood_name\n",
    "        if dataset_name == \"svhn\":\n",
    "            root = getattr(self.cfg, \"ood_root\", None)\n",
    "            if root is None:\n",
    "                root = Path(self.cfg.root).expanduser() / \"svhn\"\n",
    "            root = Path(root).expanduser()\n",
    "            split = getattr(self.cfg, \"ood_split\", \"test\")\n",
    "            return TorchvisionSVHN(\n",
    "                root=str(root),\n",
    "                split=split,\n",
    "                download=True,\n",
    "                transform=transform,\n",
    "            )\n",
    "        raise ValueError(\n",
    "            f\"Unsupported external OOD dataset: {self.external_ood_name}\"\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_ood(self):\n",
    "        if self.ood_test_loader is None:\n",
    "            raise RuntimeError(\"OOD loader not initialized.\")\n",
    "        self.model.eval()\n",
    "        if self.tuner is not None:\n",
    "            self.tuner.eval()\n",
    "        if self.head is not None:\n",
    "            self.head.eval()\n",
    "\n",
    "        amp_enabled = self.cfg.prec == \"amp\" and self.device.type == \"cuda\"\n",
    "\n",
    "        def collect_scores(loader):\n",
    "            scores = []\n",
    "            for images, _ in loader:\n",
    "                images = images.to(self.device)\n",
    "                batch_size, ncrops, c, h, w = images.size()\n",
    "                images = images.view(batch_size * ncrops, c, h, w)\n",
    "                with autocast(enabled=amp_enabled):\n",
    "                    logits = self.model(images)\n",
    "                logits = logits.view(batch_size, ncrops, -1).mean(dim=1)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                scores.extend(probs.max(dim=1)[0].cpu().numpy())\n",
    "            return scores\n",
    "\n",
    "        id_scores = collect_scores(self.test_loader)\n",
    "        ood_scores = collect_scores(self.ood_test_loader)\n",
    "        metrics = compute_ood_metrics(id_scores, ood_scores)\n",
    "        self.last_ood_scores = {\"id\": id_scores, \"ood\": ood_scores, \"metrics\": metrics}\n",
    "        return metrics\n",
    "\n",
    "    def _log_progress(\n",
    "        self,\n",
    "        epoch,\n",
    "        _logic_batch_idx,\n",
    "        _logic_batch_num,\n",
    "        meters,\n",
    "        time_start,\n",
    "        num_epochs,\n",
    "    ):\n",
    "        cls_accs = [m.avg for m in meters[\"cls_meters\"]]\n",
    "        mean_acc = float(np.mean(cls_accs))\n",
    "        many_acc = float(np.mean([cls_accs[i] for i in self.many_idxs])) if len(self.many_idxs) else float(\"nan\")\n",
    "        med_acc = float(np.mean([cls_accs[i] for i in self.med_idxs])) if len(self.med_idxs) else float(\"nan\")\n",
    "        few_acc = float(np.mean([cls_accs[i] for i in self.few_idxs])) if len(self.few_idxs) else float(\"nan\")\n",
    "        history_entry = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"batch\": _logic_batch_idx + 1,\n",
    "            \"total_batches\": _logic_batch_num,\n",
    "            \"loss\": float(meters[\"loss\"].avg),\n",
    "            \"acc\": float(meters[\"acc\"].avg),\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"many_acc\": many_acc,\n",
    "            \"med_acc\": med_acc,\n",
    "            \"few_acc\": few_acc,\n",
    "            \"lr\": float(self.optim.param_groups[0][\"lr\"]),\n",
    "        }\n",
    "        self.training_history.append(history_entry)\n",
    "        super()._log_progress(\n",
    "            epoch,\n",
    "            _logic_batch_idx,\n",
    "            _logic_batch_num,\n",
    "            meters,\n",
    "            time_start,\n",
    "            num_epochs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34450219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:06<00:00, 24.2MB/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on all 100 CIFAR-100 classes (first 10): ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle']\n",
      "Using SVHN at /content/metalora/data/svhn as the OOD dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_aware': {'dataset': 'CIFAR100', 'root': '/content/metalora/data', 'imb_factor': None, 'head_init_folder': '/content/metalora/output/notebooks/cifar100_class_aware_svhn', 'backbone': 'CLIP-ViT-B/16', 'resolution': 224, 'output_dir': '/content/metalora/output/notebooks/cifar100_class_aware_svhn', 'print_freq': 20, 'seed': 0, 'deterministic': True, 'num_workers': 2, 'prec': 'amp', 'num_epochs': 5, 'batch_size': 64, 'accum_step': 1, 'lr': 0.01, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 0.0005, 'momentum': 0.9, 'loss_type': 'CE', 'classifier': 'CosineClassifier', 'scale': 25, 'fine_tuning': False, 'head_only': False, 'full_tuning': False, 'bias_tuning': False, 'ln_tuning': False, 'bn_tuning': False, 'vpt_shallow': False, 'vpt_deep': False, 'adapter': False, 'adaptformer': False, 'lora': False, 'lora_mlp': False, 'scale_alpha': 1, 'ssf_attn': False, 'ssf_mlp': False, 'ssf_ln': False, 'mask': False, 'partial': None, 'vpt_len': None, 'adapter_dim': None, 'adaptformer_scale': 'learnable', 'mask_ratio': None, 'mask_seed': None, 'init_head': 'text_feat', 'prompt': 'default', 'tte': False, 'expand': 24, 'tte_mode': 'fivecrop', 'randaug_times': 1, 'zero_shot': False, 'test_only': False, 'test_train': False, 'model_dir': None, 'use_flora': False, 'flora': {'arch': {'modules': ['q', 'v', 'k', 'out', 'mlp1', 'mlp2'], 'layers': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'rank': 4, 'alpha': None}, 'optimizer': {'default_lr': 0.02, 'lr_config': {'layers': {}, 'modules': {}, 'specific': {}}}}, 'use_meta': True, 'meta_data_ratio': 0.1, 'meta_lr': 0.001, 'meta_update_freq': 1, 'meta_inner_steps': 5, 'ood_dataset': 'svhn', 'ood_root': '/content/metalora/data/svhn', 'ood_split': 'test'},\n",
       " 'baseline': {'dataset': 'CIFAR100', 'root': '/content/metalora/data', 'imb_factor': None, 'head_init_folder': '/content/metalora/output/notebooks/cifar100_baseline_svhn', 'backbone': 'CLIP-ViT-B/16', 'resolution': 224, 'output_dir': '/content/metalora/output/notebooks/cifar100_baseline_svhn', 'print_freq': 20, 'seed': 0, 'deterministic': True, 'num_workers': 2, 'prec': 'amp', 'num_epochs': 5, 'batch_size': 64, 'accum_step': 1, 'lr': 0.01, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 0.0005, 'momentum': 0.9, 'loss_type': 'CE', 'classifier': 'CosineClassifier', 'scale': 25, 'fine_tuning': False, 'head_only': False, 'full_tuning': False, 'bias_tuning': False, 'ln_tuning': False, 'bn_tuning': False, 'vpt_shallow': False, 'vpt_deep': False, 'adapter': False, 'adaptformer': False, 'lora': False, 'lora_mlp': False, 'scale_alpha': 1, 'ssf_attn': False, 'ssf_mlp': False, 'ssf_ln': False, 'mask': False, 'partial': None, 'vpt_len': None, 'adapter_dim': None, 'adaptformer_scale': 'learnable', 'mask_ratio': None, 'mask_seed': None, 'init_head': 'text_feat', 'prompt': 'default', 'tte': False, 'expand': 24, 'tte_mode': 'fivecrop', 'randaug_times': 1, 'zero_shot': False, 'test_only': False, 'test_train': False, 'model_dir': None, 'use_flora': False, 'flora': {'arch': {'modules': ['q', 'v', 'k', 'out', 'mlp1', 'mlp2'], 'layers': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'rank': 4, 'alpha': None}, 'optimizer': {'default_lr': 0.02, 'lr_config': {'layers': {}, 'modules': {}, 'specific': {}}}}, 'use_meta': True, 'meta_data_ratio': 0.1, 'meta_lr': 0.001, 'meta_update_freq': 1, 'meta_inner_steps': 5, 'ood_dataset': 'svhn', 'ood_root': '/content/metalora/data/svhn', 'ood_split': 'test'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_CLASSES = list(range(100))\n",
    "\n",
    "default_data_root = Path(\n",
    "    os.environ.get(\"CIFAR100_ROOT\") or (REPO_ROOT / \"data\")\n",
    ").expanduser().resolve()\n",
    "svhn_root = Path(\n",
    "    os.environ.get(\"SVHN_ROOT\") or (default_data_root / \"svhn\")\n",
    ").expanduser().resolve()\n",
    "\n",
    "class_names = getattr(TorchvisionCIFAR100, \"classes\", None)\n",
    "if class_names is None:\n",
    "    preview_dataset = TorchvisionCIFAR100(\n",
    "        root=str(default_data_root), train=True, download=True\n",
    "    )\n",
    "    class_names = preview_dataset.classes\n",
    "\n",
    "id_names_preview = [class_names[idx] for idx in ID_CLASSES[:10]]\n",
    "print(\n",
    "    f\"Training on all {len(ID_CLASSES)} CIFAR-100 classes (first 10): {id_names_preview}\"\n",
    ")\n",
    "print(f\"Using SVHN at {svhn_root} as the OOD dataset.\")\n",
    "\n",
    "logger_root = REPO_ROOT / \"output\" / \"notebooks\" / \"logs\"\n",
    "os.makedirs(logger_root, exist_ok=True)\n",
    "logger.init(str(logger_root))\n",
    "\n",
    "def base_config(dataset_config_name, tag):\n",
    "    cfg = load_experiment_config(\n",
    "        REPO_ROOT,\n",
    "        dataset_name=dataset_config_name,\n",
    "        model_name=\"clip_vit_b16\",\n",
    "        tuner_name=None,\n",
    "    )\n",
    "    cfg.use_meta = True\n",
    "    cfg.root = str(default_data_root)\n",
    "    cfg.num_epochs = 5\n",
    "    cfg.batch_size = 64\n",
    "    cfg.accum_step = 1\n",
    "    cfg.loss_type = \"CE\"\n",
    "    cfg.head_only = False\n",
    "    cfg.init_head = \"text_feat\"\n",
    "    cfg.tte = False\n",
    "    cfg.lr = 0.01\n",
    "    cfg.print_freq = 20\n",
    "    cfg.seed = 0\n",
    "    cfg.deterministic = True\n",
    "    cfg.num_workers = min(4, os.cpu_count() or 4)\n",
    "    cfg.prec = \"amp\" if device.type == \"cuda\" else \"fp32\"\n",
    "    cfg.ood_dataset = \"svhn\"\n",
    "    cfg.ood_root = str(svhn_root)\n",
    "    cfg.ood_split = \"test\"\n",
    "    cfg.output_dir = str(\n",
    "        REPO_ROOT / \"output\" / \"notebooks\" / f\"{dataset_config_name}_{tag}\"\n",
    "    )\n",
    "    cfg.head_init_folder = cfg.output_dir\n",
    "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "    return cfg\n",
    "\n",
    "def clone_cfg(cfg):\n",
    "    return OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n",
    "\n",
    "EXPERIMENT_SPECS = [\n",
    "    (\"cifar100\", \"balanced\"),\n",
    "    (\"cifar100_ir10\", \"ir10\"),\n",
    "    (\"cifar100_ir50\", \"ir50\"),\n",
    "    (\"cifar100_ir100\", \"ir100\"),\n",
    "]\n",
    "\n",
    "experiments = {}\n",
    "for dataset_config_name, tag in EXPERIMENT_SPECS:\n",
    "    cfg_class_aware = base_config(dataset_config_name, f\"{tag}_class_aware\")\n",
    "    cfg_baseline = clone_cfg(cfg_class_aware)\n",
    "    cfg_baseline.output_dir = str(\n",
    "        REPO_ROOT / \"output\" / \"notebooks\" / f\"{dataset_config_name}_{tag}_baseline\"\n",
    ")\n",
    "    cfg_baseline.head_init_folder = cfg_baseline.output_dir\n",
    "    os.makedirs(cfg_baseline.output_dir, exist_ok=True)\n",
    "    experiments[tag] = {\n",
    "        \"dataset_config\": dataset_config_name,\n",
    "        \"class_aware_cfg\": cfg_class_aware,\n",
    "        \"baseline_cfg\": cfg_baseline,\n",
    "        \"class_aware_k\": 4,\n",
    "    }\n",
    "\n",
    "if experiments[\"balanced\"][\"class_aware_cfg\"].seed is not None:\n",
    "    seed = experiments[\"balanced\"][\"class_aware_cfg\"].seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if experiments[\"balanced\"][\"class_aware_cfg\"].deterministic and torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00/64.3M [00:00<?, ?B/s]\u001b[0m\n",
      "  0%|          | 32.8k/64.3M [00:00<09:29, 113kB/s]\u001b[0m\n",
      "  0%|          | 65.5k/64.3M [00:00<06:43, 159kB/s]\u001b[0m\n",
      "  0%|          | 32.8k/64.3M [00:00<09:29, 113kB/s]\u001b[0m\n",
      "  0%|          | 65.5k/64.3M [00:00<06:43, 159kB/s]\u001b[0m\n",
      "  0%|          | 98.3k/64.3M [00:00<05:50, 183kB/s]\u001b[0m\n",
      "  0%|          | 98.3k/64.3M [00:00<05:50, 183kB/s]\u001b[0m\n",
      "  0%|          | 131k/64.3M [00:00<05:25, 197kB/s]\u001b[0m\n",
      "  0%|          | 131k/64.3M [00:00<05:25, 197kB/s]\u001b[0m\n",
      "  0%|          | 164k/64.3M [00:00<05:11, 206kB/s]\u001b[0m\n",
      "  0%|          | 164k/64.3M [00:00<05:11, 206kB/s]\u001b[0m\n",
      "  0%|          | 197k/64.3M [00:01<05:02, 212kB/s]\u001b[0m\n",
      "  0%|          | 197k/64.3M [00:01<05:02, 212kB/s]\u001b[0m\n",
      "  0%|          | 262k/64.3M [00:01<03:44, 285kB/s]\u001b[0m\n",
      "  0%|          | 262k/64.3M [00:01<03:44, 285kB/s]\u001b[0m\n",
      "  0%|          | 295k/64.3M [00:01<03:59, 267kB/s]\u001b[0m\n",
      "  0%|          | 295k/64.3M [00:01<03:59, 267kB/s]\u001b[0m\n",
      "  1%|          | 360k/64.3M [00:01<03:19, 321kB/s]\u001b[0m\n",
      "  1%|          | 426k/64.3M [00:01<02:57, 359kB/s]\u001b[0m\n",
      "  1%|          | 360k/64.3M [00:01<03:19, 321kB/s]\u001b[0m\n",
      "  1%|          | 426k/64.3M [00:01<02:57, 359kB/s]\u001b[0m\n",
      "  1%|          | 492k/64.3M [00:01<02:44, 387kB/s]\u001b[0m\n",
      "  1%|          | 492k/64.3M [00:01<02:44, 387kB/s]\u001b[0m\n",
      "  1%|          | 590k/64.3M [00:01<02:15, 472kB/s]\u001b[0m\n",
      "  1%|          | 590k/64.3M [00:01<02:15, 472kB/s]\u001b[0m\n",
      "  1%|          | 688k/64.3M [00:02<01:59, 532kB/s]\u001b[0m\n",
      "  1%|          | 688k/64.3M [00:02<01:59, 532kB/s]\u001b[0m\n",
      "  1%|          | 786k/64.3M [00:02<01:50, 576kB/s]\u001b[0m\n",
      "  1%|          | 786k/64.3M [00:02<01:50, 576kB/s]\u001b[0m\n",
      "  1%|▏         | 918k/64.3M [00:02<01:34, 672kB/s]\u001b[0m\n",
      "  1%|▏         | 918k/64.3M [00:02<01:34, 672kB/s]\u001b[0m\n",
      "  2%|▏         | 1.08M/64.3M [00:02<01:18, 807kB/s]\u001b[0m\n",
      "  2%|▏         | 1.08M/64.3M [00:02<01:18, 807kB/s]\u001b[0m\n",
      "  2%|▏         | 1.28M/64.3M [00:02<01:05, 967kB/s]\u001b[0m\n",
      "  2%|▏         | 1.28M/64.3M [00:02<01:05, 967kB/s]\u001b[0m\n",
      "  2%|▏         | 1.51M/64.3M [00:02<00:54, 1.15MB/s]\u001b[0m\n",
      "  2%|▏         | 1.51M/64.3M [00:02<00:54, 1.15MB/s]\u001b[0m\n",
      "  3%|▎         | 1.77M/64.3M [00:02<00:46, 1.34MB/s]\u001b[0m\n",
      "  3%|▎         | 1.77M/64.3M [00:02<00:46, 1.34MB/s]\u001b[0m\n",
      "  3%|▎         | 2.10M/64.3M [00:03<00:38, 1.61MB/s]\u001b[0m\n",
      "  3%|▎         | 2.10M/64.3M [00:03<00:38, 1.61MB/s]\u001b[0m\n",
      "  4%|▍         | 2.46M/64.3M [00:03<00:33, 1.86MB/s]\u001b[0m\n",
      "  4%|▍         | 2.46M/64.3M [00:03<00:33, 1.86MB/s]\u001b[0m\n",
      "  4%|▍         | 2.85M/64.3M [00:03<00:29, 2.11MB/s]\u001b[0m\n",
      "  4%|▍         | 2.85M/64.3M [00:03<00:29, 2.11MB/s]\u001b[0m\n",
      "  5%|▌         | 3.31M/64.3M [00:03<00:25, 2.41MB/s]\u001b[0m\n",
      "  5%|▌         | 3.31M/64.3M [00:03<00:25, 2.41MB/s]\u001b[0m\n",
      "  6%|▌         | 3.80M/64.3M [00:03<00:22, 2.70MB/s]\u001b[0m\n",
      "  6%|▌         | 3.80M/64.3M [00:03<00:22, 2.70MB/s]\u001b[0m\n",
      "  7%|▋         | 4.36M/64.3M [00:03<00:19, 3.02MB/s]\u001b[0m\n",
      "  7%|▋         | 4.36M/64.3M [00:03<00:19, 3.02MB/s]\u001b[0m\n",
      "  8%|▊         | 4.95M/64.3M [00:03<00:17, 3.33MB/s]\u001b[0m\n",
      "  8%|▊         | 4.95M/64.3M [00:03<00:17, 3.33MB/s]\u001b[0m\n",
      "  9%|▊         | 5.60M/64.3M [00:04<00:15, 3.67MB/s]\u001b[0m\n",
      "  9%|▊         | 5.60M/64.3M [00:04<00:15, 3.67MB/s]\u001b[0m\n",
      " 10%|▉         | 6.32M/64.3M [00:04<00:14, 4.05MB/s]\u001b[0m\n",
      " 10%|▉         | 6.32M/64.3M [00:04<00:14, 4.05MB/s]\u001b[0m\n",
      " 11%|█         | 7.11M/64.3M [00:04<00:12, 4.44MB/s]\u001b[0m\n",
      " 11%|█         | 7.11M/64.3M [00:04<00:12, 4.44MB/s]\u001b[0m\n",
      " 12%|█▏        | 8.00M/64.3M [00:04<00:11, 4.92MB/s]\u001b[0m\n",
      " 12%|█▏        | 8.00M/64.3M [00:04<00:11, 4.92MB/s]\u001b[0m\n",
      " 14%|█▍        | 8.95M/64.3M [00:04<00:10, 5.39MB/s]\u001b[0m\n",
      " 14%|█▍        | 8.95M/64.3M [00:04<00:10, 5.39MB/s]\u001b[0m\n",
      " 15%|█▌        | 9.96M/64.3M [00:04<00:09, 5.86MB/s]\u001b[0m\n",
      " 15%|█▌        | 9.96M/64.3M [00:04<00:09, 5.86MB/s]\u001b[0m\n",
      " 17%|█▋        | 11.1M/64.3M [00:04<00:08, 6.38MB/s]\u001b[0m\n",
      " 17%|█▋        | 11.1M/64.3M [00:04<00:08, 6.38MB/s]\u001b[0m\n",
      " 18%|█▊        | 11.7M/64.3M [00:05<00:09, 5.76MB/s]\u001b[0m\n",
      " 18%|█▊        | 11.7M/64.3M [00:05<00:09, 5.76MB/s]\u001b[0m\n",
      " 20%|██        | 13.1M/64.3M [00:05<00:07, 6.82MB/s]\u001b[0m\n",
      " 20%|██        | 13.1M/64.3M [00:05<00:07, 6.82MB/s]\u001b[0m\n",
      " 22%|██▏       | 14.0M/64.3M [00:05<00:07, 6.58MB/s]\u001b[0m\n",
      " 22%|██▏       | 14.0M/64.3M [00:05<00:07, 6.58MB/s]\u001b[0m\n",
      " 23%|██▎       | 14.8M/64.3M [00:05<00:07, 6.42MB/s]\u001b[0m\n",
      " 23%|██▎       | 14.8M/64.3M [00:05<00:07, 6.42MB/s]\u001b[0m\n",
      " 24%|██▍       | 15.7M/64.3M [00:05<00:07, 6.31MB/s]\u001b[0m\n",
      " 24%|██▍       | 15.7M/64.3M [00:05<00:07, 6.31MB/s]\u001b[0m\n",
      " 26%|██▌       | 16.6M/64.3M [00:05<00:07, 6.29MB/s]\u001b[0m\n",
      " 27%|██▋       | 17.6M/64.3M [00:06<00:07, 6.32MB/s]\u001b[0m\n",
      " 26%|██▌       | 16.6M/64.3M [00:05<00:07, 6.29MB/s]\u001b[0m\n",
      " 27%|██▋       | 17.6M/64.3M [00:06<00:07, 6.32MB/s]\u001b[0m\n",
      " 29%|██▉       | 18.5M/64.3M [00:06<00:07, 6.37MB/s]\u001b[0m\n",
      " 29%|██▉       | 18.5M/64.3M [00:06<00:07, 6.37MB/s]\u001b[0m\n",
      " 30%|███       | 19.5M/64.3M [00:06<00:06, 6.44MB/s]\u001b[0m\n",
      " 30%|███       | 19.5M/64.3M [00:06<00:06, 6.44MB/s]\u001b[0m\n",
      " 32%|███▏      | 20.5M/64.3M [00:06<00:06, 6.49MB/s]\u001b[0m\n",
      " 32%|███▏      | 20.5M/64.3M [00:06<00:06, 6.49MB/s]\u001b[0m\n",
      " 33%|███▎      | 21.5M/64.3M [00:06<00:06, 6.59MB/s]\u001b[0m\n",
      " 33%|███▎      | 21.5M/64.3M [00:06<00:06, 6.59MB/s]\u001b[0m\n",
      " 35%|███▍      | 22.5M/64.3M [00:06<00:06, 6.69MB/s]\u001b[0m\n",
      " 35%|███▍      | 22.5M/64.3M [00:06<00:06, 6.69MB/s]\u001b[0m\n",
      " 37%|███▋      | 23.5M/64.3M [00:06<00:06, 6.76MB/s]\u001b[0m\n",
      " 37%|███▋      | 23.5M/64.3M [00:06<00:06, 6.76MB/s]\u001b[0m\n",
      " 38%|███▊      | 24.5M/64.3M [00:07<00:05, 6.86MB/s]\u001b[0m\n",
      " 38%|███▊      | 24.5M/64.3M [00:07<00:05, 6.86MB/s]\u001b[0m\n",
      " 40%|███▉      | 25.6M/64.3M [00:07<00:05, 6.95MB/s]\u001b[0m\n",
      " 40%|███▉      | 25.6M/64.3M [00:07<00:05, 6.95MB/s]\u001b[0m\n",
      " 41%|████▏     | 26.6M/64.3M [00:07<00:05, 7.04MB/s]\u001b[0m\n",
      " 41%|████▏     | 26.6M/64.3M [00:07<00:05, 7.04MB/s]\u001b[0m\n",
      " 43%|████▎     | 27.7M/64.3M [00:07<00:05, 7.14MB/s]\u001b[0m\n",
      " 43%|████▎     | 27.7M/64.3M [00:07<00:05, 7.14MB/s]\u001b[0m\n",
      " 45%|████▍     | 28.8M/64.3M [00:07<00:04, 7.21MB/s]\u001b[0m\n",
      " 45%|████▍     | 28.8M/64.3M [00:07<00:04, 7.21MB/s]\u001b[0m\n",
      " 46%|████▋     | 29.9M/64.3M [00:07<00:04, 7.26MB/s]\u001b[0m\n",
      " 46%|████▋     | 29.9M/64.3M [00:07<00:04, 7.26MB/s]\u001b[0m\n",
      " 48%|████▊     | 31.0M/64.3M [00:07<00:04, 7.35MB/s]\u001b[0m\n",
      " 48%|████▊     | 31.0M/64.3M [00:07<00:04, 7.35MB/s]\u001b[0m\n",
      " 50%|████▉     | 32.1M/64.3M [00:08<00:04, 7.38MB/s]\u001b[0m\n",
      " 50%|████▉     | 32.1M/64.3M [00:08<00:04, 7.38MB/s]\u001b[0m\n",
      " 52%|█████▏    | 33.2M/64.3M [00:08<00:04, 7.50MB/s]\u001b[0m\n",
      " 52%|█████▏    | 33.2M/64.3M [00:08<00:04, 7.50MB/s]\u001b[0m\n",
      " 53%|█████▎    | 34.3M/64.3M [00:08<00:03, 7.55MB/s]\u001b[0m\n",
      " 53%|█████▎    | 34.3M/64.3M [00:08<00:03, 7.55MB/s]\u001b[0m\n",
      " 55%|█████▌    | 35.5M/64.3M [00:08<00:03, 7.63MB/s]\u001b[0m\n",
      " 55%|█████▌    | 35.5M/64.3M [00:08<00:03, 7.63MB/s]\u001b[0m\n",
      " 57%|█████▋    | 36.6M/64.3M [00:08<00:03, 7.67MB/s]\u001b[0m\n",
      " 57%|█████▋    | 36.6M/64.3M [00:08<00:03, 7.67MB/s]\u001b[0m\n",
      " 59%|█████▉    | 37.8M/64.3M [00:08<00:03, 7.74MB/s]\u001b[0m\n",
      " 59%|█████▉    | 37.8M/64.3M [00:08<00:03, 7.74MB/s]\u001b[0m\n",
      " 61%|██████    | 38.9M/64.3M [00:08<00:03, 7.77MB/s]\u001b[0m\n",
      " 61%|██████    | 38.9M/64.3M [00:08<00:03, 7.77MB/s]\u001b[0m\n",
      " 62%|██████▏   | 40.1M/64.3M [00:09<00:03, 7.83MB/s]\u001b[0m\n",
      " 62%|██████▏   | 40.1M/64.3M [00:09<00:03, 7.83MB/s]\u001b[0m\n",
      " 64%|██████▍   | 41.3M/64.3M [00:09<00:02, 7.85MB/s]\u001b[0m\n",
      " 64%|██████▍   | 41.3M/64.3M [00:09<00:02, 7.85MB/s]\u001b[0m\n",
      " 66%|██████▌   | 42.4M/64.3M [00:09<00:02, 7.91MB/s]\u001b[0m\n",
      " 66%|██████▌   | 42.4M/64.3M [00:09<00:02, 7.91MB/s]\u001b[0m\n",
      " 68%|██████▊   | 43.6M/64.3M [00:09<00:02, 7.95MB/s]\u001b[0m\n",
      " 68%|██████▊   | 43.6M/64.3M [00:09<00:02, 7.95MB/s]\u001b[0m\n",
      " 70%|██████▉   | 44.8M/64.3M [00:09<00:02, 7.98MB/s]\u001b[0m\n",
      " 70%|██████▉   | 44.8M/64.3M [00:09<00:02, 7.98MB/s]\u001b[0m\n",
      " 72%|███████▏  | 46.0M/64.3M [00:09<00:02, 8.00MB/s]\u001b[0m\n",
      " 72%|███████▏  | 46.0M/64.3M [00:09<00:02, 8.00MB/s]\u001b[0m\n",
      " 73%|███████▎  | 47.2M/64.3M [00:09<00:02, 8.06MB/s]\u001b[0m\n",
      " 73%|███████▎  | 47.2M/64.3M [00:09<00:02, 8.06MB/s]\u001b[0m\n",
      " 75%|███████▌  | 48.4M/64.3M [00:10<00:01, 8.08MB/s]\u001b[0m\n",
      " 75%|███████▌  | 48.4M/64.3M [00:10<00:01, 8.08MB/s]\u001b[0m\n",
      " 77%|███████▋  | 49.6M/64.3M [00:10<00:01, 8.12MB/s]\u001b[0m\n",
      " 77%|███████▋  | 49.6M/64.3M [00:10<00:01, 8.12MB/s]\u001b[0m\n",
      " 79%|███████▉  | 50.8M/64.3M [00:10<00:01, 8.14MB/s]\u001b[0m\n",
      " 79%|███████▉  | 50.8M/64.3M [00:10<00:01, 8.14MB/s]\u001b[0m\n",
      " 81%|████████  | 52.0M/64.3M [00:10<00:01, 8.16MB/s]\u001b[0m\n",
      " 81%|████████  | 52.0M/64.3M [00:10<00:01, 8.16MB/s]\u001b[0m\n",
      " 83%|████████▎ | 53.2M/64.3M [00:10<00:01, 8.19MB/s]\u001b[0m\n",
      " 83%|████████▎ | 53.2M/64.3M [00:10<00:01, 8.19MB/s]\u001b[0m\n",
      " 85%|████████▍ | 54.4M/64.3M [00:10<00:01, 8.22MB/s]\u001b[0m\n",
      " 85%|████████▍ | 54.4M/64.3M [00:10<00:01, 8.22MB/s]\u001b[0m\n",
      " 87%|████████▋ | 55.6M/64.3M [00:10<00:01, 8.24MB/s]\u001b[0m\n",
      " 87%|████████▋ | 55.6M/64.3M [00:10<00:01, 8.24MB/s]\u001b[0m\n",
      " 88%|████████▊ | 56.8M/64.3M [00:11<00:00, 8.25MB/s]\u001b[0m\n",
      " 88%|████████▊ | 56.8M/64.3M [00:11<00:00, 8.25MB/s]\u001b[0m\n",
      " 90%|█████████ | 58.1M/64.3M [00:11<00:00, 8.28MB/s]\u001b[0m\n",
      " 90%|█████████ | 58.1M/64.3M [00:11<00:00, 8.28MB/s]\u001b[0m\n",
      " 92%|█████████▏| 59.3M/64.3M [00:11<00:00, 8.30MB/s]\u001b[0m\n",
      " 92%|█████████▏| 59.3M/64.3M [00:11<00:00, 8.30MB/s]\u001b[0m\n",
      " 94%|█████████▍| 60.5M/64.3M [00:11<00:00, 8.30MB/s]\u001b[0m\n",
      " 94%|█████████▍| 60.5M/64.3M [00:11<00:00, 8.30MB/s]\u001b[0m\n",
      " 96%|█████████▌| 61.7M/64.3M [00:11<00:00, 8.32MB/s]\u001b[0m\n",
      " 96%|█████████▌| 61.7M/64.3M [00:11<00:00, 8.32MB/s]\u001b[0m\n",
      " 97%|█████████▋| 62.6M/64.3M [00:11<00:00, 7.52MB/s]\u001b[0m\n",
      " 97%|█████████▋| 62.6M/64.3M [00:11<00:00, 7.52MB/s]\u001b[0m\n",
      " 99%|█████████▉| 63.8M/64.3M [00:12<00:00, 7.78MB/s]\u001b[0m\n",
      "100%|██████████| 64.3M/64.3M [00:12<00:00, 5.34MB/s]\u001b[0m\n",
      " 99%|█████████▉| 63.8M/64.3M [00:12<00:00, 7.78MB/s]\u001b[0m\n",
      "100%|██████████| 64.3M/64.3M [00:12<00:00, 5.34MB/s]\u001b[0m\n",
      "\u001b[37mTrain sampler: class-aware sampler\u001b[0m\n",
      "\u001b[37mTrain samples: 50000 | ID classes: 100 | OOD dataset (SVHN): 26032 samples\u001b[0m\n",
      "  0%|                                               | 0.00/351M [00:00<?, ?iB/s]\u001b[0m\n",
      "  4%|█▌                                     | 13.7M/351M [00:00<00:02, 137MiB/s]\u001b[0m\n",
      "\u001b[37mTrain sampler: class-aware sampler\u001b[0m\n",
      "\u001b[37mTrain samples: 50000 | ID classes: 100 | OOD dataset (SVHN): 26032 samples\u001b[0m\n",
      "  0%|                                               | 0.00/351M [00:00<?, ?iB/s]\u001b[0m\n",
      "  4%|█▌                                     | 13.7M/351M [00:00<00:02, 137MiB/s]\u001b[0m\n",
      "  8%|███▏                                   | 28.5M/351M [00:00<00:02, 143MiB/s]\u001b[0m\n",
      "  8%|███▏                                   | 28.5M/351M [00:00<00:02, 143MiB/s]\u001b[0m\n",
      " 12%|████▋                                 | 42.8M/351M [00:00<00:03, 84.5MiB/s]\u001b[0m\n",
      " 12%|████▋                                 | 42.8M/351M [00:00<00:03, 84.5MiB/s]\u001b[0m\n",
      " 16%|██████▍                                | 57.4M/351M [00:00<00:02, 101MiB/s]\u001b[0m\n",
      " 16%|██████▍                                | 57.4M/351M [00:00<00:02, 101MiB/s]\u001b[0m\n",
      " 20%|███████▌                              | 69.4M/351M [00:00<00:03, 84.2MiB/s]\u001b[0m\n",
      " 20%|███████▌                              | 69.4M/351M [00:00<00:03, 84.2MiB/s]\u001b[0m\n",
      " 24%|█████████▎                            | 85.7M/351M [00:00<00:03, 84.3MiB/s]\u001b[0m\n",
      " 24%|█████████▎                            | 85.7M/351M [00:00<00:03, 84.3MiB/s]\u001b[0m\n",
      " 28%|██████████▍                           | 96.6M/351M [00:01<00:02, 89.8MiB/s]\u001b[0m\n",
      " 28%|██████████▍                           | 96.6M/351M [00:01<00:02, 89.8MiB/s]\u001b[0m\n",
      " 31%|████████████▌                           | 110M/351M [00:01<00:02, 100MiB/s]\u001b[0m\n",
      " 31%|████████████▌                           | 110M/351M [00:01<00:02, 100MiB/s]\u001b[0m\n",
      " 35%|██████████████                          | 124M/351M [00:01<00:02, 110MiB/s]\u001b[0m\n",
      " 35%|██████████████                          | 124M/351M [00:01<00:02, 110MiB/s]\u001b[0m\n",
      " 39%|███████████████▍                        | 136M/351M [00:01<00:01, 112MiB/s]\u001b[0m\n",
      " 39%|███████████████▍                        | 136M/351M [00:01<00:01, 112MiB/s]\u001b[0m\n",
      " 42%|████████████████▊                       | 148M/351M [00:01<00:01, 103MiB/s]\u001b[0m\n",
      " 42%|████████████████▊                       | 148M/351M [00:01<00:01, 103MiB/s]\u001b[0m\n",
      " 45%|█████████████████▌                     | 158M/351M [00:01<00:02, 94.8MiB/s]\u001b[0m\n",
      " 45%|█████████████████▌                     | 158M/351M [00:01<00:02, 94.8MiB/s]\u001b[0m\n",
      " 50%|████████████████████                    | 176M/351M [00:01<00:01, 114MiB/s]\u001b[0m\n",
      " 50%|████████████████████                    | 176M/351M [00:01<00:01, 114MiB/s]\u001b[0m\n",
      " 54%|████████████████████▉                  | 188M/351M [00:01<00:01, 95.1MiB/s]\u001b[0m\n",
      " 54%|████████████████████▉                  | 188M/351M [00:01<00:01, 95.1MiB/s]\u001b[0m\n",
      " 57%|██████████████████████                 | 198M/351M [00:02<00:01, 90.6MiB/s]\u001b[0m\n",
      " 57%|██████████████████████                 | 198M/351M [00:02<00:01, 90.6MiB/s]\u001b[0m\n",
      " 61%|████████████████████████▍               | 214M/351M [00:02<00:01, 106MiB/s]\u001b[0m\n",
      " 61%|████████████████████████▍               | 214M/351M [00:02<00:01, 106MiB/s]\u001b[0m\n",
      " 64%|█████████████████████████              | 225M/351M [00:02<00:02, 58.5MiB/s]\u001b[0m\n",
      " 64%|█████████████████████████              | 225M/351M [00:02<00:02, 58.5MiB/s]\u001b[0m\n",
      " 67%|██████████████████████████             | 234M/351M [00:02<00:01, 59.0MiB/s]\u001b[0m\n",
      " 67%|██████████████████████████             | 234M/351M [00:02<00:01, 59.0MiB/s]\u001b[0m\n",
      " 70%|███████████████████████████▎           | 246M/351M [00:02<00:01, 70.1MiB/s]\u001b[0m\n",
      " 70%|███████████████████████████▎           | 246M/351M [00:02<00:01, 70.1MiB/s]\u001b[0m\n",
      " 73%|████████████████████████████▌          | 257M/351M [00:02<00:01, 77.7MiB/s]\u001b[0m\n",
      " 73%|████████████████████████████▌          | 257M/351M [00:02<00:01, 77.7MiB/s]\u001b[0m\n",
      " 76%|█████████████████████████████▌         | 266M/351M [00:03<00:01, 65.8MiB/s]\u001b[0m\n",
      " 76%|█████████████████████████████▌         | 266M/351M [00:03<00:01, 65.8MiB/s]\u001b[0m\n",
      " 78%|██████████████████████████████▌        | 274M/351M [00:03<00:01, 62.4MiB/s]\u001b[0m\n",
      " 78%|██████████████████████████████▌        | 274M/351M [00:03<00:01, 62.4MiB/s]\u001b[0m\n",
      " 81%|███████████████████████████████▊       | 286M/351M [00:03<00:00, 71.9MiB/s]\u001b[0m\n",
      " 81%|███████████████████████████████▊       | 286M/351M [00:03<00:00, 71.9MiB/s]\u001b[0m\n",
      " 84%|████████████████████████████████▋      | 294M/351M [00:03<00:00, 72.9MiB/s]\u001b[0m\n",
      " 84%|████████████████████████████████▋      | 294M/351M [00:03<00:00, 72.9MiB/s]\u001b[0m\n",
      " 86%|█████████████████████████████████▌     | 302M/351M [00:03<00:00, 68.6MiB/s]\u001b[0m\n",
      " 86%|█████████████████████████████████▌     | 302M/351M [00:03<00:00, 68.6MiB/s]\u001b[0m\n",
      " 89%|██████████████████████████████████▋    | 313M/351M [00:03<00:00, 77.5MiB/s]\u001b[0m\n",
      " 89%|██████████████████████████████████▋    | 313M/351M [00:03<00:00, 77.5MiB/s]\u001b[0m\n",
      " 92%|███████████████████████████████████▋   | 321M/351M [00:03<00:00, 79.4MiB/s]\u001b[0m\n",
      " 92%|███████████████████████████████████▋   | 321M/351M [00:03<00:00, 79.4MiB/s]\u001b[0m\n",
      " 94%|████████████████████████████████████▋  | 329M/351M [00:03<00:00, 78.2MiB/s]\u001b[0m\n",
      " 94%|████████████████████████████████████▋  | 329M/351M [00:03<00:00, 78.2MiB/s]\u001b[0m\n",
      " 96%|█████████████████████████████████████▌ | 338M/351M [00:04<00:00, 78.4MiB/s]\u001b[0m\n",
      " 96%|█████████████████████████████████████▌ | 338M/351M [00:04<00:00, 78.4MiB/s]\u001b[0m\n",
      " 99%|██████████████████████████████████████▍| 346M/351M [00:04<00:00, 78.4MiB/s]\u001b[0m\n",
      " 99%|██████████████████████████████████████▍| 346M/351M [00:04<00:00, 78.4MiB/s]\u001b[0m\n",
      "100%|███████████████████████████████████████| 351M/351M [00:04<00:00, 83.9MiB/s]\u001b[0m\n",
      "100%|███████████████████████████████████████| 351M/351M [00:04<00:00, 83.9MiB/s]\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mText features not found at /content/metalora/output/notebooks/cifar100_class_aware_svhn/txtfeat_clip_vit_b16.pth.\u001b[0m\n",
      "\u001b[37mComputing text features...\u001b[0m\n",
      "\u001b[37mComputing text features\u001b[0m\n",
      "\u001b[37mText features not found at /content/metalora/output/notebooks/cifar100_class_aware_svhn/txtfeat_clip_vit_b16.pth.\u001b[0m\n",
      "\u001b[37mComputing text features...\u001b[0m\n",
      "\u001b[37mComputing text features\u001b[0m\n",
      "\u001b[37mSaving text features to /content/metalora/output/notebooks/cifar100_class_aware_svhn/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37mSaving text features to /content/metalora/output/notebooks/cifar100_class_aware_svhn/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mTuner mode: Only tuning the tuner and head\u001b[0m\n",
      "\u001b[37mTurning off gradients in the model\u001b[0m\n",
      "\u001b[37mTurning on gradients in the tuner and head\u001b[0m\n",
      "\u001b[37mTotal params: 149697536\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mTuner mode: Only tuning the tuner and head\u001b[0m\n",
      "\u001b[37mTurning off gradients in the model\u001b[0m\n",
      "\u001b[37mTurning on gradients in the tuner and head\u001b[0m\n",
      "\u001b[37mTotal params: 149697536\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37mHead params: 76800\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:453: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if cfg.prec == \"amp\" else None\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mHead params: 76800\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:453: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if cfg.prec == \"amp\" else None\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 50000\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 50000\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_class_aware_svhn/tensorboard)\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_class_aware_svhn/tensorboard)\u001b[0m\n",
      "\u001b[37mTrain sampler: standard shuffled batches\u001b[0m\n",
      "\u001b[37mTrain samples: 50000 | ID classes: 100 | OOD dataset (SVHN): 26032 samples\u001b[0m\n",
      "\u001b[37mTrain sampler: standard shuffled batches\u001b[0m\n",
      "\u001b[37mTrain samples: 50000 | ID classes: 100 | OOD dataset (SVHN): 26032 samples\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mText features not found at /content/metalora/output/notebooks/cifar100_baseline_svhn/txtfeat_clip_vit_b16.pth.\u001b[0m\n",
      "\u001b[37mComputing text features...\u001b[0m\n",
      "\u001b[37mComputing text features\u001b[0m\n",
      "\u001b[37mText features not found at /content/metalora/output/notebooks/cifar100_baseline_svhn/txtfeat_clip_vit_b16.pth.\u001b[0m\n",
      "\u001b[37mComputing text features...\u001b[0m\n",
      "\u001b[37mComputing text features\u001b[0m\n",
      "\u001b[37mSaving text features to /content/metalora/output/notebooks/cifar100_baseline_svhn/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mTuner mode: Only tuning the tuner and head\u001b[0m\n",
      "\u001b[37mTurning off gradients in the model\u001b[0m\n",
      "\u001b[37mSaving text features to /content/metalora/output/notebooks/cifar100_baseline_svhn/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mTuner mode: Only tuning the tuner and head\u001b[0m\n",
      "\u001b[37mTurning off gradients in the model\u001b[0m\n",
      "\u001b[37mTurning on gradients in the tuner and head\u001b[0m\n",
      "\u001b[37mTotal params: 149697536\u001b[0m\n",
      "\u001b[37mTurning on gradients in the tuner and head\u001b[0m\n",
      "\u001b[37mTotal params: 149697536\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37mHead params: 76800\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37mHead params: 76800\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 50000\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 50000\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_baseline_svhn/tensorboard)\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_baseline_svhn/tensorboard)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainers = {}\n",
    "for tag, spec in experiments.items():\n",
    "    print(f\"\\nInitializing trainers for {tag} (dataset config: {spec['dataset_config']})\")\n",
    "    class_aware_trainer = ClassAwareOODTrainer(\n",
    "        spec[\"class_aware_cfg\"],\n",
    "        device,\n",
    "        ID_CLASSES,\n",
    "        ood_classes=None,\n",
    "        class_aware_k=spec[\"class_aware_k\"],\n",
    "    )\n",
    "    class_aware_trainer.initialize()\n",
    "    baseline_trainer = ClassAwareOODTrainer(\n",
    "        spec[\"baseline_cfg\"],\n",
    "        device,\n",
    "        ID_CLASSES,\n",
    "        ood_classes=None,\n",
    "        class_aware_k=None,\n",
    "    )\n",
    "    baseline_trainer.initialize()\n",
    "    trainers[tag] = {\n",
    "        \"Class-Aware\": class_aware_trainer,\n",
    "        \"Baseline\": baseline_trainer,\n",
    "    }\n",
    "\n",
    "trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa29caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\n",
      "================================================================================\n",
      "                         Training: Class-Aware Sampler                          \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                 Training model                                 \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                 Training model                                 \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/782] time 0.258 (0.394) data 0.000 (0.047) loss 3.4600 (3.7148) acc 32.8125 (29.7556) (mean 27.3572 many 27.3572 med nan few nan) lr 1.0000e-02 elapsed 0:00:07 eta 0:25:33\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/782] time 0.258 (0.394) data 0.000 (0.047) loss 3.4600 (3.7148) acc 32.8125 (29.7556) (mean 27.3572 many 27.3572 med nan few nan) lr 1.0000e-02 elapsed 0:00:07 eta 0:25:33\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/782] time 0.265 (0.328) data 0.000 (0.024) loss 3.0191 (3.4431) acc 43.7500 (30.5893) (mean 30.4880 many 30.4880 med nan few nan) lr 1.0000e-02 elapsed 0:00:13 eta 0:21:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/782] time 0.265 (0.328) data 0.000 (0.024) loss 3.0191 (3.4431) acc 43.7500 (30.5893) (mean 30.4880 many 30.4880 med nan few nan) lr 1.0000e-02 elapsed 0:00:13 eta 0:21:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/782] time 0.266 (0.307) data 0.000 (0.016) loss 2.9020 (3.1681) acc 40.6250 (33.8734) (mean 33.7958 many 33.7958 med nan few nan) lr 1.0000e-02 elapsed 0:00:18 eta 0:19:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/782] time 0.266 (0.307) data 0.000 (0.016) loss 2.9020 (3.1681) acc 40.6250 (33.8734) (mean 33.7958 many 33.7958 med nan few nan) lr 1.0000e-02 elapsed 0:00:18 eta 0:19:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/782] time 0.273 (0.297) data 0.000 (0.012) loss 3.1010 (3.0338) acc 26.5625 (33.9413) (mean 34.3668 many 34.3668 med nan few nan) lr 1.0000e-02 elapsed 0:00:23 eta 0:18:58\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/782] time 0.273 (0.297) data 0.000 (0.012) loss 3.1010 (3.0338) acc 26.5625 (33.9413) (mean 34.3668 many 34.3668 med nan few nan) lr 1.0000e-02 elapsed 0:00:23 eta 0:18:58\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/782] time 0.272 (0.292) data 0.000 (0.010) loss 3.1330 (2.9456) acc 26.5625 (36.4301) (mean 37.0355 many 37.0355 med nan few nan) lr 1.0000e-02 elapsed 0:00:29 eta 0:18:33\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/782] time 0.272 (0.292) data 0.000 (0.010) loss 3.1330 (2.9456) acc 26.5625 (36.4301) (mean 37.0355 many 37.0355 med nan few nan) lr 1.0000e-02 elapsed 0:00:29 eta 0:18:33\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/782] time 0.272 (0.289) data 0.000 (0.008) loss 2.8298 (2.8524) acc 32.8125 (36.4407) (mean 37.9514 many 37.9514 med nan few nan) lr 1.0000e-02 elapsed 0:00:34 eta 0:18:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/782] time 0.272 (0.289) data 0.000 (0.008) loss 2.8298 (2.8524) acc 32.8125 (36.4407) (mean 37.9514 many 37.9514 med nan few nan) lr 1.0000e-02 elapsed 0:00:34 eta 0:18:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/782] time 0.277 (0.288) data 0.003 (0.007) loss 3.0820 (2.6897) acc 31.2500 (40.0070) (mean 40.8041 many 40.8041 med nan few nan) lr 1.0000e-02 elapsed 0:00:40 eta 0:18:05\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/782] time 0.277 (0.288) data 0.003 (0.007) loss 3.0820 (2.6897) acc 31.2500 (40.0070) (mean 40.8041 many 40.8041 med nan few nan) lr 1.0000e-02 elapsed 0:00:40 eta 0:18:05\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/782] time 0.279 (0.287) data 0.000 (0.006) loss 2.0684 (2.5995) acc 59.3750 (41.6841) (mean 40.7402 many 40.7402 med nan few nan) lr 1.0000e-02 elapsed 0:00:45 eta 0:17:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/782] time 0.279 (0.287) data 0.000 (0.006) loss 2.0684 (2.5995) acc 59.3750 (41.6841) (mean 40.7402 many 40.7402 med nan few nan) lr 1.0000e-02 elapsed 0:00:45 eta 0:17:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/782] time 0.300 (0.287) data 0.005 (0.006) loss 2.3778 (2.5290) acc 35.9375 (42.6974) (mean 42.6155 many 42.6155 med nan few nan) lr 1.0000e-02 elapsed 0:00:51 eta 0:17:51\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/782] time 0.300 (0.287) data 0.005 (0.006) loss 2.3778 (2.5290) acc 35.9375 (42.6974) (mean 42.6155 many 42.6155 med nan few nan) lr 1.0000e-02 elapsed 0:00:51 eta 0:17:51\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/782] time 0.295 (0.288) data 0.000 (0.005) loss 2.5925 (2.5450) acc 45.3125 (43.5123) (mean 43.9578 many 43.9578 med nan few nan) lr 1.0000e-02 elapsed 0:00:57 eta 0:17:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/782] time 0.295 (0.288) data 0.000 (0.005) loss 2.5925 (2.5450) acc 45.3125 (43.5123) (mean 43.9578 many 43.9578 med nan few nan) lr 1.0000e-02 elapsed 0:00:57 eta 0:17:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/782] time 0.301 (0.288) data 0.000 (0.005) loss 2.6331 (2.5650) acc 42.1875 (40.4307) (mean 41.6881 many 41.6881 med nan few nan) lr 1.0000e-02 elapsed 0:01:03 eta 0:17:43\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/782] time 0.301 (0.288) data 0.000 (0.005) loss 2.6331 (2.5650) acc 42.1875 (40.4307) (mean 41.6881 many 41.6881 med nan few nan) lr 1.0000e-02 elapsed 0:01:03 eta 0:17:43\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/782] time 0.306 (0.289) data 0.000 (0.004) loss 2.6790 (2.4735) acc 40.6250 (42.9566) (mean 42.8933 many 42.8933 med nan few nan) lr 1.0000e-02 elapsed 0:01:09 eta 0:17:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/782] time 0.306 (0.289) data 0.000 (0.004) loss 2.6790 (2.4735) acc 40.6250 (42.9566) (mean 42.8933 many 42.8933 med nan few nan) lr 1.0000e-02 elapsed 0:01:09 eta 0:17:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/782] time 0.301 (0.290) data 0.000 (0.004) loss 2.2789 (2.3653) acc 50.0000 (45.5301) (mean 44.7404 many 44.7404 med nan few nan) lr 1.0000e-02 elapsed 0:01:15 eta 0:17:39\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/782] time 0.301 (0.290) data 0.000 (0.004) loss 2.2789 (2.3653) acc 50.0000 (45.5301) (mean 44.7404 many 44.7404 med nan few nan) lr 1.0000e-02 elapsed 0:01:15 eta 0:17:39\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/782] time 0.313 (0.292) data 0.000 (0.004) loss 2.2939 (2.4504) acc 53.1250 (42.1294) (mean 43.4774 many 43.4774 med nan few nan) lr 1.0000e-02 elapsed 0:01:21 eta 0:17:39\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/782] time 0.313 (0.292) data 0.000 (0.004) loss 2.2939 (2.4504) acc 53.1250 (42.1294) (mean 43.4774 many 43.4774 med nan few nan) lr 1.0000e-02 elapsed 0:01:21 eta 0:17:39\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/782] time 0.324 (0.294) data 0.000 (0.004) loss 2.4518 (2.3913) acc 43.7500 (45.4287) (mean 45.6909 many 45.6909 med nan few nan) lr 1.0000e-02 elapsed 0:01:28 eta 0:17:40\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/782] time 0.324 (0.294) data 0.000 (0.004) loss 2.4518 (2.3913) acc 43.7500 (45.4287) (mean 45.6909 many 45.6909 med nan few nan) lr 1.0000e-02 elapsed 0:01:28 eta 0:17:40\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/782] time 0.325 (0.296) data 0.000 (0.003) loss 2.5756 (2.3781) acc 42.1875 (44.5736) (mean 45.6690 many 45.6690 med nan few nan) lr 1.0000e-02 elapsed 0:01:34 eta 0:17:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/782] time 0.325 (0.296) data 0.000 (0.003) loss 2.5756 (2.3781) acc 42.1875 (44.5736) (mean 45.6690 many 45.6690 med nan few nan) lr 1.0000e-02 elapsed 0:01:34 eta 0:17:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/782] time 0.323 (0.297) data 0.001 (0.003) loss 2.5801 (2.2949) acc 39.0625 (45.9760) (mean 46.4905 many 46.4905 med nan few nan) lr 1.0000e-02 elapsed 0:01:41 eta 0:17:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/782] time 0.323 (0.297) data 0.001 (0.003) loss 2.5801 (2.2949) acc 39.0625 (45.9760) (mean 46.4905 many 46.4905 med nan few nan) lr 1.0000e-02 elapsed 0:01:41 eta 0:17:41\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/782] time 0.314 (0.298) data 0.000 (0.003) loss 2.1167 (2.2391) acc 51.5625 (48.4586) (mean 47.8486 many 47.8486 med nan few nan) lr 1.0000e-02 elapsed 0:01:47 eta 0:17:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/782] time 0.314 (0.298) data 0.000 (0.003) loss 2.1167 (2.2391) acc 51.5625 (48.4586) (mean 47.8486 many 47.8486 med nan few nan) lr 1.0000e-02 elapsed 0:01:47 eta 0:17:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/782] time 0.306 (0.299) data 0.000 (0.003) loss 2.4813 (2.2609) acc 42.1875 (45.1406) (mean 45.4981 many 45.4981 med nan few nan) lr 1.0000e-02 elapsed 0:01:53 eta 0:17:34\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/782] time 0.306 (0.299) data 0.000 (0.003) loss 2.4813 (2.2609) acc 42.1875 (45.1406) (mean 45.4981 many 45.4981 med nan few nan) lr 1.0000e-02 elapsed 0:01:53 eta 0:17:34\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/782] time 0.303 (0.299) data 0.000 (0.003) loss 2.0084 (2.2649) acc 50.0000 (46.3866) (mean 46.5006 many 46.5006 med nan few nan) lr 1.0000e-02 elapsed 0:01:59 eta 0:17:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/782] time 0.303 (0.299) data 0.000 (0.003) loss 2.0084 (2.2649) acc 50.0000 (46.3866) (mean 46.5006 many 46.5006 med nan few nan) lr 1.0000e-02 elapsed 0:01:59 eta 0:17:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/782] time 0.304 (0.299) data 0.000 (0.003) loss 2.1776 (2.2283) acc 48.4375 (47.0400) (mean 47.0010 many 47.0010 med nan few nan) lr 1.0000e-02 elapsed 0:02:05 eta 0:17:24\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/782] time 0.304 (0.299) data 0.000 (0.003) loss 2.1776 (2.2283) acc 48.4375 (47.0400) (mean 47.0010 many 47.0010 med nan few nan) lr 1.0000e-02 elapsed 0:02:05 eta 0:17:24\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/782] time 0.304 (0.299) data 0.000 (0.003) loss 2.3530 (2.3136) acc 43.7500 (44.3739) (mean 44.9617 many 44.9617 med nan few nan) lr 1.0000e-02 elapsed 0:02:11 eta 0:17:18\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/782] time 0.304 (0.299) data 0.000 (0.003) loss 2.3530 (2.3136) acc 43.7500 (44.3739) (mean 44.9617 many 44.9617 med nan few nan) lr 1.0000e-02 elapsed 0:02:11 eta 0:17:18\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/782] time 0.305 (0.300) data 0.000 (0.003) loss 2.5773 (2.2577) acc 42.1875 (47.1501) (mean 47.3439 many 47.3439 med nan few nan) lr 1.0000e-02 elapsed 0:02:17 eta 0:17:13\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/782] time 0.305 (0.300) data 0.000 (0.003) loss 2.5773 (2.2577) acc 42.1875 (47.1501) (mean 47.3439 many 47.3439 med nan few nan) lr 1.0000e-02 elapsed 0:02:17 eta 0:17:13\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/782] time 0.301 (0.300) data 0.000 (0.002) loss 2.3779 (2.2374) acc 45.3125 (47.1640) (mean 47.4169 many 47.4169 med nan few nan) lr 1.0000e-02 elapsed 0:02:23 eta 0:17:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/782] time 0.301 (0.300) data 0.000 (0.002) loss 2.3779 (2.2374) acc 45.3125 (47.1640) (mean 47.4169 many 47.4169 med nan few nan) lr 1.0000e-02 elapsed 0:02:23 eta 0:17:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/782] time 0.303 (0.300) data 0.000 (0.002) loss 1.9531 (2.1983) acc 48.4375 (46.6482) (mean 46.9791 many 46.9791 med nan few nan) lr 1.0000e-02 elapsed 0:02:29 eta 0:17:02\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/782] time 0.303 (0.300) data 0.000 (0.002) loss 1.9531 (2.1983) acc 48.4375 (46.6482) (mean 46.9791 many 46.9791 med nan few nan) lr 1.0000e-02 elapsed 0:02:29 eta 0:17:02\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/782] time 0.300 (0.300) data 0.000 (0.002) loss 2.0310 (2.2341) acc 57.8125 (47.3914) (mean 47.2760 many 47.2760 med nan few nan) lr 1.0000e-02 elapsed 0:02:36 eta 0:16:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/782] time 0.300 (0.300) data 0.000 (0.002) loss 2.0310 (2.2341) acc 57.8125 (47.3914) (mean 47.2760 many 47.2760 med nan few nan) lr 1.0000e-02 elapsed 0:02:36 eta 0:16:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/782] time 0.304 (0.300) data 0.000 (0.002) loss 2.2256 (2.1882) acc 50.0000 (47.6816) (mean 48.1780 many 48.1780 med nan few nan) lr 1.0000e-02 elapsed 0:02:42 eta 0:16:52\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/782] time 0.304 (0.300) data 0.000 (0.002) loss 2.2256 (2.1882) acc 50.0000 (47.6816) (mean 48.1780 many 48.1780 med nan few nan) lr 1.0000e-02 elapsed 0:02:42 eta 0:16:52\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/782] time 0.308 (0.301) data 0.000 (0.002) loss 2.5289 (2.2354) acc 31.2500 (45.5283) (mean 46.7913 many 46.7913 med nan few nan) lr 1.0000e-02 elapsed 0:02:48 eta 0:16:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/782] time 0.308 (0.301) data 0.000 (0.002) loss 2.5289 (2.2354) acc 31.2500 (45.5283) (mean 46.7913 many 46.7913 med nan few nan) lr 1.0000e-02 elapsed 0:02:48 eta 0:16:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/782] time 0.315 (0.301) data 0.000 (0.002) loss 2.3440 (2.1666) acc 39.0625 (48.2945) (mean 48.3177 many 48.3177 med nan few nan) lr 1.0000e-02 elapsed 0:02:54 eta 0:16:42\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/782] time 0.315 (0.301) data 0.000 (0.002) loss 2.3440 (2.1666) acc 39.0625 (48.2945) (mean 48.3177 many 48.3177 med nan few nan) lr 1.0000e-02 elapsed 0:02:54 eta 0:16:42\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/782] time 0.310 (0.301) data 0.000 (0.002) loss 2.0457 (2.1344) acc 50.0000 (48.9227) (mean 49.1225 many 49.1225 med nan few nan) lr 1.0000e-02 elapsed 0:03:00 eta 0:16:37\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/782] time 0.310 (0.301) data 0.000 (0.002) loss 2.0457 (2.1344) acc 50.0000 (48.9227) (mean 49.1225 many 49.1225 med nan few nan) lr 1.0000e-02 elapsed 0:03:00 eta 0:16:37\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/782] time 0.313 (0.302) data 0.000 (0.002) loss 1.6583 (2.1134) acc 68.7500 (48.8086) (mean 48.2766 many 48.2766 med nan few nan) lr 1.0000e-02 elapsed 0:03:07 eta 0:16:32\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/782] time 0.313 (0.302) data 0.000 (0.002) loss 1.6583 (2.1134) acc 68.7500 (48.8086) (mean 48.2766 many 48.2766 med nan few nan) lr 1.0000e-02 elapsed 0:03:07 eta 0:16:32\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [640/782] time 0.309 (0.302) data 0.000 (0.002) loss 2.1089 (2.1688) acc 48.4375 (48.3988) (mean 47.6497 many 47.6497 med nan few nan) lr 1.0000e-02 elapsed 0:03:13 eta 0:16:27\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [640/782] time 0.309 (0.302) data 0.000 (0.002) loss 2.1089 (2.1688) acc 48.4375 (48.3988) (mean 47.6497 many 47.6497 med nan few nan) lr 1.0000e-02 elapsed 0:03:13 eta 0:16:27\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [660/782] time 0.307 (0.302) data 0.000 (0.002) loss 2.1040 (2.1345) acc 54.6875 (49.8070) (mean 49.5252 many 49.5252 med nan few nan) lr 1.0000e-02 elapsed 0:03:19 eta 0:16:22\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [660/782] time 0.307 (0.302) data 0.000 (0.002) loss 2.1040 (2.1345) acc 54.6875 (49.8070) (mean 49.5252 many 49.5252 med nan few nan) lr 1.0000e-02 elapsed 0:03:19 eta 0:16:22\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [680/782] time 0.310 (0.302) data 0.000 (0.002) loss 2.0059 (2.1285) acc 50.0000 (49.6353) (mean 49.4485 many 49.4485 med nan few nan) lr 1.0000e-02 elapsed 0:03:25 eta 0:16:16\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [680/782] time 0.310 (0.302) data 0.000 (0.002) loss 2.0059 (2.1285) acc 50.0000 (49.6353) (mean 49.4485 many 49.4485 med nan few nan) lr 1.0000e-02 elapsed 0:03:25 eta 0:16:16\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [700/782] time 0.308 (0.302) data 0.000 (0.002) loss 1.6542 (2.1673) acc 62.5000 (46.4341) (mean 47.1926 many 47.1926 med nan few nan) lr 1.0000e-02 elapsed 0:03:31 eta 0:16:10\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [700/782] time 0.308 (0.302) data 0.000 (0.002) loss 1.6542 (2.1673) acc 62.5000 (46.4341) (mean 47.1926 many 47.1926 med nan few nan) lr 1.0000e-02 elapsed 0:03:31 eta 0:16:10\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [720/782] time 0.307 (0.302) data 0.000 (0.002) loss 2.2312 (2.1301) acc 50.0000 (49.3750) (mean 48.6465 many 48.6465 med nan few nan) lr 1.0000e-02 elapsed 0:03:37 eta 0:16:04\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [720/782] time 0.307 (0.302) data 0.000 (0.002) loss 2.2312 (2.1301) acc 50.0000 (49.3750) (mean 48.6465 many 48.6465 med nan few nan) lr 1.0000e-02 elapsed 0:03:37 eta 0:16:04\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [740/782] time 0.309 (0.303) data 0.000 (0.002) loss 2.1373 (2.1398) acc 48.4375 (48.5844) (mean 49.2154 many 49.2154 med nan few nan) lr 1.0000e-02 elapsed 0:03:43 eta 0:15:59\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [740/782] time 0.309 (0.303) data 0.000 (0.002) loss 2.1373 (2.1398) acc 48.4375 (48.5844) (mean 49.2154 many 49.2154 med nan few nan) lr 1.0000e-02 elapsed 0:03:43 eta 0:15:59\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [760/782] time 0.309 (0.303) data 0.002 (0.002) loss 2.0970 (2.1419) acc 48.4375 (48.7407) (mean 48.8584 many 48.8584 med nan few nan) lr 1.0000e-02 elapsed 0:03:50 eta 0:15:53\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [760/782] time 0.309 (0.303) data 0.002 (0.002) loss 2.0970 (2.1419) acc 48.4375 (48.7407) (mean 48.8584 many 48.8584 med nan few nan) lr 1.0000e-02 elapsed 0:03:50 eta 0:15:53\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [780/782] time 0.308 (0.303) data 0.000 (0.002) loss 2.1521 (2.1560) acc 43.7500 (47.9573) (mean 48.7676 many 48.7676 med nan few nan) lr 1.0000e-02 elapsed 0:03:56 eta 0:15:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [780/782] time 0.308 (0.303) data 0.000 (0.002) loss 2.1521 (2.1560) acc 43.7500 (47.9573) (mean 48.7676 many 48.7676 med nan few nan) lr 1.0000e-02 elapsed 0:03:56 eta 0:15:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [782/782] time 0.309 (0.303) data 0.000 (0.002) loss 2.4800 (2.1930) acc 50.0000 (48.2048) (mean 48.4905 many 48.4905 med nan few nan) lr 1.0000e-02 elapsed 0:03:56 eta 0:15:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [782/782] time 0.309 (0.303) data 0.000 (0.002) loss 2.4800 (2.1930) acc 50.0000 (48.2048) (mean 48.4905 many 48.4905 med nan few nan) lr 1.0000e-02 elapsed 0:03:56 eta 0:15:47\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/782] time 0.304 (0.303) data 0.000 (0.002) loss 2.0249 (2.0768) acc 43.7500 (48.3916) (mean 48.7006 many 48.7006 med nan few nan) lr 9.0451e-03 elapsed 0:04:03 eta 0:15:42\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/782] time 0.304 (0.303) data 0.000 (0.002) loss 2.0249 (2.0768) acc 43.7500 (48.3916) (mean 48.7006 many 48.7006 med nan few nan) lr 9.0451e-03 elapsed 0:04:03 eta 0:15:42\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/782] time 0.310 (0.303) data 0.000 (0.002) loss 2.1296 (2.1032) acc 48.4375 (48.3552) (mean 48.8837 many 48.8837 med nan few nan) lr 9.0451e-03 elapsed 0:04:09 eta 0:15:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/782] time 0.310 (0.303) data 0.000 (0.002) loss 2.1296 (2.1032) acc 48.4375 (48.3552) (mean 48.8837 many 48.8837 med nan few nan) lr 9.0451e-03 elapsed 0:04:09 eta 0:15:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/782] time 0.305 (0.303) data 0.000 (0.002) loss 2.1334 (2.0810) acc 53.1250 (49.5993) (mean 49.5662 many 49.5662 med nan few nan) lr 9.0451e-03 elapsed 0:04:15 eta 0:15:30\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/782] time 0.305 (0.303) data 0.000 (0.002) loss 2.1334 (2.0810) acc 53.1250 (49.5993) (mean 49.5662 many 49.5662 med nan few nan) lr 9.0451e-03 elapsed 0:04:15 eta 0:15:30\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/782] time 0.309 (0.303) data 0.000 (0.002) loss 2.3518 (2.1114) acc 39.0625 (49.6814) (mean 49.7921 many 49.7921 med nan few nan) lr 9.0451e-03 elapsed 0:04:21 eta 0:15:24\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/782] time 0.309 (0.303) data 0.000 (0.002) loss 2.3518 (2.1114) acc 39.0625 (49.6814) (mean 49.7921 many 49.7921 med nan few nan) lr 9.0451e-03 elapsed 0:04:21 eta 0:15:24\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/782] time 0.308 (0.304) data 0.000 (0.002) loss 2.2739 (2.0667) acc 42.1875 (49.8982) (mean 50.3679 many 50.3679 med nan few nan) lr 9.0451e-03 elapsed 0:04:27 eta 0:15:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/782] time 0.308 (0.304) data 0.000 (0.002) loss 2.2739 (2.0667) acc 42.1875 (49.8982) (mean 50.3679 many 50.3679 med nan few nan) lr 9.0451e-03 elapsed 0:04:27 eta 0:15:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/782] time 0.304 (0.304) data 0.000 (0.002) loss 2.2044 (2.0724) acc 43.7500 (49.1215) (mean 49.5563 many 49.5563 med nan few nan) lr 9.0451e-03 elapsed 0:04:33 eta 0:15:13\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/782] time 0.304 (0.304) data 0.000 (0.002) loss 2.2044 (2.0724) acc 43.7500 (49.1215) (mean 49.5563 many 49.5563 med nan few nan) lr 9.0451e-03 elapsed 0:04:33 eta 0:15:13\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/782] time 0.310 (0.304) data 0.005 (0.002) loss 1.7486 (2.0781) acc 59.3750 (48.3579) (mean 47.5065 many 47.5065 med nan few nan) lr 9.0451e-03 elapsed 0:04:40 eta 0:15:07\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/782] time 0.310 (0.304) data 0.005 (0.002) loss 1.7486 (2.0781) acc 59.3750 (48.3579) (mean 47.5065 many 47.5065 med nan few nan) lr 9.0451e-03 elapsed 0:04:40 eta 0:15:07\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/782] time 0.309 (0.304) data 0.000 (0.002) loss 2.2795 (2.0616) acc 45.3125 (49.3413) (mean 49.0250 many 49.0250 med nan few nan) lr 9.0451e-03 elapsed 0:04:46 eta 0:15:01\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/782] time 0.309 (0.304) data 0.000 (0.002) loss 2.2795 (2.0616) acc 45.3125 (49.3413) (mean 49.0250 many 49.0250 med nan few nan) lr 9.0451e-03 elapsed 0:04:46 eta 0:15:01\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/782] time 0.323 (0.304) data 0.004 (0.002) loss 2.2212 (2.1033) acc 46.8750 (49.6481) (mean 49.7550 many 49.7550 med nan few nan) lr 9.0451e-03 elapsed 0:04:52 eta 0:14:56\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/782] time 0.323 (0.304) data 0.004 (0.002) loss 2.2212 (2.1033) acc 46.8750 (49.6481) (mean 49.7550 many 49.7550 med nan few nan) lr 9.0451e-03 elapsed 0:04:52 eta 0:14:56\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/782] time 0.310 (0.304) data 0.000 (0.002) loss 2.3586 (2.1408) acc 37.5000 (46.5594) (mean 47.8354 many 47.8354 med nan few nan) lr 9.0451e-03 elapsed 0:04:58 eta 0:14:50\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/782] time 0.310 (0.304) data 0.000 (0.002) loss 2.3586 (2.1408) acc 37.5000 (46.5594) (mean 47.8354 many 47.8354 med nan few nan) lr 9.0451e-03 elapsed 0:04:58 eta 0:14:50\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/782] time 0.314 (0.304) data 0.004 (0.002) loss 1.5715 (2.0579) acc 67.1875 (49.5053) (mean 49.7780 many 49.7780 med nan few nan) lr 9.0451e-03 elapsed 0:05:04 eta 0:14:44\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/782] time 0.314 (0.304) data 0.004 (0.002) loss 1.5715 (2.0579) acc 67.1875 (49.5053) (mean 49.7780 many 49.7780 med nan few nan) lr 9.0451e-03 elapsed 0:05:04 eta 0:14:44\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/782] time 0.306 (0.304) data 0.001 (0.002) loss 1.8632 (2.0746) acc 53.1250 (50.5086) (mean 50.7977 many 50.7977 med nan few nan) lr 9.0451e-03 elapsed 0:05:11 eta 0:14:38\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/782] time 0.306 (0.304) data 0.001 (0.002) loss 1.8632 (2.0746) acc 53.1250 (50.5086) (mean 50.7977 many 50.7977 med nan few nan) lr 9.0451e-03 elapsed 0:05:11 eta 0:14:38\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/782] time 0.312 (0.304) data 0.003 (0.002) loss 1.8311 (1.9963) acc 56.2500 (53.0223) (mean 52.5216 many 52.5216 med nan few nan) lr 9.0451e-03 elapsed 0:05:17 eta 0:14:33\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/782] time 0.312 (0.304) data 0.003 (0.002) loss 1.8311 (1.9963) acc 56.2500 (53.0223) (mean 52.5216 many 52.5216 med nan few nan) lr 9.0451e-03 elapsed 0:05:17 eta 0:14:33\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/782] time 0.307 (0.305) data 0.000 (0.002) loss 2.2686 (2.0587) acc 48.4375 (51.0121) (mean 51.3713 many 51.3713 med nan few nan) lr 9.0451e-03 elapsed 0:05:23 eta 0:14:27\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/782] time 0.307 (0.305) data 0.000 (0.002) loss 2.2686 (2.0587) acc 48.4375 (51.0121) (mean 51.3713 many 51.3713 med nan few nan) lr 9.0451e-03 elapsed 0:05:23 eta 0:14:27\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/782] time 0.309 (0.305) data 0.002 (0.002) loss 2.3129 (2.1190) acc 35.9375 (48.1977) (mean 49.5950 many 49.5950 med nan few nan) lr 9.0451e-03 elapsed 0:05:29 eta 0:14:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/782] time 0.309 (0.305) data 0.002 (0.002) loss 2.3129 (2.1190) acc 35.9375 (48.1977) (mean 49.5950 many 49.5950 med nan few nan) lr 9.0451e-03 elapsed 0:05:29 eta 0:14:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/782] time 0.312 (0.305) data 0.000 (0.002) loss 2.1030 (2.1444) acc 51.5625 (47.9264) (mean 48.7078 many 48.7078 med nan few nan) lr 9.0451e-03 elapsed 0:05:36 eta 0:14:15\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/782] time 0.312 (0.305) data 0.000 (0.002) loss 2.1030 (2.1444) acc 51.5625 (47.9264) (mean 48.7078 many 48.7078 med nan few nan) lr 9.0451e-03 elapsed 0:05:36 eta 0:14:15\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/782] time 0.306 (0.305) data 0.003 (0.002) loss 1.8908 (2.0188) acc 54.6875 (50.7128) (mean 50.7910 many 50.7910 med nan few nan) lr 9.0451e-03 elapsed 0:05:42 eta 0:14:10\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/782] time 0.306 (0.305) data 0.003 (0.002) loss 1.8908 (2.0188) acc 54.6875 (50.7128) (mean 50.7910 many 50.7910 med nan few nan) lr 9.0451e-03 elapsed 0:05:42 eta 0:14:10\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/782] time 0.317 (0.305) data 0.000 (0.002) loss 1.5043 (1.9595) acc 57.8125 (50.1401) (mean 49.2669 many 49.2669 med nan few nan) lr 9.0451e-03 elapsed 0:05:48 eta 0:14:04\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/782] time 0.317 (0.305) data 0.000 (0.002) loss 1.5043 (1.9595) acc 57.8125 (50.1401) (mean 49.2669 many 49.2669 med nan few nan) lr 9.0451e-03 elapsed 0:05:48 eta 0:14:04\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/782] time 0.306 (0.305) data 0.000 (0.002) loss 2.4890 (2.1021) acc 39.0625 (48.7834) (mean 49.3580 many 49.3580 med nan few nan) lr 9.0451e-03 elapsed 0:05:54 eta 0:13:58\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/782] time 0.306 (0.305) data 0.000 (0.002) loss 2.4890 (2.1021) acc 39.0625 (48.7834) (mean 49.3580 many 49.3580 med nan few nan) lr 9.0451e-03 elapsed 0:05:54 eta 0:13:58\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/782] time 0.315 (0.305) data 0.000 (0.002) loss 1.8752 (1.9812) acc 51.5625 (50.3986) (mean 50.2574 many 50.2574 med nan few nan) lr 9.0451e-03 elapsed 0:06:00 eta 0:13:52\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/782] time 0.315 (0.305) data 0.000 (0.002) loss 1.8752 (1.9812) acc 51.5625 (50.3986) (mean 50.2574 many 50.2574 med nan few nan) lr 9.0451e-03 elapsed 0:06:00 eta 0:13:52\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/782] time 0.307 (0.305) data 0.000 (0.002) loss 1.9731 (2.0413) acc 51.5625 (49.1517) (mean 49.4064 many 49.4064 med nan few nan) lr 9.0451e-03 elapsed 0:06:07 eta 0:13:46\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/782] time 0.307 (0.305) data 0.000 (0.002) loss 1.9731 (2.0413) acc 51.5625 (49.1517) (mean 49.4064 many 49.4064 med nan few nan) lr 9.0451e-03 elapsed 0:06:07 eta 0:13:46\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/782] time 0.312 (0.305) data 0.000 (0.002) loss 1.7210 (2.0106) acc 56.2500 (51.0395) (mean 50.7380 many 50.7380 med nan few nan) lr 9.0451e-03 elapsed 0:06:13 eta 0:13:40\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/782] time 0.312 (0.305) data 0.000 (0.002) loss 1.7210 (2.0106) acc 56.2500 (51.0395) (mean 50.7380 many 50.7380 med nan few nan) lr 9.0451e-03 elapsed 0:06:13 eta 0:13:40\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/782] time 0.307 (0.306) data 0.000 (0.002) loss 1.8020 (2.0269) acc 54.6875 (50.3735) (mean 50.2170 many 50.2170 med nan few nan) lr 9.0451e-03 elapsed 0:06:19 eta 0:13:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/782] time 0.307 (0.306) data 0.000 (0.002) loss 1.8020 (2.0269) acc 54.6875 (50.3735) (mean 50.2170 many 50.2170 med nan few nan) lr 9.0451e-03 elapsed 0:06:19 eta 0:13:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/782] time 0.314 (0.306) data 0.000 (0.002) loss 2.4354 (2.0582) acc 43.7500 (50.4146) (mean 50.2319 many 50.2319 med nan few nan) lr 9.0451e-03 elapsed 0:06:25 eta 0:13:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/782] time 0.314 (0.306) data 0.000 (0.002) loss 2.4354 (2.0582) acc 43.7500 (50.4146) (mean 50.2319 many 50.2319 med nan few nan) lr 9.0451e-03 elapsed 0:06:25 eta 0:13:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/782] time 0.310 (0.306) data 0.000 (0.002) loss 1.8234 (2.0464) acc 56.2500 (50.7557) (mean 51.0438 many 51.0438 med nan few nan) lr 9.0451e-03 elapsed 0:06:31 eta 0:13:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/782] time 0.310 (0.306) data 0.000 (0.002) loss 1.8234 (2.0464) acc 56.2500 (50.7557) (mean 51.0438 many 51.0438 med nan few nan) lr 9.0451e-03 elapsed 0:06:31 eta 0:13:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/782] time 0.310 (0.306) data 0.000 (0.002) loss 1.9477 (2.0389) acc 50.0000 (50.3637) (mean 50.8334 many 50.8334 med nan few nan) lr 9.0451e-03 elapsed 0:06:38 eta 0:13:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/782] time 0.310 (0.306) data 0.000 (0.002) loss 1.9477 (2.0389) acc 50.0000 (50.3637) (mean 50.8334 many 50.8334 med nan few nan) lr 9.0451e-03 elapsed 0:06:38 eta 0:13:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/782] time 0.313 (0.306) data 0.001 (0.002) loss 2.2081 (1.9978) acc 51.5625 (52.0705) (mean 50.7831 many 50.7831 med nan few nan) lr 9.0451e-03 elapsed 0:06:44 eta 0:13:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/782] time 0.313 (0.306) data 0.001 (0.002) loss 2.2081 (1.9978) acc 51.5625 (52.0705) (mean 50.7831 many 50.7831 med nan few nan) lr 9.0451e-03 elapsed 0:06:44 eta 0:13:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/782] time 0.311 (0.306) data 0.001 (0.002) loss 1.9178 (1.9984) acc 62.5000 (51.9473) (mean 52.4133 many 52.4133 med nan few nan) lr 9.0451e-03 elapsed 0:06:50 eta 0:13:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/782] time 0.311 (0.306) data 0.001 (0.002) loss 1.9178 (1.9984) acc 62.5000 (51.9473) (mean 52.4133 many 52.4133 med nan few nan) lr 9.0451e-03 elapsed 0:06:50 eta 0:13:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/782] time 0.307 (0.306) data 0.000 (0.002) loss 2.0704 (2.0227) acc 43.7500 (50.4568) (mean 51.3595 many 51.3595 med nan few nan) lr 9.0451e-03 elapsed 0:06:56 eta 0:12:59\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/782] time 0.307 (0.306) data 0.000 (0.002) loss 2.0704 (2.0227) acc 43.7500 (50.4568) (mean 51.3595 many 51.3595 med nan few nan) lr 9.0451e-03 elapsed 0:06:56 eta 0:12:59\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [600/782] time 0.312 (0.306) data 0.000 (0.002) loss 2.3151 (2.0768) acc 45.3125 (50.2698) (mean 50.7739 many 50.7739 med nan few nan) lr 9.0451e-03 elapsed 0:07:02 eta 0:12:53\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [600/782] time 0.312 (0.306) data 0.000 (0.002) loss 2.3151 (2.0768) acc 45.3125 (50.2698) (mean 50.7739 many 50.7739 med nan few nan) lr 9.0451e-03 elapsed 0:07:02 eta 0:12:53\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/782] time 0.311 (0.306) data 0.000 (0.001) loss 1.9190 (1.9826) acc 50.0000 (50.7392) (mean 50.4451 many 50.4451 med nan few nan) lr 9.0451e-03 elapsed 0:07:09 eta 0:12:47\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/782] time 0.311 (0.306) data 0.000 (0.001) loss 1.9190 (1.9826) acc 50.0000 (50.7392) (mean 50.4451 many 50.4451 med nan few nan) lr 9.0451e-03 elapsed 0:07:09 eta 0:12:47\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [640/782] time 0.311 (0.306) data 0.000 (0.001) loss 2.0426 (2.0561) acc 46.8750 (48.9796) (mean 49.9001 many 49.9001 med nan few nan) lr 9.0451e-03 elapsed 0:07:15 eta 0:12:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [640/782] time 0.311 (0.306) data 0.000 (0.001) loss 2.0426 (2.0561) acc 46.8750 (48.9796) (mean 49.9001 many 49.9001 med nan few nan) lr 9.0451e-03 elapsed 0:07:15 eta 0:12:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [660/782] time 0.311 (0.306) data 0.000 (0.001) loss 1.8053 (1.9881) acc 56.2500 (51.2447) (mean 50.7148 many 50.7148 med nan few nan) lr 9.0451e-03 elapsed 0:07:21 eta 0:12:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [660/782] time 0.311 (0.306) data 0.000 (0.001) loss 1.8053 (1.9881) acc 56.2500 (51.2447) (mean 50.7148 many 50.7148 med nan few nan) lr 9.0451e-03 elapsed 0:07:21 eta 0:12:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [680/782] time 0.308 (0.306) data 0.000 (0.001) loss 2.2317 (1.9964) acc 39.0625 (51.1206) (mean 52.0875 many 52.0875 med nan few nan) lr 9.0451e-03 elapsed 0:07:27 eta 0:12:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [680/782] time 0.308 (0.306) data 0.000 (0.001) loss 2.2317 (1.9964) acc 39.0625 (51.1206) (mean 52.0875 many 52.0875 med nan few nan) lr 9.0451e-03 elapsed 0:07:27 eta 0:12:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [700/782] time 0.312 (0.306) data 0.000 (0.001) loss 2.2209 (2.0629) acc 40.6250 (49.3582) (mean 50.4164 many 50.4164 med nan few nan) lr 9.0451e-03 elapsed 0:07:34 eta 0:12:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [700/782] time 0.312 (0.306) data 0.000 (0.001) loss 2.2209 (2.0629) acc 40.6250 (49.3582) (mean 50.4164 many 50.4164 med nan few nan) lr 9.0451e-03 elapsed 0:07:34 eta 0:12:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [720/782] time 0.315 (0.306) data 0.000 (0.001) loss 1.9511 (2.0076) acc 51.5625 (50.4519) (mean 50.7761 many 50.7761 med nan few nan) lr 9.0451e-03 elapsed 0:07:40 eta 0:12:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [720/782] time 0.315 (0.306) data 0.000 (0.001) loss 1.9511 (2.0076) acc 51.5625 (50.4519) (mean 50.7761 many 50.7761 med nan few nan) lr 9.0451e-03 elapsed 0:07:40 eta 0:12:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [740/782] time 0.311 (0.306) data 0.000 (0.001) loss 1.5621 (2.0084) acc 62.5000 (51.2851) (mean 51.0554 many 51.0554 med nan few nan) lr 9.0451e-03 elapsed 0:07:46 eta 0:12:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [740/782] time 0.311 (0.306) data 0.000 (0.001) loss 1.5621 (2.0084) acc 62.5000 (51.2851) (mean 51.0554 many 51.0554 med nan few nan) lr 9.0451e-03 elapsed 0:07:46 eta 0:12:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [760/782] time 0.314 (0.306) data 0.000 (0.001) loss 1.7626 (2.0638) acc 57.8125 (49.5462) (mean 50.2599 many 50.2599 med nan few nan) lr 9.0451e-03 elapsed 0:07:52 eta 0:12:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [760/782] time 0.314 (0.306) data 0.000 (0.001) loss 1.7626 (2.0638) acc 57.8125 (49.5462) (mean 50.2599 many 50.2599 med nan few nan) lr 9.0451e-03 elapsed 0:07:52 eta 0:12:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [780/782] time 0.313 (0.307) data 0.000 (0.001) loss 1.8304 (2.0045) acc 62.5000 (51.1975) (mean 50.5214 many 50.5214 med nan few nan) lr 9.0451e-03 elapsed 0:07:58 eta 0:11:59\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [780/782] time 0.313 (0.307) data 0.000 (0.001) loss 1.8304 (2.0045) acc 62.5000 (51.1975) (mean 50.5214 many 50.5214 med nan few nan) lr 9.0451e-03 elapsed 0:07:58 eta 0:11:59\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [782/782] time 0.306 (0.307) data 0.000 (0.001) loss 2.9624 (2.1237) acc 25.0000 (47.2044) (mean 49.4515 many 49.4515 med nan few nan) lr 9.0451e-03 elapsed 0:07:59 eta 0:11:59\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [782/782] time 0.306 (0.307) data 0.000 (0.001) loss 2.9624 (2.1237) acc 25.0000 (47.2044) (mean 49.4515 many 49.4515 med nan few nan) lr 9.0451e-03 elapsed 0:07:59 eta 0:11:59\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/782] time 0.309 (0.307) data 0.000 (0.002) loss 1.9198 (1.9754) acc 54.6875 (51.3065) (mean 50.4491 many 50.4491 med nan few nan) lr 6.5451e-03 elapsed 0:08:06 eta 0:11:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/782] time 0.309 (0.307) data 0.000 (0.002) loss 1.9198 (1.9754) acc 54.6875 (51.3065) (mean 50.4491 many 50.4491 med nan few nan) lr 6.5451e-03 elapsed 0:08:06 eta 0:11:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/782] time 0.310 (0.307) data 0.000 (0.002) loss 1.8536 (2.0018) acc 51.5625 (52.2753) (mean 51.7962 many 51.7962 med nan few nan) lr 6.5451e-03 elapsed 0:08:12 eta 0:11:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/782] time 0.310 (0.307) data 0.000 (0.002) loss 1.8536 (2.0018) acc 51.5625 (52.2753) (mean 51.7962 many 51.7962 med nan few nan) lr 6.5451e-03 elapsed 0:08:12 eta 0:11:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/782] time 0.312 (0.307) data 0.002 (0.002) loss 2.2598 (1.9877) acc 46.8750 (52.7145) (mean 52.7356 many 52.7356 med nan few nan) lr 6.5451e-03 elapsed 0:08:18 eta 0:11:41\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/782] time 0.312 (0.307) data 0.002 (0.002) loss 2.2598 (1.9877) acc 46.8750 (52.7145) (mean 52.7356 many 52.7356 med nan few nan) lr 6.5451e-03 elapsed 0:08:18 eta 0:11:41\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/782] time 0.306 (0.307) data 0.000 (0.002) loss 2.3688 (2.0569) acc 45.3125 (49.3953) (mean 50.8806 many 50.8806 med nan few nan) lr 6.5451e-03 elapsed 0:08:24 eta 0:11:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/782] time 0.306 (0.307) data 0.000 (0.002) loss 2.3688 (2.0569) acc 45.3125 (49.3953) (mean 50.8806 many 50.8806 med nan few nan) lr 6.5451e-03 elapsed 0:08:24 eta 0:11:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/782] time 0.307 (0.307) data 0.000 (0.002) loss 1.7484 (1.9725) acc 59.3750 (50.8066) (mean 51.2495 many 51.2495 med nan few nan) lr 6.5451e-03 elapsed 0:08:30 eta 0:11:29\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/782] time 0.307 (0.307) data 0.000 (0.002) loss 1.7484 (1.9725) acc 59.3750 (50.8066) (mean 51.2495 many 51.2495 med nan few nan) lr 6.5451e-03 elapsed 0:08:30 eta 0:11:29\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/782] time 0.311 (0.307) data 0.000 (0.002) loss 1.9176 (1.9985) acc 51.5625 (49.9317) (mean 50.5458 many 50.5458 med nan few nan) lr 6.5451e-03 elapsed 0:08:37 eta 0:11:23\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/782] time 0.311 (0.307) data 0.000 (0.002) loss 1.9176 (1.9985) acc 51.5625 (49.9317) (mean 50.5458 many 50.5458 med nan few nan) lr 6.5451e-03 elapsed 0:08:37 eta 0:11:23\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/782] time 0.310 (0.307) data 0.000 (0.002) loss 1.9160 (1.9768) acc 50.0000 (52.9662) (mean 52.4125 many 52.4125 med nan few nan) lr 6.5451e-03 elapsed 0:08:43 eta 0:11:17\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/782] time 0.310 (0.307) data 0.000 (0.002) loss 1.9160 (1.9768) acc 50.0000 (52.9662) (mean 52.4125 many 52.4125 med nan few nan) lr 6.5451e-03 elapsed 0:08:43 eta 0:11:17\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/782] time 0.308 (0.307) data 0.000 (0.002) loss 1.9927 (1.8944) acc 50.0000 (54.8599) (mean 55.2249 many 55.2249 med nan few nan) lr 6.5451e-03 elapsed 0:08:49 eta 0:11:11\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/782] time 0.308 (0.307) data 0.000 (0.002) loss 1.9927 (1.8944) acc 50.0000 (54.8599) (mean 55.2249 many 55.2249 med nan few nan) lr 6.5451e-03 elapsed 0:08:49 eta 0:11:11\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/782] time 0.312 (0.307) data 0.000 (0.002) loss 1.7374 (1.9740) acc 68.7500 (52.7594) (mean 52.5807 many 52.5807 med nan few nan) lr 6.5451e-03 elapsed 0:08:55 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/782] time 0.312 (0.307) data 0.000 (0.002) loss 1.7374 (1.9740) acc 68.7500 (52.7594) (mean 52.5807 many 52.5807 med nan few nan) lr 6.5451e-03 elapsed 0:08:55 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/782] time 0.315 (0.307) data 0.000 (0.002) loss 1.8983 (2.0005) acc 62.5000 (52.7712) (mean 52.5100 many 52.5100 med nan few nan) lr 6.5451e-03 elapsed 0:09:01 eta 0:10:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/782] time 0.315 (0.307) data 0.000 (0.002) loss 1.8983 (2.0005) acc 62.5000 (52.7712) (mean 52.5100 many 52.5100 med nan few nan) lr 6.5451e-03 elapsed 0:09:01 eta 0:10:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/782] time 0.308 (0.307) data 0.000 (0.002) loss 2.0045 (2.0136) acc 53.1250 (51.3507) (mean 52.1840 many 52.1840 med nan few nan) lr 6.5451e-03 elapsed 0:09:08 eta 0:10:52\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/782] time 0.308 (0.307) data 0.000 (0.002) loss 2.0045 (2.0136) acc 53.1250 (51.3507) (mean 52.1840 many 52.1840 med nan few nan) lr 6.5451e-03 elapsed 0:09:08 eta 0:10:52\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/782] time 0.307 (0.307) data 0.000 (0.002) loss 1.9864 (2.0180) acc 51.5625 (51.2836) (mean 51.8064 many 51.8064 med nan few nan) lr 6.5451e-03 elapsed 0:09:14 eta 0:10:46\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/782] time 0.307 (0.307) data 0.000 (0.002) loss 1.9864 (2.0180) acc 51.5625 (51.2836) (mean 51.8064 many 51.8064 med nan few nan) lr 6.5451e-03 elapsed 0:09:14 eta 0:10:46\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/782] time 0.314 (0.307) data 0.000 (0.002) loss 1.9997 (1.9562) acc 54.6875 (51.4929) (mean 51.2717 many 51.2717 med nan few nan) lr 6.5451e-03 elapsed 0:09:20 eta 0:10:40\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/782] time 0.314 (0.307) data 0.000 (0.002) loss 1.9997 (1.9562) acc 54.6875 (51.4929) (mean 51.2717 many 51.2717 med nan few nan) lr 6.5451e-03 elapsed 0:09:20 eta 0:10:40\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/782] time 0.306 (0.307) data 0.000 (0.002) loss 1.8093 (1.9528) acc 51.5625 (51.4148) (mean 51.2404 many 51.2404 med nan few nan) lr 6.5451e-03 elapsed 0:09:26 eta 0:10:34\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/782] time 0.306 (0.307) data 0.000 (0.002) loss 1.8093 (1.9528) acc 51.5625 (51.4148) (mean 51.2404 many 51.2404 med nan few nan) lr 6.5451e-03 elapsed 0:09:26 eta 0:10:34\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/782] time 0.307 (0.307) data 0.000 (0.001) loss 1.9368 (1.9364) acc 54.6875 (52.6000) (mean 52.9582 many 52.9582 med nan few nan) lr 6.5451e-03 elapsed 0:09:32 eta 0:10:28\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/782] time 0.307 (0.307) data 0.000 (0.001) loss 1.9368 (1.9364) acc 54.6875 (52.6000) (mean 52.9582 many 52.9582 med nan few nan) lr 6.5451e-03 elapsed 0:09:32 eta 0:10:28\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/782] time 0.304 (0.307) data 0.000 (0.001) loss 1.6444 (1.9223) acc 62.5000 (53.8425) (mean 53.3681 many 53.3681 med nan few nan) lr 6.5451e-03 elapsed 0:09:39 eta 0:10:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/782] time 0.304 (0.307) data 0.000 (0.001) loss 1.6444 (1.9223) acc 62.5000 (53.8425) (mean 53.3681 many 53.3681 med nan few nan) lr 6.5451e-03 elapsed 0:09:39 eta 0:10:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/782] time 0.308 (0.307) data 0.000 (0.001) loss 2.3847 (1.9735) acc 45.3125 (52.3975) (mean 52.4540 many 52.4540 med nan few nan) lr 6.5451e-03 elapsed 0:09:45 eta 0:10:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/782] time 0.308 (0.307) data 0.000 (0.001) loss 2.3847 (1.9735) acc 45.3125 (52.3975) (mean 52.4540 many 52.4540 med nan few nan) lr 6.5451e-03 elapsed 0:09:45 eta 0:10:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/782] time 0.311 (0.307) data 0.000 (0.001) loss 2.4056 (2.0139) acc 37.5000 (49.9982) (mean 51.2569 many 51.2569 med nan few nan) lr 6.5451e-03 elapsed 0:09:51 eta 0:10:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/782] time 0.311 (0.307) data 0.000 (0.001) loss 2.4056 (2.0139) acc 37.5000 (49.9982) (mean 51.2569 many 51.2569 med nan few nan) lr 6.5451e-03 elapsed 0:09:51 eta 0:10:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/782] time 0.306 (0.307) data 0.000 (0.001) loss 1.9744 (1.9416) acc 56.2500 (53.0539) (mean 52.8053 many 52.8053 med nan few nan) lr 6.5451e-03 elapsed 0:09:57 eta 0:10:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/782] time 0.306 (0.307) data 0.000 (0.001) loss 1.9744 (1.9416) acc 56.2500 (53.0539) (mean 52.8053 many 52.8053 med nan few nan) lr 6.5451e-03 elapsed 0:09:57 eta 0:10:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/782] time 0.309 (0.307) data 0.000 (0.001) loss 1.6301 (1.8888) acc 53.1250 (53.3853) (mean 53.4319 many 53.4319 med nan few nan) lr 6.5451e-03 elapsed 0:10:03 eta 0:09:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/782] time 0.309 (0.307) data 0.000 (0.001) loss 1.6301 (1.8888) acc 53.1250 (53.3853) (mean 53.4319 many 53.4319 med nan few nan) lr 6.5451e-03 elapsed 0:10:03 eta 0:09:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/782] time 0.309 (0.307) data 0.000 (0.001) loss 2.0964 (1.9492) acc 48.4375 (54.0774) (mean 54.9461 many 54.9461 med nan few nan) lr 6.5451e-03 elapsed 0:10:10 eta 0:09:52\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/782] time 0.309 (0.307) data 0.000 (0.001) loss 2.0964 (1.9492) acc 48.4375 (54.0774) (mean 54.9461 many 54.9461 med nan few nan) lr 6.5451e-03 elapsed 0:10:10 eta 0:09:52\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/782] time 0.314 (0.307) data 0.000 (0.001) loss 2.0495 (2.0243) acc 43.7500 (48.8765) (mean 50.8014 many 50.8014 med nan few nan) lr 6.5451e-03 elapsed 0:10:16 eta 0:09:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/782] time 0.314 (0.307) data 0.000 (0.001) loss 2.0495 (2.0243) acc 43.7500 (48.8765) (mean 50.8014 many 50.8014 med nan few nan) lr 6.5451e-03 elapsed 0:10:16 eta 0:09:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/782] time 0.305 (0.307) data 0.000 (0.001) loss 1.9475 (1.9991) acc 51.5625 (52.5720) (mean 52.9688 many 52.9688 med nan few nan) lr 6.5451e-03 elapsed 0:10:22 eta 0:09:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/782] time 0.305 (0.307) data 0.000 (0.001) loss 1.9475 (1.9991) acc 51.5625 (52.5720) (mean 52.9688 many 52.9688 med nan few nan) lr 6.5451e-03 elapsed 0:10:22 eta 0:09:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/782] time 0.312 (0.307) data 0.000 (0.001) loss 1.7419 (1.9712) acc 65.6250 (52.3679) (mean 53.0533 many 53.0533 med nan few nan) lr 6.5451e-03 elapsed 0:10:28 eta 0:09:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/782] time 0.312 (0.307) data 0.000 (0.001) loss 1.7419 (1.9712) acc 65.6250 (52.3679) (mean 53.0533 many 53.0533 med nan few nan) lr 6.5451e-03 elapsed 0:10:28 eta 0:09:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/782] time 0.314 (0.307) data 0.001 (0.001) loss 2.1750 (1.9202) acc 45.3125 (53.3387) (mean 53.7912 many 53.7912 med nan few nan) lr 6.5451e-03 elapsed 0:10:34 eta 0:09:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/782] time 0.314 (0.307) data 0.001 (0.001) loss 2.1750 (1.9202) acc 45.3125 (53.3387) (mean 53.7912 many 53.7912 med nan few nan) lr 6.5451e-03 elapsed 0:10:34 eta 0:09:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/782] time 0.307 (0.307) data 0.000 (0.001) loss 2.0712 (1.9740) acc 50.0000 (51.0071) (mean 51.7547 many 51.7547 med nan few nan) lr 6.5451e-03 elapsed 0:10:40 eta 0:09:21\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/782] time 0.307 (0.307) data 0.000 (0.001) loss 2.0712 (1.9740) acc 50.0000 (51.0071) (mean 51.7547 many 51.7547 med nan few nan) lr 6.5451e-03 elapsed 0:10:40 eta 0:09:21\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/782] time 0.306 (0.308) data 0.001 (0.001) loss 1.6279 (2.0163) acc 71.8750 (53.3453) (mean 52.4784 many 52.4784 med nan few nan) lr 6.5451e-03 elapsed 0:10:47 eta 0:09:15\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/782] time 0.306 (0.308) data 0.001 (0.001) loss 1.6279 (2.0163) acc 71.8750 (53.3453) (mean 52.4784 many 52.4784 med nan few nan) lr 6.5451e-03 elapsed 0:10:47 eta 0:09:15\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/782] time 0.313 (0.308) data 0.000 (0.001) loss 2.3112 (1.9681) acc 46.8750 (52.9176) (mean 52.6097 many 52.6097 med nan few nan) lr 6.5451e-03 elapsed 0:10:53 eta 0:09:09\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/782] time 0.313 (0.308) data 0.000 (0.001) loss 2.3112 (1.9681) acc 46.8750 (52.9176) (mean 52.6097 many 52.6097 med nan few nan) lr 6.5451e-03 elapsed 0:10:53 eta 0:09:09\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.9446 (1.9798) acc 59.3750 (54.0392) (mean 53.8434 many 53.8434 med nan few nan) lr 6.5451e-03 elapsed 0:10:59 eta 0:09:03\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.9446 (1.9798) acc 59.3750 (54.0392) (mean 53.8434 many 53.8434 med nan few nan) lr 6.5451e-03 elapsed 0:10:59 eta 0:09:03\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.8593 (1.9981) acc 54.6875 (52.0020) (mean 52.2691 many 52.2691 med nan few nan) lr 6.5451e-03 elapsed 0:11:05 eta 0:08:57\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.8593 (1.9981) acc 54.6875 (52.0020) (mean 52.2691 many 52.2691 med nan few nan) lr 6.5451e-03 elapsed 0:11:05 eta 0:08:57\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/782] time 0.308 (0.308) data 0.002 (0.001) loss 1.9127 (1.9343) acc 57.8125 (53.9900) (mean 54.0952 many 54.0952 med nan few nan) lr 6.5451e-03 elapsed 0:11:11 eta 0:08:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/782] time 0.308 (0.308) data 0.002 (0.001) loss 1.9127 (1.9343) acc 57.8125 (53.9900) (mean 54.0952 many 54.0952 med nan few nan) lr 6.5451e-03 elapsed 0:11:11 eta 0:08:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [640/782] time 0.304 (0.308) data 0.000 (0.001) loss 1.9645 (1.9522) acc 57.8125 (54.0220) (mean 54.6778 many 54.6778 med nan few nan) lr 6.5451e-03 elapsed 0:11:18 eta 0:08:44\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [640/782] time 0.304 (0.308) data 0.000 (0.001) loss 1.9645 (1.9522) acc 57.8125 (54.0220) (mean 54.6778 many 54.6778 med nan few nan) lr 6.5451e-03 elapsed 0:11:18 eta 0:08:44\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [660/782] time 0.316 (0.308) data 0.003 (0.001) loss 1.7330 (1.9657) acc 64.0625 (53.1085) (mean 52.8802 many 52.8802 med nan few nan) lr 6.5451e-03 elapsed 0:11:24 eta 0:08:38\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [660/782] time 0.316 (0.308) data 0.003 (0.001) loss 1.7330 (1.9657) acc 64.0625 (53.1085) (mean 52.8802 many 52.8802 med nan few nan) lr 6.5451e-03 elapsed 0:11:24 eta 0:08:38\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [680/782] time 0.312 (0.308) data 0.000 (0.001) loss 1.9675 (2.0542) acc 54.6875 (49.8754) (mean 50.2937 many 50.2937 med nan few nan) lr 6.5451e-03 elapsed 0:11:30 eta 0:08:32\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [680/782] time 0.312 (0.308) data 0.000 (0.001) loss 1.9675 (2.0542) acc 54.6875 (49.8754) (mean 50.2937 many 50.2937 med nan few nan) lr 6.5451e-03 elapsed 0:11:30 eta 0:08:32\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [700/782] time 0.314 (0.308) data 0.000 (0.001) loss 2.1839 (2.0288) acc 48.4375 (50.8035) (mean 51.4565 many 51.4565 med nan few nan) lr 6.5451e-03 elapsed 0:11:36 eta 0:08:26\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [700/782] time 0.314 (0.308) data 0.000 (0.001) loss 2.1839 (2.0288) acc 48.4375 (50.8035) (mean 51.4565 many 51.4565 med nan few nan) lr 6.5451e-03 elapsed 0:11:36 eta 0:08:26\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [720/782] time 0.313 (0.308) data 0.000 (0.001) loss 2.2431 (2.0211) acc 51.5625 (51.6227) (mean 52.1329 many 52.1329 med nan few nan) lr 6.5451e-03 elapsed 0:11:42 eta 0:08:20\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [720/782] time 0.313 (0.308) data 0.000 (0.001) loss 2.2431 (2.0211) acc 51.5625 (51.6227) (mean 52.1329 many 52.1329 med nan few nan) lr 6.5451e-03 elapsed 0:11:42 eta 0:08:20\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [740/782] time 0.302 (0.308) data 0.002 (0.001) loss 2.1564 (2.0213) acc 43.7500 (51.8904) (mean 52.5453 many 52.5453 med nan few nan) lr 6.5451e-03 elapsed 0:11:49 eta 0:08:14\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [740/782] time 0.302 (0.308) data 0.002 (0.001) loss 2.1564 (2.0213) acc 43.7500 (51.8904) (mean 52.5453 many 52.5453 med nan few nan) lr 6.5451e-03 elapsed 0:11:49 eta 0:08:14\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [760/782] time 0.311 (0.308) data 0.000 (0.001) loss 1.9503 (2.0104) acc 54.6875 (51.5766) (mean 51.2330 many 51.2330 med nan few nan) lr 6.5451e-03 elapsed 0:11:55 eta 0:08:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [760/782] time 0.311 (0.308) data 0.000 (0.001) loss 1.9503 (2.0104) acc 54.6875 (51.5766) (mean 51.2330 many 51.2330 med nan few nan) lr 6.5451e-03 elapsed 0:11:55 eta 0:08:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [780/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.7883 (1.9909) acc 59.3750 (53.3908) (mean 53.1594 many 53.1594 med nan few nan) lr 6.5451e-03 elapsed 0:12:01 eta 0:08:01\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [780/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.7883 (1.9909) acc 59.3750 (53.3908) (mean 53.1594 many 53.1594 med nan few nan) lr 6.5451e-03 elapsed 0:12:01 eta 0:08:01\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [782/782] time 0.301 (0.308) data 0.000 (0.001) loss 2.1800 (1.9912) acc 37.5000 (51.7778) (mean 52.7855 many 52.7855 med nan few nan) lr 6.5451e-03 elapsed 0:12:01 eta 0:08:01\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [782/782] time 0.301 (0.308) data 0.000 (0.001) loss 2.1800 (1.9912) acc 37.5000 (51.7778) (mean 52.7855 many 52.7855 med nan few nan) lr 6.5451e-03 elapsed 0:12:01 eta 0:08:01\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/782] time 0.307 (0.308) data 0.000 (0.002) loss 2.2567 (2.0209) acc 39.0625 (50.3625) (mean 51.3708 many 51.3708 med nan few nan) lr 3.4549e-03 elapsed 0:12:08 eta 0:07:55\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/782] time 0.307 (0.308) data 0.000 (0.002) loss 2.2567 (2.0209) acc 39.0625 (50.3625) (mean 51.3708 many 51.3708 med nan few nan) lr 3.4549e-03 elapsed 0:12:08 eta 0:07:55\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/782] time 0.313 (0.308) data 0.001 (0.002) loss 2.0742 (1.9566) acc 43.7500 (52.9137) (mean 52.7477 many 52.7477 med nan few nan) lr 3.4549e-03 elapsed 0:12:14 eta 0:07:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/782] time 0.313 (0.308) data 0.001 (0.002) loss 2.0742 (1.9566) acc 43.7500 (52.9137) (mean 52.7477 many 52.7477 med nan few nan) lr 3.4549e-03 elapsed 0:12:14 eta 0:07:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/782] time 0.309 (0.308) data 0.000 (0.002) loss 1.8846 (1.9116) acc 51.5625 (53.7785) (mean 53.4334 many 53.4334 med nan few nan) lr 3.4549e-03 elapsed 0:12:21 eta 0:07:43\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/782] time 0.309 (0.308) data 0.000 (0.002) loss 1.8846 (1.9116) acc 51.5625 (53.7785) (mean 53.4334 many 53.4334 med nan few nan) lr 3.4549e-03 elapsed 0:12:21 eta 0:07:43\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/782] time 0.302 (0.308) data 0.002 (0.002) loss 1.8212 (1.9141) acc 60.9375 (54.7395) (mean 53.5104 many 53.5104 med nan few nan) lr 3.4549e-03 elapsed 0:12:27 eta 0:07:36\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/782] time 0.302 (0.308) data 0.002 (0.002) loss 1.8212 (1.9141) acc 60.9375 (54.7395) (mean 53.5104 many 53.5104 med nan few nan) lr 3.4549e-03 elapsed 0:12:27 eta 0:07:36\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/782] time 0.311 (0.308) data 0.000 (0.002) loss 1.7455 (1.9114) acc 65.6250 (55.6343) (mean 54.9158 many 54.9158 med nan few nan) lr 3.4549e-03 elapsed 0:12:33 eta 0:07:30\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/782] time 0.311 (0.308) data 0.000 (0.002) loss 1.7455 (1.9114) acc 65.6250 (55.6343) (mean 54.9158 many 54.9158 med nan few nan) lr 3.4549e-03 elapsed 0:12:33 eta 0:07:30\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/782] time 0.314 (0.308) data 0.000 (0.002) loss 1.8351 (1.8794) acc 50.0000 (55.0779) (mean 55.3265 many 55.3265 med nan few nan) lr 3.4549e-03 elapsed 0:12:39 eta 0:07:24\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/782] time 0.314 (0.308) data 0.000 (0.002) loss 1.8351 (1.8794) acc 50.0000 (55.0779) (mean 55.3265 many 55.3265 med nan few nan) lr 3.4549e-03 elapsed 0:12:39 eta 0:07:24\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/782] time 0.308 (0.308) data 0.000 (0.002) loss 2.0139 (2.0361) acc 50.0000 (51.3618) (mean 52.9594 many 52.9594 med nan few nan) lr 3.4549e-03 elapsed 0:12:45 eta 0:07:18\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/782] time 0.308 (0.308) data 0.000 (0.002) loss 2.0139 (2.0361) acc 50.0000 (51.3618) (mean 52.9594 many 52.9594 med nan few nan) lr 3.4549e-03 elapsed 0:12:45 eta 0:07:18\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/782] time 0.309 (0.308) data 0.004 (0.002) loss 1.5119 (1.8985) acc 62.5000 (54.4977) (mean 53.9839 many 53.9839 med nan few nan) lr 3.4549e-03 elapsed 0:12:52 eta 0:07:12\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/782] time 0.309 (0.308) data 0.004 (0.002) loss 1.5119 (1.8985) acc 62.5000 (54.4977) (mean 53.9839 many 53.9839 med nan few nan) lr 3.4549e-03 elapsed 0:12:52 eta 0:07:12\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/782] time 0.313 (0.308) data 0.000 (0.001) loss 1.6693 (1.8934) acc 65.6250 (55.3915) (mean 54.9296 many 54.9296 med nan few nan) lr 3.4549e-03 elapsed 0:12:58 eta 0:07:06\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/782] time 0.313 (0.308) data 0.000 (0.001) loss 1.6693 (1.8934) acc 65.6250 (55.3915) (mean 54.9296 many 54.9296 med nan few nan) lr 3.4549e-03 elapsed 0:12:58 eta 0:07:06\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.8828 (1.9161) acc 60.9375 (54.4764) (mean 54.0988 many 54.0988 med nan few nan) lr 3.4549e-03 elapsed 0:13:04 eta 0:07:00\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.8828 (1.9161) acc 60.9375 (54.4764) (mean 54.0988 many 54.0988 med nan few nan) lr 3.4549e-03 elapsed 0:13:04 eta 0:07:00\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/782] time 0.308 (0.308) data 0.000 (0.001) loss 2.2180 (1.9715) acc 42.1875 (51.6775) (mean 52.6473 many 52.6473 med nan few nan) lr 3.4549e-03 elapsed 0:13:10 eta 0:06:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/782] time 0.308 (0.308) data 0.000 (0.001) loss 2.2180 (1.9715) acc 42.1875 (51.6775) (mean 52.6473 many 52.6473 med nan few nan) lr 3.4549e-03 elapsed 0:13:10 eta 0:06:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/782] time 0.317 (0.308) data 0.000 (0.001) loss 2.0910 (2.0009) acc 50.0000 (51.2661) (mean 52.0099 many 52.0099 med nan few nan) lr 3.4549e-03 elapsed 0:13:16 eta 0:06:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/782] time 0.317 (0.308) data 0.000 (0.001) loss 2.0910 (2.0009) acc 50.0000 (51.2661) (mean 52.0099 many 52.0099 med nan few nan) lr 3.4549e-03 elapsed 0:13:16 eta 0:06:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/782] time 0.318 (0.308) data 0.000 (0.001) loss 1.9982 (1.8876) acc 51.5625 (56.4013) (mean 55.5657 many 55.5657 med nan few nan) lr 3.4549e-03 elapsed 0:13:23 eta 0:06:41\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/782] time 0.318 (0.308) data 0.000 (0.001) loss 1.9982 (1.8876) acc 51.5625 (56.4013) (mean 55.5657 many 55.5657 med nan few nan) lr 3.4549e-03 elapsed 0:13:23 eta 0:06:41\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/782] time 0.316 (0.308) data 0.000 (0.001) loss 1.7361 (1.9043) acc 57.8125 (53.6673) (mean 53.8919 many 53.8919 med nan few nan) lr 3.4549e-03 elapsed 0:13:29 eta 0:06:35\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/782] time 0.316 (0.308) data 0.000 (0.001) loss 1.7361 (1.9043) acc 57.8125 (53.6673) (mean 53.8919 many 53.8919 med nan few nan) lr 3.4549e-03 elapsed 0:13:29 eta 0:06:35\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/782] time 0.317 (0.308) data 0.000 (0.001) loss 1.7227 (1.9219) acc 60.9375 (54.8448) (mean 54.9475 many 54.9475 med nan few nan) lr 3.4549e-03 elapsed 0:13:35 eta 0:06:29\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/782] time 0.317 (0.308) data 0.000 (0.001) loss 1.7227 (1.9219) acc 60.9375 (54.8448) (mean 54.9475 many 54.9475 med nan few nan) lr 3.4549e-03 elapsed 0:13:35 eta 0:06:29\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.5563 (1.8917) acc 64.0625 (55.4164) (mean 54.8689 many 54.8689 med nan few nan) lr 3.4549e-03 elapsed 0:13:41 eta 0:06:23\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.5563 (1.8917) acc 64.0625 (55.4164) (mean 54.8689 many 54.8689 med nan few nan) lr 3.4549e-03 elapsed 0:13:41 eta 0:06:23\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.6973 (1.9865) acc 62.5000 (51.1443) (mean 51.6182 many 51.6182 med nan few nan) lr 3.4549e-03 elapsed 0:13:48 eta 0:06:17\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.6973 (1.9865) acc 62.5000 (51.1443) (mean 51.6182 many 51.6182 med nan few nan) lr 3.4549e-03 elapsed 0:13:48 eta 0:06:17\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.0889 (1.8783) acc 48.4375 (54.5601) (mean 54.7398 many 54.7398 med nan few nan) lr 3.4549e-03 elapsed 0:13:54 eta 0:06:11\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.0889 (1.8783) acc 48.4375 (54.5601) (mean 54.7398 many 54.7398 med nan few nan) lr 3.4549e-03 elapsed 0:13:54 eta 0:06:11\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.1327 (1.9163) acc 46.8750 (52.9289) (mean 54.1470 many 54.1470 med nan few nan) lr 3.4549e-03 elapsed 0:14:00 eta 0:06:04\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.1327 (1.9163) acc 46.8750 (52.9289) (mean 54.1470 many 54.1470 med nan few nan) lr 3.4549e-03 elapsed 0:14:00 eta 0:06:04\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/782] time 0.312 (0.308) data 0.000 (0.001) loss 1.7390 (1.9496) acc 57.8125 (51.1012) (mean 51.7822 many 51.7822 med nan few nan) lr 3.4549e-03 elapsed 0:14:06 eta 0:05:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/782] time 0.312 (0.308) data 0.000 (0.001) loss 1.7390 (1.9496) acc 57.8125 (51.1012) (mean 51.7822 many 51.7822 med nan few nan) lr 3.4549e-03 elapsed 0:14:06 eta 0:05:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.8345 (1.9493) acc 54.6875 (53.5989) (mean 54.3576 many 54.3576 med nan few nan) lr 3.4549e-03 elapsed 0:14:12 eta 0:05:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.8345 (1.9493) acc 54.6875 (53.5989) (mean 54.3576 many 54.3576 med nan few nan) lr 3.4549e-03 elapsed 0:14:12 eta 0:05:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/782] time 0.316 (0.308) data 0.000 (0.001) loss 1.9836 (1.9103) acc 51.5625 (53.6955) (mean 53.6088 many 53.6088 med nan few nan) lr 3.4549e-03 elapsed 0:14:19 eta 0:05:46\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/782] time 0.316 (0.308) data 0.000 (0.001) loss 1.9836 (1.9103) acc 51.5625 (53.6955) (mean 53.6088 many 53.6088 med nan few nan) lr 3.4549e-03 elapsed 0:14:19 eta 0:05:46\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/782] time 0.304 (0.308) data 0.000 (0.001) loss 1.9192 (1.9101) acc 53.1250 (56.1618) (mean 55.4230 many 55.4230 med nan few nan) lr 3.4549e-03 elapsed 0:14:25 eta 0:05:40\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/782] time 0.304 (0.308) data 0.000 (0.001) loss 1.9192 (1.9101) acc 53.1250 (56.1618) (mean 55.4230 many 55.4230 med nan few nan) lr 3.4549e-03 elapsed 0:14:25 eta 0:05:40\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.8454 (1.9192) acc 57.8125 (54.2290) (mean 54.0798 many 54.0798 med nan few nan) lr 3.4549e-03 elapsed 0:14:31 eta 0:05:34\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.8454 (1.9192) acc 57.8125 (54.2290) (mean 54.0798 many 54.0798 med nan few nan) lr 3.4549e-03 elapsed 0:14:31 eta 0:05:34\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.8416 (1.9100) acc 59.3750 (53.8414) (mean 53.3975 many 53.3975 med nan few nan) lr 3.4549e-03 elapsed 0:14:37 eta 0:05:28\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.8416 (1.9100) acc 59.3750 (53.8414) (mean 53.3975 many 53.3975 med nan few nan) lr 3.4549e-03 elapsed 0:14:37 eta 0:05:28\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.7825 (1.9039) acc 57.8125 (52.7888) (mean 52.7759 many 52.7759 med nan few nan) lr 3.4549e-03 elapsed 0:14:43 eta 0:05:21\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/782] time 0.308 (0.308) data 0.000 (0.001) loss 1.7825 (1.9039) acc 57.8125 (52.7888) (mean 52.7759 many 52.7759 med nan few nan) lr 3.4549e-03 elapsed 0:14:43 eta 0:05:21\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.0291 (1.8945) acc 59.3750 (55.4614) (mean 55.6629 many 55.6629 med nan few nan) lr 3.4549e-03 elapsed 0:14:50 eta 0:05:15\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.0291 (1.8945) acc 59.3750 (55.4614) (mean 55.6629 many 55.6629 med nan few nan) lr 3.4549e-03 elapsed 0:14:50 eta 0:05:15\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/782] time 0.313 (0.308) data 0.000 (0.001) loss 2.1622 (1.9126) acc 48.4375 (53.9004) (mean 54.2789 many 54.2789 med nan few nan) lr 3.4549e-03 elapsed 0:14:56 eta 0:05:09\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/782] time 0.313 (0.308) data 0.000 (0.001) loss 2.1622 (1.9126) acc 48.4375 (53.9004) (mean 54.2789 many 54.2789 med nan few nan) lr 3.4549e-03 elapsed 0:14:56 eta 0:05:09\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/782] time 0.310 (0.308) data 0.000 (0.001) loss 1.9258 (1.9062) acc 56.2500 (54.7107) (mean 54.1360 many 54.1360 med nan few nan) lr 3.4549e-03 elapsed 0:15:02 eta 0:05:03\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/782] time 0.310 (0.308) data 0.000 (0.001) loss 1.9258 (1.9062) acc 56.2500 (54.7107) (mean 54.1360 many 54.1360 med nan few nan) lr 3.4549e-03 elapsed 0:15:02 eta 0:05:03\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/782] time 0.305 (0.308) data 0.000 (0.001) loss 2.2309 (1.8942) acc 53.1250 (58.0149) (mean 57.4882 many 57.4882 med nan few nan) lr 3.4549e-03 elapsed 0:15:08 eta 0:04:57\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/782] time 0.305 (0.308) data 0.000 (0.001) loss 2.2309 (1.8942) acc 53.1250 (58.0149) (mean 57.4882 many 57.4882 med nan few nan) lr 3.4549e-03 elapsed 0:15:08 eta 0:04:57\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/782] time 0.312 (0.308) data 0.000 (0.001) loss 2.1025 (1.9983) acc 54.6875 (54.3256) (mean 55.4599 many 55.4599 med nan few nan) lr 3.4549e-03 elapsed 0:15:14 eta 0:04:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/782] time 0.312 (0.308) data 0.000 (0.001) loss 2.1025 (1.9983) acc 54.6875 (54.3256) (mean 55.4599 many 55.4599 med nan few nan) lr 3.4549e-03 elapsed 0:15:14 eta 0:04:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [640/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.0396 (1.9133) acc 43.7500 (53.1284) (mean 54.0908 many 54.0908 med nan few nan) lr 3.4549e-03 elapsed 0:15:21 eta 0:04:44\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [640/782] time 0.309 (0.308) data 0.000 (0.001) loss 2.0396 (1.9133) acc 43.7500 (53.1284) (mean 54.0908 many 54.0908 med nan few nan) lr 3.4549e-03 elapsed 0:15:21 eta 0:04:44\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [660/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.9788 (1.9012) acc 59.3750 (54.7422) (mean 55.1547 many 55.1547 med nan few nan) lr 3.4549e-03 elapsed 0:15:27 eta 0:04:38\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [660/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.9788 (1.9012) acc 59.3750 (54.7422) (mean 55.1547 many 55.1547 med nan few nan) lr 3.4549e-03 elapsed 0:15:27 eta 0:04:38\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [680/782] time 0.305 (0.308) data 0.000 (0.001) loss 1.8291 (1.9362) acc 56.2500 (53.5458) (mean 53.9234 many 53.9234 med nan few nan) lr 3.4549e-03 elapsed 0:15:33 eta 0:04:32\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [680/782] time 0.305 (0.308) data 0.000 (0.001) loss 1.8291 (1.9362) acc 56.2500 (53.5458) (mean 53.9234 many 53.9234 med nan few nan) lr 3.4549e-03 elapsed 0:15:33 eta 0:04:32\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [700/782] time 0.300 (0.308) data 0.000 (0.001) loss 1.7133 (1.8753) acc 54.6875 (55.4274) (mean 55.0233 many 55.0233 med nan few nan) lr 3.4549e-03 elapsed 0:15:39 eta 0:04:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [700/782] time 0.300 (0.308) data 0.000 (0.001) loss 1.7133 (1.8753) acc 54.6875 (55.4274) (mean 55.0233 many 55.0233 med nan few nan) lr 3.4549e-03 elapsed 0:15:39 eta 0:04:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [720/782] time 0.315 (0.308) data 0.000 (0.001) loss 1.9073 (1.8875) acc 45.3125 (52.9761) (mean 53.3780 many 53.3780 med nan few nan) lr 3.4549e-03 elapsed 0:15:45 eta 0:04:20\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [720/782] time 0.315 (0.308) data 0.000 (0.001) loss 1.9073 (1.8875) acc 45.3125 (52.9761) (mean 53.3780 many 53.3780 med nan few nan) lr 3.4549e-03 elapsed 0:15:45 eta 0:04:20\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [740/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.7282 (1.9617) acc 62.5000 (52.1061) (mean 52.2375 many 52.2375 med nan few nan) lr 3.4549e-03 elapsed 0:15:52 eta 0:04:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [740/782] time 0.309 (0.308) data 0.000 (0.001) loss 1.7282 (1.9617) acc 62.5000 (52.1061) (mean 52.2375 many 52.2375 med nan few nan) lr 3.4549e-03 elapsed 0:15:52 eta 0:04:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [760/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.7709 (1.8979) acc 53.1250 (53.5870) (mean 53.4638 many 53.4638 med nan few nan) lr 3.4549e-03 elapsed 0:15:58 eta 0:04:07\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [760/782] time 0.307 (0.308) data 0.000 (0.001) loss 1.7709 (1.8979) acc 53.1250 (53.5870) (mean 53.4638 many 53.4638 med nan few nan) lr 3.4549e-03 elapsed 0:15:58 eta 0:04:07\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [780/782] time 0.312 (0.308) data 0.000 (0.001) loss 1.7446 (1.8706) acc 56.2500 (54.7247) (mean 54.5684 many 54.5684 med nan few nan) lr 3.4549e-03 elapsed 0:16:04 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [780/782] time 0.312 (0.308) data 0.000 (0.001) loss 1.7446 (1.8706) acc 56.2500 (54.7247) (mean 54.5684 many 54.5684 med nan few nan) lr 3.4549e-03 elapsed 0:16:04 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [782/782] time 0.306 (0.308) data 0.000 (0.001) loss 2.4377 (1.9440) acc 25.0000 (51.6082) (mean 54.2073 many 54.2073 med nan few nan) lr 3.4549e-03 elapsed 0:16:04 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [782/782] time 0.306 (0.308) data 0.000 (0.001) loss 2.4377 (1.9440) acc 25.0000 (51.6082) (mean 54.2073 many 54.2073 med nan few nan) lr 3.4549e-03 elapsed 0:16:04 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/782] time 0.312 (0.309) data 0.002 (0.001) loss 1.8231 (1.8957) acc 56.2500 (54.7742) (mean 54.9335 many 54.9335 med nan few nan) lr 9.5492e-04 elapsed 0:16:11 eta 0:03:55\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/782] time 0.312 (0.309) data 0.002 (0.001) loss 1.8231 (1.8957) acc 56.2500 (54.7742) (mean 54.9335 many 54.9335 med nan few nan) lr 9.5492e-04 elapsed 0:16:11 eta 0:03:55\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.8967 (1.9457) acc 46.8750 (54.8229) (mean 55.4681 many 55.4681 med nan few nan) lr 9.5492e-04 elapsed 0:16:17 eta 0:03:48\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.8967 (1.9457) acc 46.8750 (54.8229) (mean 55.4681 many 55.4681 med nan few nan) lr 9.5492e-04 elapsed 0:16:17 eta 0:03:48\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.7963 (1.8398) acc 50.0000 (56.2885) (mean 56.8614 many 56.8614 med nan few nan) lr 9.5492e-04 elapsed 0:16:23 eta 0:03:42\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.7963 (1.8398) acc 50.0000 (56.2885) (mean 56.8614 many 56.8614 med nan few nan) lr 9.5492e-04 elapsed 0:16:23 eta 0:03:42\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/782] time 0.305 (0.309) data 0.003 (0.001) loss 1.8623 (1.9616) acc 54.6875 (52.8668) (mean 54.0173 many 54.0173 med nan few nan) lr 9.5492e-04 elapsed 0:16:30 eta 0:03:36\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/782] time 0.305 (0.309) data 0.003 (0.001) loss 1.8623 (1.9616) acc 54.6875 (52.8668) (mean 54.0173 many 54.0173 med nan few nan) lr 9.5492e-04 elapsed 0:16:30 eta 0:03:36\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.9552 (1.8885) acc 53.1250 (54.6710) (mean 54.9369 many 54.9369 med nan few nan) lr 9.5492e-04 elapsed 0:16:36 eta 0:03:30\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.9552 (1.8885) acc 53.1250 (54.6710) (mean 54.9369 many 54.9369 med nan few nan) lr 9.5492e-04 elapsed 0:16:36 eta 0:03:30\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/782] time 0.302 (0.309) data 0.000 (0.001) loss 1.8066 (1.8280) acc 56.2500 (56.2669) (mean 56.3088 many 56.3088 med nan few nan) lr 9.5492e-04 elapsed 0:16:42 eta 0:03:24\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/782] time 0.302 (0.309) data 0.000 (0.001) loss 1.8066 (1.8280) acc 56.2500 (56.2669) (mean 56.3088 many 56.3088 med nan few nan) lr 9.5492e-04 elapsed 0:16:42 eta 0:03:24\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/782] time 0.306 (0.309) data 0.000 (0.001) loss 2.1563 (1.8875) acc 42.1875 (54.4284) (mean 55.5253 many 55.5253 med nan few nan) lr 9.5492e-04 elapsed 0:16:48 eta 0:03:18\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/782] time 0.306 (0.309) data 0.000 (0.001) loss 2.1563 (1.8875) acc 42.1875 (54.4284) (mean 55.5253 many 55.5253 med nan few nan) lr 9.5492e-04 elapsed 0:16:48 eta 0:03:18\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/782] time 0.307 (0.309) data 0.002 (0.001) loss 1.5328 (1.8736) acc 65.6250 (55.7274) (mean 55.7551 many 55.7551 med nan few nan) lr 9.5492e-04 elapsed 0:16:54 eta 0:03:11\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/782] time 0.307 (0.309) data 0.002 (0.001) loss 1.5328 (1.8736) acc 65.6250 (55.7274) (mean 55.7551 many 55.7551 med nan few nan) lr 9.5492e-04 elapsed 0:16:54 eta 0:03:11\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/782] time 0.304 (0.309) data 0.000 (0.001) loss 1.4714 (1.8753) acc 67.1875 (55.8921) (mean 55.8841 many 55.8841 med nan few nan) lr 9.5492e-04 elapsed 0:17:00 eta 0:03:05\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/782] time 0.304 (0.309) data 0.000 (0.001) loss 1.4714 (1.8753) acc 67.1875 (55.8921) (mean 55.8841 many 55.8841 med nan few nan) lr 9.5492e-04 elapsed 0:17:00 eta 0:03:05\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/782] time 0.297 (0.309) data 0.000 (0.001) loss 1.7045 (1.8821) acc 51.5625 (55.4580) (mean 55.5202 many 55.5202 med nan few nan) lr 9.5492e-04 elapsed 0:17:07 eta 0:02:59\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/782] time 0.297 (0.309) data 0.000 (0.001) loss 1.7045 (1.8821) acc 51.5625 (55.4580) (mean 55.5202 many 55.5202 med nan few nan) lr 9.5492e-04 elapsed 0:17:07 eta 0:02:59\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/782] time 0.313 (0.309) data 0.000 (0.001) loss 1.3635 (1.8444) acc 70.3125 (55.2533) (mean 53.8285 many 53.8285 med nan few nan) lr 9.5492e-04 elapsed 0:17:13 eta 0:02:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/782] time 0.313 (0.309) data 0.000 (0.001) loss 1.3635 (1.8444) acc 70.3125 (55.2533) (mean 53.8285 many 53.8285 med nan few nan) lr 9.5492e-04 elapsed 0:17:13 eta 0:02:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [240/782] time 0.313 (0.309) data 0.001 (0.001) loss 1.8014 (1.9437) acc 56.2500 (54.2546) (mean 54.6001 many 54.6001 med nan few nan) lr 9.5492e-04 elapsed 0:17:19 eta 0:02:47\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [240/782] time 0.313 (0.309) data 0.001 (0.001) loss 1.8014 (1.9437) acc 56.2500 (54.2546) (mean 54.6001 many 54.6001 med nan few nan) lr 9.5492e-04 elapsed 0:17:19 eta 0:02:47\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [260/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.2848 (1.8960) acc 46.8750 (55.5124) (mean 56.3681 many 56.3681 med nan few nan) lr 9.5492e-04 elapsed 0:17:25 eta 0:02:41\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [260/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.2848 (1.8960) acc 46.8750 (55.5124) (mean 56.3681 many 56.3681 med nan few nan) lr 9.5492e-04 elapsed 0:17:25 eta 0:02:41\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [280/782] time 0.304 (0.309) data 0.000 (0.001) loss 2.0114 (1.9042) acc 50.0000 (54.5034) (mean 55.4677 many 55.4677 med nan few nan) lr 9.5492e-04 elapsed 0:17:31 eta 0:02:34\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [280/782] time 0.304 (0.309) data 0.000 (0.001) loss 2.0114 (1.9042) acc 50.0000 (54.5034) (mean 55.4677 many 55.4677 med nan few nan) lr 9.5492e-04 elapsed 0:17:31 eta 0:02:34\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [300/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.9281 (1.9414) acc 54.6875 (54.9514) (mean 54.8865 many 54.8865 med nan few nan) lr 9.5492e-04 elapsed 0:17:37 eta 0:02:28\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [300/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.9281 (1.9414) acc 54.6875 (54.9514) (mean 54.8865 many 54.8865 med nan few nan) lr 9.5492e-04 elapsed 0:17:37 eta 0:02:28\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [320/782] time 0.310 (0.309) data 0.002 (0.001) loss 2.1980 (1.9035) acc 43.7500 (54.5046) (mean 55.2316 many 55.2316 med nan few nan) lr 9.5492e-04 elapsed 0:17:44 eta 0:02:22\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [320/782] time 0.310 (0.309) data 0.002 (0.001) loss 2.1980 (1.9035) acc 43.7500 (54.5046) (mean 55.2316 many 55.2316 med nan few nan) lr 9.5492e-04 elapsed 0:17:44 eta 0:02:22\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [340/782] time 0.315 (0.309) data 0.000 (0.001) loss 1.9136 (1.9087) acc 53.1250 (54.4964) (mean 54.5934 many 54.5934 med nan few nan) lr 9.5492e-04 elapsed 0:17:50 eta 0:02:16\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [340/782] time 0.315 (0.309) data 0.000 (0.001) loss 1.9136 (1.9087) acc 53.1250 (54.4964) (mean 54.5934 many 54.5934 med nan few nan) lr 9.5492e-04 elapsed 0:17:50 eta 0:02:16\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [360/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9144 (1.9101) acc 62.5000 (54.7525) (mean 55.2084 many 55.2084 med nan few nan) lr 9.5492e-04 elapsed 0:17:56 eta 0:02:10\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [360/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9144 (1.9101) acc 62.5000 (54.7525) (mean 55.2084 many 55.2084 med nan few nan) lr 9.5492e-04 elapsed 0:17:56 eta 0:02:10\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [380/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9516 (1.8260) acc 48.4375 (55.9052) (mean 56.5147 many 56.5147 med nan few nan) lr 9.5492e-04 elapsed 0:18:02 eta 0:02:04\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [380/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9516 (1.8260) acc 48.4375 (55.9052) (mean 56.5147 many 56.5147 med nan few nan) lr 9.5492e-04 elapsed 0:18:02 eta 0:02:04\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [400/782] time 0.316 (0.309) data 0.000 (0.001) loss 1.8486 (1.8649) acc 57.8125 (56.0572) (mean 56.1341 many 56.1341 med nan few nan) lr 9.5492e-04 elapsed 0:18:08 eta 0:01:57\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [400/782] time 0.316 (0.309) data 0.000 (0.001) loss 1.8486 (1.8649) acc 57.8125 (56.0572) (mean 56.1341 many 56.1341 med nan few nan) lr 9.5492e-04 elapsed 0:18:08 eta 0:01:57\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [420/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.1090 (1.9135) acc 50.0000 (54.1355) (mean 55.1168 many 55.1168 med nan few nan) lr 9.5492e-04 elapsed 0:18:15 eta 0:01:51\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [420/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.1090 (1.9135) acc 50.0000 (54.1355) (mean 55.1168 many 55.1168 med nan few nan) lr 9.5492e-04 elapsed 0:18:15 eta 0:01:51\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [440/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9344 (1.9180) acc 57.8125 (53.3755) (mean 53.9423 many 53.9423 med nan few nan) lr 9.5492e-04 elapsed 0:18:21 eta 0:01:45\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [440/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9344 (1.9180) acc 57.8125 (53.3755) (mean 53.9423 many 53.9423 med nan few nan) lr 9.5492e-04 elapsed 0:18:21 eta 0:01:45\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [460/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.5331 (1.7869) acc 65.6250 (59.3876) (mean 58.7545 many 58.7545 med nan few nan) lr 9.5492e-04 elapsed 0:18:27 eta 0:01:39\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [460/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.5331 (1.7869) acc 65.6250 (59.3876) (mean 58.7545 many 58.7545 med nan few nan) lr 9.5492e-04 elapsed 0:18:27 eta 0:01:39\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [480/782] time 0.318 (0.309) data 0.000 (0.001) loss 1.9530 (1.8281) acc 54.6875 (57.3824) (mean 57.2832 many 57.2832 med nan few nan) lr 9.5492e-04 elapsed 0:18:33 eta 0:01:33\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [480/782] time 0.318 (0.309) data 0.000 (0.001) loss 1.9530 (1.8281) acc 54.6875 (57.3824) (mean 57.2832 many 57.2832 med nan few nan) lr 9.5492e-04 elapsed 0:18:33 eta 0:01:33\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [500/782] time 0.313 (0.309) data 0.000 (0.001) loss 2.2267 (1.9109) acc 51.5625 (54.9567) (mean 55.6072 many 55.6072 med nan few nan) lr 9.5492e-04 elapsed 0:18:39 eta 0:01:27\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [500/782] time 0.313 (0.309) data 0.000 (0.001) loss 2.2267 (1.9109) acc 51.5625 (54.9567) (mean 55.6072 many 55.6072 med nan few nan) lr 9.5492e-04 elapsed 0:18:39 eta 0:01:27\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [520/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.6755 (1.8876) acc 64.0625 (55.2607) (mean 55.9379 many 55.9379 med nan few nan) lr 9.5492e-04 elapsed 0:18:46 eta 0:01:20\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [520/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.6755 (1.8876) acc 64.0625 (55.2607) (mean 55.9379 many 55.9379 med nan few nan) lr 9.5492e-04 elapsed 0:18:46 eta 0:01:20\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [540/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9194 (1.9144) acc 48.4375 (52.8525) (mean 53.9016 many 53.9016 med nan few nan) lr 9.5492e-04 elapsed 0:18:52 eta 0:01:14\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [540/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9194 (1.9144) acc 48.4375 (52.8525) (mean 53.9016 many 53.9016 med nan few nan) lr 9.5492e-04 elapsed 0:18:52 eta 0:01:14\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [560/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.7532 (1.8589) acc 60.9375 (57.3316) (mean 56.0766 many 56.0766 med nan few nan) lr 9.5492e-04 elapsed 0:18:58 eta 0:01:08\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [560/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.7532 (1.8589) acc 60.9375 (57.3316) (mean 56.0766 many 56.0766 med nan few nan) lr 9.5492e-04 elapsed 0:18:58 eta 0:01:08\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [580/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.8478 (1.8573) acc 53.1250 (55.3149) (mean 55.2713 many 55.2713 med nan few nan) lr 9.5492e-04 elapsed 0:19:04 eta 0:01:02\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [580/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.8478 (1.8573) acc 53.1250 (55.3149) (mean 55.2713 many 55.2713 med nan few nan) lr 9.5492e-04 elapsed 0:19:04 eta 0:01:02\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [600/782] time 0.312 (0.309) data 0.000 (0.001) loss 2.1002 (1.9098) acc 46.8750 (53.9231) (mean 54.3375 many 54.3375 med nan few nan) lr 9.5492e-04 elapsed 0:19:10 eta 0:00:56\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [600/782] time 0.312 (0.309) data 0.000 (0.001) loss 2.1002 (1.9098) acc 46.8750 (53.9231) (mean 54.3375 many 54.3375 med nan few nan) lr 9.5492e-04 elapsed 0:19:10 eta 0:00:56\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [620/782] time 0.311 (0.309) data 0.000 (0.001) loss 2.0056 (1.9069) acc 57.8125 (54.5895) (mean 54.3851 many 54.3851 med nan few nan) lr 9.5492e-04 elapsed 0:19:17 eta 0:00:50\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [620/782] time 0.311 (0.309) data 0.000 (0.001) loss 2.0056 (1.9069) acc 57.8125 (54.5895) (mean 54.3851 many 54.3851 med nan few nan) lr 9.5492e-04 elapsed 0:19:17 eta 0:00:50\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [640/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9041 (1.8955) acc 54.6875 (54.3060) (mean 54.7655 many 54.7655 med nan few nan) lr 9.5492e-04 elapsed 0:19:23 eta 0:00:43\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [640/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9041 (1.8955) acc 54.6875 (54.3060) (mean 54.7655 many 54.7655 med nan few nan) lr 9.5492e-04 elapsed 0:19:23 eta 0:00:43\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [660/782] time 0.315 (0.309) data 0.000 (0.001) loss 1.8488 (1.8724) acc 56.2500 (56.3957) (mean 56.4304 many 56.4304 med nan few nan) lr 9.5492e-04 elapsed 0:19:29 eta 0:00:37\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [660/782] time 0.315 (0.309) data 0.000 (0.001) loss 1.8488 (1.8724) acc 56.2500 (56.3957) (mean 56.4304 many 56.4304 med nan few nan) lr 9.5492e-04 elapsed 0:19:29 eta 0:00:37\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [680/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.8058 (1.9228) acc 59.3750 (55.1589) (mean 55.4136 many 55.4136 med nan few nan) lr 9.5492e-04 elapsed 0:19:35 eta 0:00:31\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [680/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.8058 (1.9228) acc 59.3750 (55.1589) (mean 55.4136 many 55.4136 med nan few nan) lr 9.5492e-04 elapsed 0:19:35 eta 0:00:31\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [700/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.5693 (1.8823) acc 62.5000 (55.0591) (mean 55.3905 many 55.3905 med nan few nan) lr 9.5492e-04 elapsed 0:19:42 eta 0:00:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [700/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.5693 (1.8823) acc 62.5000 (55.0591) (mean 55.3905 many 55.3905 med nan few nan) lr 9.5492e-04 elapsed 0:19:42 eta 0:00:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [720/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.7383 (1.9048) acc 56.2500 (52.5480) (mean 53.6326 many 53.6326 med nan few nan) lr 9.5492e-04 elapsed 0:19:48 eta 0:00:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [720/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.7383 (1.9048) acc 56.2500 (52.5480) (mean 53.6326 many 53.6326 med nan few nan) lr 9.5492e-04 elapsed 0:19:48 eta 0:00:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [740/782] time 0.314 (0.309) data 0.000 (0.001) loss 2.2034 (1.9257) acc 46.8750 (53.9344) (mean 54.1328 many 54.1328 med nan few nan) lr 9.5492e-04 elapsed 0:19:54 eta 0:00:12\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [740/782] time 0.314 (0.309) data 0.000 (0.001) loss 2.2034 (1.9257) acc 46.8750 (53.9344) (mean 54.1328 many 54.1328 med nan few nan) lr 9.5492e-04 elapsed 0:19:54 eta 0:00:12\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [760/782] time 0.314 (0.309) data 0.000 (0.001) loss 1.5390 (1.8783) acc 65.6250 (56.6486) (mean 56.3396 many 56.3396 med nan few nan) lr 9.5492e-04 elapsed 0:20:00 eta 0:00:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [760/782] time 0.314 (0.309) data 0.000 (0.001) loss 1.5390 (1.8783) acc 65.6250 (56.6486) (mean 56.3396 many 56.3396 med nan few nan) lr 9.5492e-04 elapsed 0:20:00 eta 0:00:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [780/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.5480 (1.8004) acc 67.1875 (58.3521) (mean 57.4280 many 57.4280 med nan few nan) lr 9.5492e-04 elapsed 0:20:06 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [780/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.5480 (1.8004) acc 67.1875 (58.3521) (mean 57.4280 many 57.4280 med nan few nan) lr 9.5492e-04 elapsed 0:20:06 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [782/782] time 0.313 (0.309) data 0.000 (0.001) loss 1.1009 (1.7421) acc 75.0000 (59.2652) (mean 57.3203 many 57.3203 med nan few nan) lr 9.5492e-04 elapsed 0:20:07 eta 0:00:00\u001b[0m\n",
      "\u001b[37mFinished training\u001b[0m\n",
      "\u001b[37mNote: Printed training accuracy is approximate. Use test_train=True for precise evaluation.\u001b[0m\n",
      "\u001b[37mTotal training time: 0:20:07\u001b[0m\n",
      "\u001b[37mCheckpoint saved to /content/metalora/output/notebooks/cifar100_class_aware_svhn/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [782/782] time 0.313 (0.309) data 0.000 (0.001) loss 1.1009 (1.7421) acc 75.0000 (59.2652) (mean 57.3203 many 57.3203 med nan few nan) lr 9.5492e-04 elapsed 0:20:07 eta 0:00:00\u001b[0m\n",
      "\u001b[37mFinished training\u001b[0m\n",
      "\u001b[37mNote: Printed training accuracy is approximate. Use test_train=True for precise evaluation.\u001b[0m\n",
      "\u001b[37mTotal training time: 0:20:07\u001b[0m\n",
      "\u001b[37mCheckpoint saved to /content/metalora/output/notebooks/cifar100_class_aware_svhn/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      " 10%|9         | 15/157 [00:13<02:04]\u001b[0m\n",
      " 10%|9         | 15/157 [00:13<02:04]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:49]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:49]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:38<01:35]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:38<01:35]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:22]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:22]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:04<01:09]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:04<01:09]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:29<00:44]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:29<00:44]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:42<00:31]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:42<00:31]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:07<00:05]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:07<00:05]\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,529\n",
      "* accuracy: 75.3%\n",
      "* error: 24.7%\n",
      "* macro_f1: 75.0%\u001b[0m\n",
      "\u001b[37m* class acc: [94. 89. 88. 72. 34. 74. 73. 74. 85. 95. 60. 69. 87. 81. 86. 85. 80. 94.\n",
      " 69. 85. 88. 85. 85. 77. 85. 67. 64. 46. 85. 76. 72. 81. 57. 63. 76. 72.\n",
      " 77. 86. 67. 89. 68. 84. 58. 85. 63. 33. 90. 55. 96. 88. 56. 77. 83. 98.\n",
      " 84. 38. 88. 83. 93. 56. 89. 78. 77. 52. 63. 83. 67. 55. 98. 87. 84. 92.\n",
      " 55. 66. 32. 78. 89. 76. 66. 80. 62. 69. 94. 78. 72. 82. 81. 92. 65. 93.\n",
      " 80. 76. 75. 57. 84. 65. 44. 74. 84. 87.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 32.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 73.5%\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,529\n",
      "* accuracy: 75.3%\n",
      "* error: 24.7%\n",
      "* macro_f1: 75.0%\u001b[0m\n",
      "\u001b[37m* class acc: [94. 89. 88. 72. 34. 74. 73. 74. 85. 95. 60. 69. 87. 81. 86. 85. 80. 94.\n",
      " 69. 85. 88. 85. 85. 77. 85. 67. 64. 46. 85. 76. 72. 81. 57. 63. 76. 72.\n",
      " 77. 86. 67. 89. 68. 84. 58. 85. 63. 33. 90. 55. 96. 88. 56. 77. 83. 98.\n",
      " 84. 38. 88. 83. 93. 56. 89. 78. 77. 52. 63. 83. 67. 55. 98. 87. 84. 92.\n",
      " 55. 66. 32. 78. 89. 76. 66. 80. 62. 69. 94. 78. 72. 82. 81. 92. 65. 93.\n",
      " 80. 76. 75. 57. 84. 65. 44. 74. 84. 87.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 32.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 73.5%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.3%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.3%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.3%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.29, 'error_rate': 24.709999999999994, 'macro_f1': 74.96132337155717, 'worst_case_acc': 32.0, 'hmean_acc': np.float64(71.32235795533218), 'gmean_acc': np.float64(73.52552456954989), 'many_acc': np.float64(75.29), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.29)})\u001b[0m\n",
      "\u001b[37m* average: 75.3%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.29, 'error_rate': 24.709999999999994, 'macro_f1': 74.96132337155717, 'worst_case_acc': 32.0, 'hmean_acc': np.float64(71.32235795533218), 'gmean_acc': np.float64(73.52552456954989), 'many_acc': np.float64(75.29), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.29)})\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                           Training: Baseline Sampler                           \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                 Training model                                 \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                           Training: Baseline Sampler                           \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                 Training model                                 \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/782] time 0.307 (0.327) data 0.000 (0.020) loss 3.4546 (3.5395) acc 32.8125 (32.5357) (mean 35.5511 many 35.5511 med nan few nan) lr 1.0000e-02 elapsed 0:00:06 eta 0:21:12\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/782] time 0.307 (0.327) data 0.000 (0.020) loss 3.4546 (3.5395) acc 32.8125 (32.5357) (mean 35.5511 many 35.5511 med nan few nan) lr 1.0000e-02 elapsed 0:00:06 eta 0:21:12\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/782] time 0.313 (0.317) data 0.000 (0.010) loss 3.1482 (3.3284) acc 34.3750 (33.3064) (mean 33.3487 many 33.3487 med nan few nan) lr 1.0000e-02 elapsed 0:00:12 eta 0:20:25\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/782] time 0.313 (0.317) data 0.000 (0.010) loss 3.1482 (3.3284) acc 34.3750 (33.3064) (mean 33.3487 many 33.3487 med nan few nan) lr 1.0000e-02 elapsed 0:00:12 eta 0:20:25\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/782] time 0.304 (0.313) data 0.000 (0.007) loss 3.0841 (3.0762) acc 37.5000 (35.8918) (mean 34.0474 many 34.0474 med nan few nan) lr 1.0000e-02 elapsed 0:00:18 eta 0:20:05\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/782] time 0.304 (0.313) data 0.000 (0.007) loss 3.0841 (3.0762) acc 37.5000 (35.8918) (mean 34.0474 many 34.0474 med nan few nan) lr 1.0000e-02 elapsed 0:00:18 eta 0:20:05\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/782] time 0.309 (0.312) data 0.000 (0.005) loss 2.7826 (2.8962) acc 37.5000 (37.6155) (mean 35.9602 many 35.9602 med nan few nan) lr 1.0000e-02 elapsed 0:00:24 eta 0:19:53\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/782] time 0.309 (0.312) data 0.000 (0.005) loss 2.7826 (2.8962) acc 37.5000 (37.6155) (mean 35.9602 many 35.9602 med nan few nan) lr 1.0000e-02 elapsed 0:00:24 eta 0:19:53\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/782] time 0.313 (0.311) data 0.000 (0.004) loss 2.9327 (2.7866) acc 31.2500 (39.5270) (mean 39.4174 many 39.4174 med nan few nan) lr 1.0000e-02 elapsed 0:00:31 eta 0:19:43\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/782] time 0.313 (0.311) data 0.000 (0.004) loss 2.9327 (2.7866) acc 31.2500 (39.5270) (mean 39.4174 many 39.4174 med nan few nan) lr 1.0000e-02 elapsed 0:00:31 eta 0:19:43\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/782] time 0.308 (0.310) data 0.000 (0.004) loss 2.7199 (2.6739) acc 37.5000 (40.3565) (mean 41.0526 many 41.0526 med nan few nan) lr 1.0000e-02 elapsed 0:00:37 eta 0:19:35\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/782] time 0.308 (0.310) data 0.000 (0.004) loss 2.7199 (2.6739) acc 37.5000 (40.3565) (mean 41.0526 many 41.0526 med nan few nan) lr 1.0000e-02 elapsed 0:00:37 eta 0:19:35\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/782] time 0.306 (0.310) data 0.000 (0.003) loss 2.1243 (2.5120) acc 53.1250 (45.1168) (mean 43.9438 many 43.9438 med nan few nan) lr 1.0000e-02 elapsed 0:00:43 eta 0:19:28\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/782] time 0.306 (0.310) data 0.000 (0.003) loss 2.1243 (2.5120) acc 53.1250 (45.1168) (mean 43.9438 many 43.9438 med nan few nan) lr 1.0000e-02 elapsed 0:00:43 eta 0:19:28\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/782] time 0.312 (0.310) data 0.000 (0.003) loss 2.4301 (2.5278) acc 45.3125 (44.4655) (mean 44.0806 many 44.0806 med nan few nan) lr 1.0000e-02 elapsed 0:00:49 eta 0:19:21\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/782] time 0.312 (0.310) data 0.000 (0.003) loss 2.4301 (2.5278) acc 45.3125 (44.4655) (mean 44.0806 many 44.0806 med nan few nan) lr 1.0000e-02 elapsed 0:00:49 eta 0:19:21\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/782] time 0.304 (0.310) data 0.000 (0.003) loss 2.6881 (2.5529) acc 35.9375 (41.7798) (mean 43.5972 many 43.5972 med nan few nan) lr 1.0000e-02 elapsed 0:00:55 eta 0:19:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/782] time 0.304 (0.310) data 0.000 (0.003) loss 2.6881 (2.5529) acc 35.9375 (41.7798) (mean 43.5972 many 43.5972 med nan few nan) lr 1.0000e-02 elapsed 0:00:55 eta 0:19:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/782] time 0.317 (0.310) data 0.000 (0.002) loss 2.2967 (2.4774) acc 45.3125 (42.5360) (mean 43.5459 many 43.5459 med nan few nan) lr 1.0000e-02 elapsed 0:01:01 eta 0:19:09\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/782] time 0.317 (0.310) data 0.000 (0.002) loss 2.2967 (2.4774) acc 45.3125 (42.5360) (mean 43.5459 many 43.5459 med nan few nan) lr 1.0000e-02 elapsed 0:01:01 eta 0:19:09\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/782] time 0.308 (0.310) data 0.000 (0.002) loss 2.5886 (2.3761) acc 35.9375 (45.7881) (mean 45.3166 many 45.3166 med nan few nan) lr 1.0000e-02 elapsed 0:01:08 eta 0:19:03\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/782] time 0.308 (0.310) data 0.000 (0.002) loss 2.5886 (2.3761) acc 35.9375 (45.7881) (mean 45.3166 many 45.3166 med nan few nan) lr 1.0000e-02 elapsed 0:01:08 eta 0:19:03\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/782] time 0.306 (0.310) data 0.000 (0.002) loss 2.1488 (2.3390) acc 53.1250 (47.8614) (mean 47.6395 many 47.6395 med nan few nan) lr 1.0000e-02 elapsed 0:01:14 eta 0:18:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/782] time 0.306 (0.310) data 0.000 (0.002) loss 2.1488 (2.3390) acc 53.1250 (47.8614) (mean 47.6395 many 47.6395 med nan few nan) lr 1.0000e-02 elapsed 0:01:14 eta 0:18:57\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/782] time 0.308 (0.310) data 0.000 (0.002) loss 2.4641 (2.3280) acc 43.7500 (46.9196) (mean 47.0416 many 47.0416 med nan few nan) lr 1.0000e-02 elapsed 0:01:20 eta 0:18:50\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/782] time 0.308 (0.310) data 0.000 (0.002) loss 2.4641 (2.3280) acc 43.7500 (46.9196) (mean 47.0416 many 47.0416 med nan few nan) lr 1.0000e-02 elapsed 0:01:20 eta 0:18:50\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/782] time 0.315 (0.310) data 0.000 (0.002) loss 2.6050 (2.4795) acc 32.8125 (40.7996) (mean 43.3549 many 43.3549 med nan few nan) lr 1.0000e-02 elapsed 0:01:26 eta 0:18:44\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/782] time 0.315 (0.310) data 0.000 (0.002) loss 2.6050 (2.4795) acc 32.8125 (40.7996) (mean 43.3549 many 43.3549 med nan few nan) lr 1.0000e-02 elapsed 0:01:26 eta 0:18:44\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/782] time 0.310 (0.310) data 0.000 (0.002) loss 2.0352 (2.3000) acc 53.1250 (45.3414) (mean 44.4811 many 44.4811 med nan few nan) lr 1.0000e-02 elapsed 0:01:32 eta 0:18:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/782] time 0.310 (0.310) data 0.000 (0.002) loss 2.0352 (2.3000) acc 53.1250 (45.3414) (mean 44.4811 many 44.4811 med nan few nan) lr 1.0000e-02 elapsed 0:01:32 eta 0:18:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/782] time 0.306 (0.310) data 0.000 (0.002) loss 2.0400 (2.2628) acc 53.1250 (47.1637) (mean 46.9473 many 46.9473 med nan few nan) lr 1.0000e-02 elapsed 0:01:39 eta 0:18:32\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/782] time 0.306 (0.310) data 0.000 (0.002) loss 2.0400 (2.2628) acc 53.1250 (47.1637) (mean 46.9473 many 46.9473 med nan few nan) lr 1.0000e-02 elapsed 0:01:39 eta 0:18:32\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/782] time 0.308 (0.310) data 0.000 (0.002) loss 2.2336 (2.2235) acc 46.8750 (47.7124) (mean 47.6924 many 47.6924 med nan few nan) lr 1.0000e-02 elapsed 0:01:45 eta 0:18:26\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/782] time 0.308 (0.310) data 0.000 (0.002) loss 2.2336 (2.2235) acc 46.8750 (47.7124) (mean 47.6924 many 47.6924 med nan few nan) lr 1.0000e-02 elapsed 0:01:45 eta 0:18:26\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/782] time 0.310 (0.310) data 0.000 (0.002) loss 2.2467 (2.2807) acc 46.8750 (47.4257) (mean 46.9690 many 46.9690 med nan few nan) lr 1.0000e-02 elapsed 0:01:51 eta 0:18:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/782] time 0.310 (0.310) data 0.000 (0.002) loss 2.2467 (2.2807) acc 46.8750 (47.4257) (mean 46.9690 many 46.9690 med nan few nan) lr 1.0000e-02 elapsed 0:01:51 eta 0:18:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/782] time 0.312 (0.310) data 0.000 (0.001) loss 1.9428 (2.2405) acc 54.6875 (46.2509) (mean 45.8993 many 45.8993 med nan few nan) lr 1.0000e-02 elapsed 0:01:57 eta 0:18:13\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/782] time 0.312 (0.310) data 0.000 (0.001) loss 1.9428 (2.2405) acc 54.6875 (46.2509) (mean 45.8993 many 45.8993 med nan few nan) lr 1.0000e-02 elapsed 0:01:57 eta 0:18:13\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/782] time 0.307 (0.310) data 0.001 (0.001) loss 2.0581 (2.2665) acc 46.8750 (45.0221) (mean 45.3902 many 45.3902 med nan few nan) lr 1.0000e-02 elapsed 0:02:03 eta 0:18:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/782] time 0.307 (0.310) data 0.001 (0.001) loss 2.0581 (2.2665) acc 46.8750 (45.0221) (mean 45.3902 many 45.3902 med nan few nan) lr 1.0000e-02 elapsed 0:02:03 eta 0:18:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0096 (2.1971) acc 51.5625 (47.4175) (mean 47.1003 many 47.1003 med nan few nan) lr 1.0000e-02 elapsed 0:02:10 eta 0:18:01\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0096 (2.1971) acc 51.5625 (47.4175) (mean 47.1003 many 47.1003 med nan few nan) lr 1.0000e-02 elapsed 0:02:10 eta 0:18:01\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/782] time 0.303 (0.310) data 0.000 (0.001) loss 1.9733 (2.1132) acc 51.5625 (49.8694) (mean 48.7681 many 48.7681 med nan few nan) lr 1.0000e-02 elapsed 0:02:16 eta 0:17:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/782] time 0.303 (0.310) data 0.000 (0.001) loss 1.9733 (2.1132) acc 51.5625 (49.8694) (mean 48.7681 many 48.7681 med nan few nan) lr 1.0000e-02 elapsed 0:02:16 eta 0:17:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/782] time 0.306 (0.310) data 0.000 (0.001) loss 2.2340 (2.2228) acc 43.7500 (47.0668) (mean 48.2333 many 48.2333 med nan few nan) lr 1.0000e-02 elapsed 0:02:22 eta 0:17:48\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/782] time 0.306 (0.310) data 0.000 (0.001) loss 2.2340 (2.2228) acc 43.7500 (47.0668) (mean 48.2333 many 48.2333 med nan few nan) lr 1.0000e-02 elapsed 0:02:22 eta 0:17:48\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1321 (2.2121) acc 50.0000 (46.8752) (mean 47.6788 many 47.6788 med nan few nan) lr 1.0000e-02 elapsed 0:02:28 eta 0:17:42\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1321 (2.2121) acc 50.0000 (46.8752) (mean 47.6788 many 47.6788 med nan few nan) lr 1.0000e-02 elapsed 0:02:28 eta 0:17:42\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/782] time 0.315 (0.310) data 0.000 (0.001) loss 2.4077 (2.2160) acc 42.1875 (47.0300) (mean 47.5679 many 47.5679 med nan few nan) lr 1.0000e-02 elapsed 0:02:34 eta 0:17:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/782] time 0.315 (0.310) data 0.000 (0.001) loss 2.4077 (2.2160) acc 42.1875 (47.0300) (mean 47.5679 many 47.5679 med nan few nan) lr 1.0000e-02 elapsed 0:02:34 eta 0:17:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/782] time 0.313 (0.310) data 0.000 (0.001) loss 1.8924 (2.2442) acc 50.0000 (45.5976) (mean 46.7281 many 46.7281 med nan few nan) lr 1.0000e-02 elapsed 0:02:41 eta 0:17:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/782] time 0.313 (0.310) data 0.000 (0.001) loss 1.8924 (2.2442) acc 50.0000 (45.5976) (mean 46.7281 many 46.7281 med nan few nan) lr 1.0000e-02 elapsed 0:02:41 eta 0:17:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.1358 (2.1423) acc 48.4375 (50.0608) (mean 49.3347 many 49.3347 med nan few nan) lr 1.0000e-02 elapsed 0:02:47 eta 0:17:23\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.1358 (2.1423) acc 48.4375 (50.0608) (mean 49.3347 many 49.3347 med nan few nan) lr 1.0000e-02 elapsed 0:02:47 eta 0:17:23\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8761 (2.0743) acc 53.1250 (51.5714) (mean 50.9263 many 50.9263 med nan few nan) lr 1.0000e-02 elapsed 0:02:53 eta 0:17:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8761 (2.0743) acc 53.1250 (51.5714) (mean 50.9263 many 50.9263 med nan few nan) lr 1.0000e-02 elapsed 0:02:53 eta 0:17:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.9987 (2.2023) acc 59.3750 (48.7506) (mean 49.1946 many 49.1946 med nan few nan) lr 1.0000e-02 elapsed 0:02:59 eta 0:17:10\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.9987 (2.2023) acc 59.3750 (48.7506) (mean 49.1946 many 49.1946 med nan few nan) lr 1.0000e-02 elapsed 0:02:59 eta 0:17:10\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/782] time 0.312 (0.310) data 0.002 (0.001) loss 1.8105 (2.1048) acc 54.6875 (49.6675) (mean 49.8768 many 49.8768 med nan few nan) lr 1.0000e-02 elapsed 0:03:05 eta 0:17:04\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/782] time 0.312 (0.310) data 0.002 (0.001) loss 1.8105 (2.1048) acc 54.6875 (49.6675) (mean 49.8768 many 49.8768 med nan few nan) lr 1.0000e-02 elapsed 0:03:05 eta 0:17:04\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.7867 (2.0837) acc 59.3750 (49.9556) (mean 50.0862 many 50.0862 med nan few nan) lr 1.0000e-02 elapsed 0:03:11 eta 0:16:58\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.7867 (2.0837) acc 59.3750 (49.9556) (mean 50.0862 many 50.0862 med nan few nan) lr 1.0000e-02 elapsed 0:03:11 eta 0:16:58\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [640/782] time 0.305 (0.310) data 0.000 (0.001) loss 2.5843 (2.2400) acc 34.3750 (44.9849) (mean 46.4427 many 46.4427 med nan few nan) lr 1.0000e-02 elapsed 0:03:18 eta 0:16:52\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [640/782] time 0.305 (0.310) data 0.000 (0.001) loss 2.5843 (2.2400) acc 34.3750 (44.9849) (mean 46.4427 many 46.4427 med nan few nan) lr 1.0000e-02 elapsed 0:03:18 eta 0:16:52\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [660/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.2801 (2.1173) acc 50.0000 (48.6022) (mean 47.9859 many 47.9859 med nan few nan) lr 1.0000e-02 elapsed 0:03:24 eta 0:16:46\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [660/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.2801 (2.1173) acc 50.0000 (48.6022) (mean 47.9859 many 47.9859 med nan few nan) lr 1.0000e-02 elapsed 0:03:24 eta 0:16:46\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [680/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8033 (2.0852) acc 53.1250 (50.8344) (mean 50.2970 many 50.2970 med nan few nan) lr 1.0000e-02 elapsed 0:03:30 eta 0:16:39\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [680/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8033 (2.0852) acc 53.1250 (50.8344) (mean 50.2970 many 50.2970 med nan few nan) lr 1.0000e-02 elapsed 0:03:30 eta 0:16:39\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [700/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.0035 (2.1340) acc 53.1250 (49.6851) (mean 49.9033 many 49.9033 med nan few nan) lr 1.0000e-02 elapsed 0:03:36 eta 0:16:33\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [700/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.0035 (2.1340) acc 53.1250 (49.6851) (mean 49.9033 many 49.9033 med nan few nan) lr 1.0000e-02 elapsed 0:03:36 eta 0:16:33\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [720/782] time 0.313 (0.310) data 0.000 (0.001) loss 2.1046 (2.0901) acc 48.4375 (49.7974) (mean 49.6342 many 49.6342 med nan few nan) lr 1.0000e-02 elapsed 0:03:42 eta 0:16:27\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [720/782] time 0.313 (0.310) data 0.000 (0.001) loss 2.1046 (2.0901) acc 48.4375 (49.7974) (mean 49.6342 many 49.6342 med nan few nan) lr 1.0000e-02 elapsed 0:03:42 eta 0:16:27\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [740/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.9329 (2.1277) acc 53.1250 (47.6549) (mean 47.8150 many 47.8150 med nan few nan) lr 1.0000e-02 elapsed 0:03:49 eta 0:16:21\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [740/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.9329 (2.1277) acc 53.1250 (47.6549) (mean 47.8150 many 47.8150 med nan few nan) lr 1.0000e-02 elapsed 0:03:49 eta 0:16:21\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [760/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.5154 (2.0562) acc 67.1875 (51.2193) (mean 50.3042 many 50.3042 med nan few nan) lr 1.0000e-02 elapsed 0:03:55 eta 0:16:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [760/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.5154 (2.0562) acc 67.1875 (51.2193) (mean 50.3042 many 50.3042 med nan few nan) lr 1.0000e-02 elapsed 0:03:55 eta 0:16:15\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [780/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.4442 (2.0698) acc 34.3750 (50.5782) (mean 50.3681 many 50.3681 med nan few nan) lr 1.0000e-02 elapsed 0:04:01 eta 0:16:08\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [780/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.4442 (2.0698) acc 34.3750 (50.5782) (mean 50.3681 many 50.3681 med nan few nan) lr 1.0000e-02 elapsed 0:04:01 eta 0:16:08\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [782/782] time 0.303 (0.310) data 0.000 (0.001) loss 1.2631 (2.0003) acc 62.5000 (51.9996) (mean 50.5888 many 50.5888 med nan few nan) lr 1.0000e-02 elapsed 0:04:01 eta 0:16:08\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [782/782] time 0.303 (0.310) data 0.000 (0.001) loss 1.2631 (2.0003) acc 62.5000 (51.9996) (mean 50.5888 many 50.5888 med nan few nan) lr 1.0000e-02 elapsed 0:04:01 eta 0:16:08\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.4646 (2.0434) acc 43.7500 (51.9799) (mean 51.9165 many 51.9165 med nan few nan) lr 9.0451e-03 elapsed 0:04:08 eta 0:16:02\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.4646 (2.0434) acc 43.7500 (51.9799) (mean 51.9165 many 51.9165 med nan few nan) lr 9.0451e-03 elapsed 0:04:08 eta 0:16:02\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1567 (2.0638) acc 42.1875 (50.3646) (mean 51.6975 many 51.6975 med nan few nan) lr 9.0451e-03 elapsed 0:04:14 eta 0:15:56\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1567 (2.0638) acc 42.1875 (50.3646) (mean 51.6975 many 51.6975 med nan few nan) lr 9.0451e-03 elapsed 0:04:14 eta 0:15:56\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.2483 (1.9633) acc 43.7500 (53.4318) (mean 52.9889 many 52.9889 med nan few nan) lr 9.0451e-03 elapsed 0:04:20 eta 0:15:50\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.2483 (1.9633) acc 43.7500 (53.4318) (mean 52.9889 many 52.9889 med nan few nan) lr 9.0451e-03 elapsed 0:04:20 eta 0:15:50\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.4057 (2.0890) acc 43.7500 (48.9486) (mean 49.7438 many 49.7438 med nan few nan) lr 9.0451e-03 elapsed 0:04:27 eta 0:15:43\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.4057 (2.0890) acc 43.7500 (48.9486) (mean 49.7438 many 49.7438 med nan few nan) lr 9.0451e-03 elapsed 0:04:27 eta 0:15:43\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.2982 (2.0382) acc 46.8750 (51.8829) (mean 51.7725 many 51.7725 med nan few nan) lr 9.0451e-03 elapsed 0:04:33 eta 0:15:37\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.2982 (2.0382) acc 46.8750 (51.8829) (mean 51.7725 many 51.7725 med nan few nan) lr 9.0451e-03 elapsed 0:04:33 eta 0:15:37\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8788 (2.0909) acc 56.2500 (50.9157) (mean 50.8932 many 50.8932 med nan few nan) lr 9.0451e-03 elapsed 0:04:39 eta 0:15:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8788 (2.0909) acc 56.2500 (50.9157) (mean 50.8932 many 50.8932 med nan few nan) lr 9.0451e-03 elapsed 0:04:39 eta 0:15:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.2931 (2.1115) acc 45.3125 (49.2197) (mean 50.3977 many 50.3977 med nan few nan) lr 9.0451e-03 elapsed 0:04:45 eta 0:15:25\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.2931 (2.1115) acc 45.3125 (49.2197) (mean 50.3977 many 50.3977 med nan few nan) lr 9.0451e-03 elapsed 0:04:45 eta 0:15:25\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.9831 (2.0461) acc 51.5625 (50.6163) (mean 50.9384 many 50.9384 med nan few nan) lr 9.0451e-03 elapsed 0:04:51 eta 0:15:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.9831 (2.0461) acc 51.5625 (50.6163) (mean 50.9384 many 50.9384 med nan few nan) lr 9.0451e-03 elapsed 0:04:51 eta 0:15:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.1144 (2.0480) acc 46.8750 (48.3891) (mean 49.4250 many 49.4250 med nan few nan) lr 9.0451e-03 elapsed 0:04:58 eta 0:15:13\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.1144 (2.0480) acc 46.8750 (48.3891) (mean 49.4250 many 49.4250 med nan few nan) lr 9.0451e-03 elapsed 0:04:58 eta 0:15:13\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.6932 (1.9830) acc 62.5000 (52.7270) (mean 52.0681 many 52.0681 med nan few nan) lr 9.0451e-03 elapsed 0:05:04 eta 0:15:07\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.6932 (1.9830) acc 62.5000 (52.7270) (mean 52.0681 many 52.0681 med nan few nan) lr 9.0451e-03 elapsed 0:05:04 eta 0:15:07\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.9417 (2.0717) acc 51.5625 (50.3309) (mean 50.6518 many 50.6518 med nan few nan) lr 9.0451e-03 elapsed 0:05:10 eta 0:15:00\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.9417 (2.0717) acc 51.5625 (50.3309) (mean 50.6518 many 50.6518 med nan few nan) lr 9.0451e-03 elapsed 0:05:10 eta 0:15:00\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.1268 (2.0874) acc 50.0000 (50.6049) (mean 50.2060 many 50.2060 med nan few nan) lr 9.0451e-03 elapsed 0:05:16 eta 0:14:54\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.1268 (2.0874) acc 50.0000 (50.6049) (mean 50.2060 many 50.2060 med nan few nan) lr 9.0451e-03 elapsed 0:05:16 eta 0:14:54\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.0790 (2.0542) acc 50.0000 (50.3544) (mean 50.7196 many 50.7196 med nan few nan) lr 9.0451e-03 elapsed 0:05:22 eta 0:14:48\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.0790 (2.0542) acc 50.0000 (50.3544) (mean 50.7196 many 50.7196 med nan few nan) lr 9.0451e-03 elapsed 0:05:22 eta 0:14:48\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/782] time 0.304 (0.310) data 0.000 (0.001) loss 2.0537 (2.0767) acc 48.4375 (47.4144) (mean 48.2612 many 48.2612 med nan few nan) lr 9.0451e-03 elapsed 0:05:29 eta 0:14:42\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/782] time 0.304 (0.310) data 0.000 (0.001) loss 2.0537 (2.0767) acc 48.4375 (47.4144) (mean 48.2612 many 48.2612 med nan few nan) lr 9.0451e-03 elapsed 0:05:29 eta 0:14:42\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.0911 (1.9833) acc 51.5625 (50.9914) (mean 50.0324 many 50.0324 med nan few nan) lr 9.0451e-03 elapsed 0:05:35 eta 0:14:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.0911 (1.9833) acc 51.5625 (50.9914) (mean 50.0324 many 50.0324 med nan few nan) lr 9.0451e-03 elapsed 0:05:35 eta 0:14:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8795 (1.9496) acc 53.1250 (53.6083) (mean 52.2112 many 52.2112 med nan few nan) lr 9.0451e-03 elapsed 0:05:41 eta 0:14:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8795 (1.9496) acc 53.1250 (53.6083) (mean 52.2112 many 52.2112 med nan few nan) lr 9.0451e-03 elapsed 0:05:41 eta 0:14:29\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8439 (1.9842) acc 54.6875 (53.9456) (mean 53.0180 many 53.0180 med nan few nan) lr 9.0451e-03 elapsed 0:05:47 eta 0:14:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8439 (1.9842) acc 54.6875 (53.9456) (mean 53.0180 many 53.0180 med nan few nan) lr 9.0451e-03 elapsed 0:05:47 eta 0:14:23\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/782] time 0.315 (0.310) data 0.000 (0.001) loss 1.6075 (1.9875) acc 60.9375 (52.0893) (mean 51.5994 many 51.5994 med nan few nan) lr 9.0451e-03 elapsed 0:05:53 eta 0:14:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/782] time 0.315 (0.310) data 0.000 (0.001) loss 1.6075 (1.9875) acc 60.9375 (52.0893) (mean 51.5994 many 51.5994 med nan few nan) lr 9.0451e-03 elapsed 0:05:53 eta 0:14:17\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.2046 (2.0163) acc 45.3125 (51.9153) (mean 51.8700 many 51.8700 med nan few nan) lr 9.0451e-03 elapsed 0:05:59 eta 0:14:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.2046 (2.0163) acc 45.3125 (51.9153) (mean 51.8700 many 51.8700 med nan few nan) lr 9.0451e-03 elapsed 0:05:59 eta 0:14:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.0772 (2.0374) acc 45.3125 (49.2905) (mean 49.6823 many 49.6823 med nan few nan) lr 9.0451e-03 elapsed 0:06:06 eta 0:14:04\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.0772 (2.0374) acc 45.3125 (49.2905) (mean 49.6823 many 49.6823 med nan few nan) lr 9.0451e-03 elapsed 0:06:06 eta 0:14:04\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8894 (2.0225) acc 54.6875 (50.6945) (mean 50.3984 many 50.3984 med nan few nan) lr 9.0451e-03 elapsed 0:06:12 eta 0:13:58\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8894 (2.0225) acc 54.6875 (50.6945) (mean 50.3984 many 50.3984 med nan few nan) lr 9.0451e-03 elapsed 0:06:12 eta 0:13:58\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/782] time 0.311 (0.310) data 0.000 (0.001) loss 2.1985 (2.0335) acc 37.5000 (50.1007) (mean 50.3444 many 50.3444 med nan few nan) lr 9.0451e-03 elapsed 0:06:18 eta 0:13:52\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/782] time 0.311 (0.310) data 0.000 (0.001) loss 2.1985 (2.0335) acc 37.5000 (50.1007) (mean 50.3444 many 50.3444 med nan few nan) lr 9.0451e-03 elapsed 0:06:18 eta 0:13:52\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.3448 (2.0328) acc 42.1875 (49.2647) (mean 49.8325 many 49.8325 med nan few nan) lr 9.0451e-03 elapsed 0:06:24 eta 0:13:46\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.3448 (2.0328) acc 42.1875 (49.2647) (mean 49.8325 many 49.8325 med nan few nan) lr 9.0451e-03 elapsed 0:06:24 eta 0:13:46\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0040 (1.9377) acc 46.8750 (51.4442) (mean 51.1948 many 51.1948 med nan few nan) lr 9.0451e-03 elapsed 0:06:30 eta 0:13:39\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0040 (1.9377) acc 46.8750 (51.4442) (mean 51.1948 many 51.1948 med nan few nan) lr 9.0451e-03 elapsed 0:06:30 eta 0:13:39\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8823 (1.9256) acc 57.8125 (54.1463) (mean 52.9203 many 52.9203 med nan few nan) lr 9.0451e-03 elapsed 0:06:37 eta 0:13:33\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8823 (1.9256) acc 57.8125 (54.1463) (mean 52.9203 many 52.9203 med nan few nan) lr 9.0451e-03 elapsed 0:06:37 eta 0:13:33\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/782] time 0.312 (0.310) data 0.003 (0.001) loss 1.9339 (1.9912) acc 51.5625 (51.0771) (mean 50.9713 many 50.9713 med nan few nan) lr 9.0451e-03 elapsed 0:06:43 eta 0:13:27\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/782] time 0.312 (0.310) data 0.003 (0.001) loss 1.9339 (1.9912) acc 51.5625 (51.0771) (mean 50.9713 many 50.9713 med nan few nan) lr 9.0451e-03 elapsed 0:06:43 eta 0:13:27\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/782] time 0.305 (0.310) data 0.000 (0.001) loss 2.0900 (2.0195) acc 51.5625 (51.2535) (mean 50.7728 many 50.7728 med nan few nan) lr 9.0451e-03 elapsed 0:06:49 eta 0:13:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/782] time 0.305 (0.310) data 0.000 (0.001) loss 2.0900 (2.0195) acc 51.5625 (51.2535) (mean 50.7728 many 50.7728 med nan few nan) lr 9.0451e-03 elapsed 0:06:49 eta 0:13:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1501 (2.0568) acc 48.4375 (50.1874) (mean 50.6510 many 50.6510 med nan few nan) lr 9.0451e-03 elapsed 0:06:55 eta 0:13:15\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1501 (2.0568) acc 48.4375 (50.1874) (mean 50.6510 many 50.6510 med nan few nan) lr 9.0451e-03 elapsed 0:06:55 eta 0:13:15\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/782] time 0.305 (0.310) data 0.000 (0.001) loss 1.7179 (1.9731) acc 60.9375 (51.9583) (mean 51.0967 many 51.0967 med nan few nan) lr 9.0451e-03 elapsed 0:07:01 eta 0:13:08\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/782] time 0.305 (0.310) data 0.000 (0.001) loss 1.7179 (1.9731) acc 60.9375 (51.9583) (mean 51.0967 many 51.0967 med nan few nan) lr 9.0451e-03 elapsed 0:07:01 eta 0:13:08\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/782] time 0.306 (0.310) data 0.000 (0.001) loss 2.3004 (1.9838) acc 45.3125 (51.4484) (mean 50.9507 many 50.9507 med nan few nan) lr 9.0451e-03 elapsed 0:07:14 eta 0:12:56\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/782] time 0.306 (0.310) data 0.000 (0.001) loss 2.3004 (1.9838) acc 45.3125 (51.4484) (mean 50.9507 many 50.9507 med nan few nan) lr 9.0451e-03 elapsed 0:07:14 eta 0:12:56\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [640/782] time 0.310 (0.309) data 0.002 (0.001) loss 2.5222 (2.1070) acc 35.9375 (49.5363) (mean 50.8810 many 50.8810 med nan few nan) lr 9.0451e-03 elapsed 0:07:20 eta 0:12:49\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [640/782] time 0.310 (0.309) data 0.002 (0.001) loss 2.5222 (2.1070) acc 35.9375 (49.5363) (mean 50.8810 many 50.8810 med nan few nan) lr 9.0451e-03 elapsed 0:07:20 eta 0:12:49\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [660/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.7792 (1.9882) acc 57.8125 (51.5545) (mean 51.5375 many 51.5375 med nan few nan) lr 9.0451e-03 elapsed 0:07:26 eta 0:12:43\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [660/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.7792 (1.9882) acc 57.8125 (51.5545) (mean 51.5375 many 51.5375 med nan few nan) lr 9.0451e-03 elapsed 0:07:26 eta 0:12:43\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [680/782] time 0.304 (0.309) data 0.000 (0.001) loss 2.3681 (2.0052) acc 45.3125 (51.4372) (mean 51.4812 many 51.4812 med nan few nan) lr 9.0451e-03 elapsed 0:07:32 eta 0:12:37\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [680/782] time 0.304 (0.309) data 0.000 (0.001) loss 2.3681 (2.0052) acc 45.3125 (51.4372) (mean 51.4812 many 51.4812 med nan few nan) lr 9.0451e-03 elapsed 0:07:32 eta 0:12:37\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [700/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.5424 (2.0191) acc 37.5000 (50.6926) (mean 51.4000 many 51.4000 med nan few nan) lr 9.0451e-03 elapsed 0:07:38 eta 0:12:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [700/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.5424 (2.0191) acc 37.5000 (50.6926) (mean 51.4000 many 51.4000 med nan few nan) lr 9.0451e-03 elapsed 0:07:38 eta 0:12:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [720/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.4817 (1.9712) acc 62.5000 (51.2408) (mean 51.7003 many 51.7003 med nan few nan) lr 9.0451e-03 elapsed 0:07:44 eta 0:12:24\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [720/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.4817 (1.9712) acc 62.5000 (51.2408) (mean 51.7003 many 51.7003 med nan few nan) lr 9.0451e-03 elapsed 0:07:44 eta 0:12:24\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [740/782] time 0.314 (0.309) data 0.000 (0.001) loss 2.2599 (1.9654) acc 45.3125 (51.8399) (mean 52.0112 many 52.0112 med nan few nan) lr 9.0451e-03 elapsed 0:07:50 eta 0:12:18\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [740/782] time 0.314 (0.309) data 0.000 (0.001) loss 2.2599 (1.9654) acc 45.3125 (51.8399) (mean 52.0112 many 52.0112 med nan few nan) lr 9.0451e-03 elapsed 0:07:50 eta 0:12:18\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [760/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.9493 (2.0095) acc 54.6875 (49.2869) (mean 49.7035 many 49.7035 med nan few nan) lr 9.0451e-03 elapsed 0:07:57 eta 0:12:12\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [760/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.9493 (2.0095) acc 54.6875 (49.2869) (mean 49.7035 many 49.7035 med nan few nan) lr 9.0451e-03 elapsed 0:07:57 eta 0:12:12\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [780/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.1944 (1.9608) acc 42.1875 (51.8624) (mean 51.6460 many 51.6460 med nan few nan) lr 9.0451e-03 elapsed 0:08:03 eta 0:12:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [780/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.1944 (1.9608) acc 42.1875 (51.8624) (mean 51.6460 many 51.6460 med nan few nan) lr 9.0451e-03 elapsed 0:08:03 eta 0:12:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [782/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.5666 (1.9680) acc 68.7500 (52.2585) (mean 50.9727 many 50.9727 med nan few nan) lr 9.0451e-03 elapsed 0:08:03 eta 0:12:05\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [782/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.5666 (1.9680) acc 68.7500 (52.2585) (mean 50.9727 many 50.9727 med nan few nan) lr 9.0451e-03 elapsed 0:08:03 eta 0:12:05\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/782] time 0.308 (0.309) data 0.001 (0.001) loss 2.0309 (1.9637) acc 53.1250 (53.7377) (mean 52.8825 many 52.8825 med nan few nan) lr 6.5451e-03 elapsed 0:08:10 eta 0:11:59\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/782] time 0.308 (0.309) data 0.001 (0.001) loss 2.0309 (1.9637) acc 53.1250 (53.7377) (mean 52.8825 many 52.8825 med nan few nan) lr 6.5451e-03 elapsed 0:08:10 eta 0:11:59\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.3902 (2.0783) acc 54.6875 (51.0866) (mean 51.4811 many 51.4811 med nan few nan) lr 6.5451e-03 elapsed 0:08:16 eta 0:11:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.3902 (2.0783) acc 54.6875 (51.0866) (mean 51.4811 many 51.4811 med nan few nan) lr 6.5451e-03 elapsed 0:08:16 eta 0:11:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.7961 (1.9127) acc 62.5000 (54.2504) (mean 52.6905 many 52.6905 med nan few nan) lr 6.5451e-03 elapsed 0:08:22 eta 0:11:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.7961 (1.9127) acc 62.5000 (54.2504) (mean 52.6905 many 52.6905 med nan few nan) lr 6.5451e-03 elapsed 0:08:22 eta 0:11:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/782] time 0.317 (0.310) data 0.000 (0.001) loss 1.6393 (1.9963) acc 60.9375 (51.2395) (mean 52.0036 many 52.0036 med nan few nan) lr 6.5451e-03 elapsed 0:08:28 eta 0:11:41\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/782] time 0.317 (0.310) data 0.000 (0.001) loss 1.6393 (1.9963) acc 60.9375 (51.2395) (mean 52.0036 many 52.0036 med nan few nan) lr 6.5451e-03 elapsed 0:08:28 eta 0:11:41\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8975 (1.9117) acc 59.3750 (54.6205) (mean 53.7371 many 53.7371 med nan few nan) lr 6.5451e-03 elapsed 0:08:35 eta 0:11:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8975 (1.9117) acc 59.3750 (54.6205) (mean 53.7371 many 53.7371 med nan few nan) lr 6.5451e-03 elapsed 0:08:35 eta 0:11:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.1847 (2.0160) acc 48.4375 (50.4134) (mean 51.3486 many 51.3486 med nan few nan) lr 6.5451e-03 elapsed 0:08:41 eta 0:11:29\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.1847 (2.0160) acc 48.4375 (50.4134) (mean 51.3486 many 51.3486 med nan few nan) lr 6.5451e-03 elapsed 0:08:41 eta 0:11:29\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.3712 (2.0047) acc 46.8750 (50.5005) (mean 50.8283 many 50.8283 med nan few nan) lr 6.5451e-03 elapsed 0:08:47 eta 0:11:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.3712 (2.0047) acc 46.8750 (50.5005) (mean 50.8283 many 50.8283 med nan few nan) lr 6.5451e-03 elapsed 0:08:47 eta 0:11:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/782] time 0.315 (0.310) data 0.000 (0.001) loss 2.1428 (1.9756) acc 46.8750 (51.1196) (mean 51.4963 many 51.4963 med nan few nan) lr 6.5451e-03 elapsed 0:08:53 eta 0:11:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/782] time 0.315 (0.310) data 0.000 (0.001) loss 2.1428 (1.9756) acc 46.8750 (51.1196) (mean 51.4963 many 51.4963 med nan few nan) lr 6.5451e-03 elapsed 0:08:53 eta 0:11:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/782] time 0.306 (0.310) data 0.000 (0.001) loss 2.0120 (2.0414) acc 46.8750 (50.0737) (mean 51.6772 many 51.6772 med nan few nan) lr 6.5451e-03 elapsed 0:09:00 eta 0:11:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/782] time 0.306 (0.310) data 0.000 (0.001) loss 2.0120 (2.0414) acc 46.8750 (50.0737) (mean 51.6772 many 51.6772 med nan few nan) lr 6.5451e-03 elapsed 0:09:00 eta 0:11:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/782] time 0.313 (0.310) data 0.000 (0.001) loss 1.9648 (1.9160) acc 54.6875 (53.7192) (mean 53.5121 many 53.5121 med nan few nan) lr 6.5451e-03 elapsed 0:09:06 eta 0:11:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/782] time 0.313 (0.310) data 0.000 (0.001) loss 1.9648 (1.9160) acc 54.6875 (53.7192) (mean 53.5121 many 53.5121 med nan few nan) lr 6.5451e-03 elapsed 0:09:06 eta 0:11:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1586 (1.9117) acc 50.0000 (54.1663) (mean 53.8844 many 53.8844 med nan few nan) lr 6.5451e-03 elapsed 0:09:12 eta 0:10:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.1586 (1.9117) acc 50.0000 (54.1663) (mean 53.8844 many 53.8844 med nan few nan) lr 6.5451e-03 elapsed 0:09:12 eta 0:10:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/782] time 0.315 (0.310) data 0.000 (0.001) loss 1.8057 (2.0521) acc 54.6875 (50.1256) (mean 51.1703 many 51.1703 med nan few nan) lr 6.5451e-03 elapsed 0:09:18 eta 0:10:51\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/782] time 0.315 (0.310) data 0.000 (0.001) loss 1.8057 (2.0521) acc 54.6875 (50.1256) (mean 51.1703 many 51.1703 med nan few nan) lr 6.5451e-03 elapsed 0:09:18 eta 0:10:51\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/782] time 0.306 (0.310) data 0.000 (0.001) loss 1.4565 (1.8830) acc 60.9375 (54.7301) (mean 53.6955 many 53.6955 med nan few nan) lr 6.5451e-03 elapsed 0:09:24 eta 0:10:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/782] time 0.306 (0.310) data 0.000 (0.001) loss 1.4565 (1.8830) acc 60.9375 (54.7301) (mean 53.6955 many 53.6955 med nan few nan) lr 6.5451e-03 elapsed 0:09:24 eta 0:10:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/782] time 0.306 (0.310) data 0.000 (0.001) loss 1.6236 (1.8584) acc 68.7500 (55.2206) (mean 54.2243 many 54.2243 med nan few nan) lr 6.5451e-03 elapsed 0:09:30 eta 0:10:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/782] time 0.306 (0.310) data 0.000 (0.001) loss 1.6236 (1.8584) acc 68.7500 (55.2206) (mean 54.2243 many 54.2243 med nan few nan) lr 6.5451e-03 elapsed 0:09:30 eta 0:10:39\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.3536 (2.0142) acc 40.6250 (49.9284) (mean 51.4085 many 51.4085 med nan few nan) lr 6.5451e-03 elapsed 0:09:37 eta 0:10:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.3536 (2.0142) acc 40.6250 (49.9284) (mean 51.4085 many 51.4085 med nan few nan) lr 6.5451e-03 elapsed 0:09:37 eta 0:10:33\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.9736 (1.9773) acc 48.4375 (50.8084) (mean 51.5386 many 51.5386 med nan few nan) lr 6.5451e-03 elapsed 0:09:43 eta 0:10:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.9736 (1.9773) acc 48.4375 (50.8084) (mean 51.5386 many 51.5386 med nan few nan) lr 6.5451e-03 elapsed 0:09:43 eta 0:10:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.0393 (2.0316) acc 51.5625 (51.4891) (mean 52.0550 many 52.0550 med nan few nan) lr 6.5451e-03 elapsed 0:09:49 eta 0:10:20\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.0393 (2.0316) acc 51.5625 (51.4891) (mean 52.0550 many 52.0550 med nan few nan) lr 6.5451e-03 elapsed 0:09:49 eta 0:10:20\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.1684 (2.0008) acc 46.8750 (51.2686) (mean 51.4198 many 51.4198 med nan few nan) lr 6.5451e-03 elapsed 0:09:55 eta 0:10:14\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.1684 (2.0008) acc 46.8750 (51.2686) (mean 51.4198 many 51.4198 med nan few nan) lr 6.5451e-03 elapsed 0:09:55 eta 0:10:14\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.9972 (1.9542) acc 45.3125 (51.8762) (mean 51.8115 many 51.8115 med nan few nan) lr 6.5451e-03 elapsed 0:10:01 eta 0:10:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.9972 (1.9542) acc 45.3125 (51.8762) (mean 51.8115 many 51.8115 med nan few nan) lr 6.5451e-03 elapsed 0:10:01 eta 0:10:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.3107 (1.9578) acc 43.7500 (53.1325) (mean 53.2092 many 53.2092 med nan few nan) lr 6.5451e-03 elapsed 0:10:08 eta 0:10:02\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.3107 (1.9578) acc 43.7500 (53.1325) (mean 53.2092 many 53.2092 med nan few nan) lr 6.5451e-03 elapsed 0:10:08 eta 0:10:02\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.6774 (1.9477) acc 62.5000 (52.9657) (mean 52.4721 many 52.4721 med nan few nan) lr 6.5451e-03 elapsed 0:10:14 eta 0:09:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.6774 (1.9477) acc 62.5000 (52.9657) (mean 52.4721 many 52.4721 med nan few nan) lr 6.5451e-03 elapsed 0:10:14 eta 0:09:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.8242 (1.8910) acc 56.2500 (54.3232) (mean 54.1617 many 54.1617 med nan few nan) lr 6.5451e-03 elapsed 0:10:20 eta 0:09:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.8242 (1.8910) acc 56.2500 (54.3232) (mean 54.1617 many 54.1617 med nan few nan) lr 6.5451e-03 elapsed 0:10:20 eta 0:09:50\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/782] time 0.315 (0.310) data 0.000 (0.001) loss 2.0543 (1.9310) acc 48.4375 (52.6772) (mean 53.3102 many 53.3102 med nan few nan) lr 6.5451e-03 elapsed 0:10:26 eta 0:09:43\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/782] time 0.315 (0.310) data 0.000 (0.001) loss 2.0543 (1.9310) acc 48.4375 (52.6772) (mean 53.3102 many 53.3102 med nan few nan) lr 6.5451e-03 elapsed 0:10:26 eta 0:09:43\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.5510 (1.8904) acc 67.1875 (53.0673) (mean 52.6239 many 52.6239 med nan few nan) lr 6.5451e-03 elapsed 0:10:32 eta 0:09:37\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.5510 (1.8904) acc 67.1875 (53.0673) (mean 52.6239 many 52.6239 med nan few nan) lr 6.5451e-03 elapsed 0:10:32 eta 0:09:37\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.0413 (1.9374) acc 56.2500 (53.1842) (mean 52.6626 many 52.6626 med nan few nan) lr 6.5451e-03 elapsed 0:10:39 eta 0:09:31\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.0413 (1.9374) acc 56.2500 (53.1842) (mean 52.6626 many 52.6626 med nan few nan) lr 6.5451e-03 elapsed 0:10:39 eta 0:09:31\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.1650 (1.9390) acc 43.7500 (53.5146) (mean 53.8158 many 53.8158 med nan few nan) lr 6.5451e-03 elapsed 0:10:45 eta 0:09:25\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/782] time 0.310 (0.310) data 0.000 (0.001) loss 2.1650 (1.9390) acc 43.7500 (53.5146) (mean 53.8158 many 53.8158 med nan few nan) lr 6.5451e-03 elapsed 0:10:45 eta 0:09:25\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.7332 (1.9634) acc 62.5000 (51.7257) (mean 52.2868 many 52.2868 med nan few nan) lr 6.5451e-03 elapsed 0:10:51 eta 0:09:18\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.7332 (1.9634) acc 62.5000 (51.7257) (mean 52.2868 many 52.2868 med nan few nan) lr 6.5451e-03 elapsed 0:10:51 eta 0:09:18\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/782] time 0.304 (0.309) data 0.000 (0.001) loss 1.8510 (1.9361) acc 50.0000 (52.9350) (mean 52.5119 many 52.5119 med nan few nan) lr 6.5451e-03 elapsed 0:10:57 eta 0:09:12\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/782] time 0.304 (0.309) data 0.000 (0.001) loss 1.8510 (1.9361) acc 50.0000 (52.9350) (mean 52.5119 many 52.5119 med nan few nan) lr 6.5451e-03 elapsed 0:10:57 eta 0:09:12\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.8482 (1.9744) acc 57.8125 (52.8964) (mean 52.8157 many 52.8157 med nan few nan) lr 6.5451e-03 elapsed 0:11:03 eta 0:09:06\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.8482 (1.9744) acc 57.8125 (52.8964) (mean 52.8157 many 52.8157 med nan few nan) lr 6.5451e-03 elapsed 0:11:03 eta 0:09:06\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/782] time 0.308 (0.309) data 0.001 (0.001) loss 1.6226 (1.9472) acc 62.5000 (51.9035) (mean 52.7837 many 52.7837 med nan few nan) lr 6.5451e-03 elapsed 0:11:09 eta 0:09:00\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/782] time 0.308 (0.309) data 0.001 (0.001) loss 1.6226 (1.9472) acc 62.5000 (51.9035) (mean 52.7837 many 52.7837 med nan few nan) lr 6.5451e-03 elapsed 0:11:09 eta 0:09:00\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.7181 (1.9162) acc 62.5000 (54.2828) (mean 53.7476 many 53.7476 med nan few nan) lr 6.5451e-03 elapsed 0:11:16 eta 0:08:54\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.7181 (1.9162) acc 62.5000 (54.2828) (mean 53.7476 many 53.7476 med nan few nan) lr 6.5451e-03 elapsed 0:11:16 eta 0:08:54\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [640/782] time 0.303 (0.309) data 0.002 (0.001) loss 1.7794 (1.9505) acc 62.5000 (53.5508) (mean 53.5162 many 53.5162 med nan few nan) lr 6.5451e-03 elapsed 0:11:22 eta 0:08:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [640/782] time 0.303 (0.309) data 0.002 (0.001) loss 1.7794 (1.9505) acc 62.5000 (53.5508) (mean 53.5162 many 53.5162 med nan few nan) lr 6.5451e-03 elapsed 0:11:22 eta 0:08:47\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [660/782] time 0.311 (0.309) data 0.001 (0.001) loss 2.1506 (1.8994) acc 51.5625 (54.4070) (mean 54.4035 many 54.4035 med nan few nan) lr 6.5451e-03 elapsed 0:11:28 eta 0:08:41\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [660/782] time 0.311 (0.309) data 0.001 (0.001) loss 2.1506 (1.8994) acc 51.5625 (54.4070) (mean 54.4035 many 54.4035 med nan few nan) lr 6.5451e-03 elapsed 0:11:28 eta 0:08:41\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [680/782] time 0.297 (0.309) data 0.000 (0.001) loss 2.1213 (1.9915) acc 48.4375 (50.7936) (mean 51.8462 many 51.8462 med nan few nan) lr 6.5451e-03 elapsed 0:11:34 eta 0:08:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [680/782] time 0.297 (0.309) data 0.000 (0.001) loss 2.1213 (1.9915) acc 48.4375 (50.7936) (mean 51.8462 many 51.8462 med nan few nan) lr 6.5451e-03 elapsed 0:11:34 eta 0:08:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [700/782] time 0.308 (0.309) data 0.001 (0.001) loss 2.1681 (1.9491) acc 43.7500 (53.2317) (mean 52.8674 many 52.8674 med nan few nan) lr 6.5451e-03 elapsed 0:11:40 eta 0:08:29\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [700/782] time 0.308 (0.309) data 0.001 (0.001) loss 2.1681 (1.9491) acc 43.7500 (53.2317) (mean 52.8674 many 52.8674 med nan few nan) lr 6.5451e-03 elapsed 0:11:40 eta 0:08:29\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [720/782] time 0.316 (0.309) data 0.000 (0.001) loss 1.8608 (1.9575) acc 51.5625 (51.2367) (mean 51.1284 many 51.1284 med nan few nan) lr 6.5451e-03 elapsed 0:11:46 eta 0:08:23\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [720/782] time 0.316 (0.309) data 0.000 (0.001) loss 1.8608 (1.9575) acc 51.5625 (51.2367) (mean 51.1284 many 51.1284 med nan few nan) lr 6.5451e-03 elapsed 0:11:46 eta 0:08:23\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [740/782] time 0.314 (0.309) data 0.000 (0.001) loss 2.2014 (1.9875) acc 45.3125 (52.3637) (mean 52.0029 many 52.0029 med nan few nan) lr 6.5451e-03 elapsed 0:11:53 eta 0:08:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [740/782] time 0.314 (0.309) data 0.000 (0.001) loss 2.2014 (1.9875) acc 45.3125 (52.3637) (mean 52.0029 many 52.0029 med nan few nan) lr 6.5451e-03 elapsed 0:11:53 eta 0:08:16\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [760/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.9286 (1.9494) acc 50.0000 (52.7780) (mean 52.2184 many 52.2184 med nan few nan) lr 6.5451e-03 elapsed 0:11:59 eta 0:08:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [760/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.9286 (1.9494) acc 50.0000 (52.7780) (mean 52.2184 many 52.2184 med nan few nan) lr 6.5451e-03 elapsed 0:11:59 eta 0:08:10\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [780/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9782 (1.9146) acc 51.5625 (52.5475) (mean 52.4579 many 52.4579 med nan few nan) lr 6.5451e-03 elapsed 0:12:05 eta 0:08:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [780/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9782 (1.9146) acc 51.5625 (52.5475) (mean 52.4579 many 52.4579 med nan few nan) lr 6.5451e-03 elapsed 0:12:05 eta 0:08:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [782/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.1742 (1.9372) acc 43.7500 (52.8448) (mean 53.1240 many 53.1240 med nan few nan) lr 6.5451e-03 elapsed 0:12:05 eta 0:08:03\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [782/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.1742 (1.9372) acc 43.7500 (52.8448) (mean 53.1240 many 53.1240 med nan few nan) lr 6.5451e-03 elapsed 0:12:05 eta 0:08:03\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/782] time 0.316 (0.310) data 0.000 (0.001) loss 1.7239 (1.8725) acc 59.3750 (55.3321) (mean 54.5525 many 54.5525 med nan few nan) lr 3.4549e-03 elapsed 0:12:12 eta 0:07:57\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/782] time 0.316 (0.310) data 0.000 (0.001) loss 1.7239 (1.8725) acc 59.3750 (55.3321) (mean 54.5525 many 54.5525 med nan few nan) lr 3.4549e-03 elapsed 0:12:12 eta 0:07:57\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.4233 (1.9693) acc 39.0625 (52.0205) (mean 52.0990 many 52.0990 med nan few nan) lr 3.4549e-03 elapsed 0:12:18 eta 0:07:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/782] time 0.307 (0.310) data 0.000 (0.001) loss 2.4233 (1.9693) acc 39.0625 (52.0205) (mean 52.0990 many 52.0990 med nan few nan) lr 3.4549e-03 elapsed 0:12:18 eta 0:07:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/782] time 0.316 (0.310) data 0.000 (0.001) loss 2.0081 (1.9459) acc 54.6875 (53.1667) (mean 53.5046 many 53.5046 med nan few nan) lr 3.4549e-03 elapsed 0:12:24 eta 0:07:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/782] time 0.316 (0.310) data 0.000 (0.001) loss 2.0081 (1.9459) acc 54.6875 (53.1667) (mean 53.5046 many 53.5046 med nan few nan) lr 3.4549e-03 elapsed 0:12:24 eta 0:07:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.0669 (1.9400) acc 45.3125 (52.5022) (mean 52.9827 many 52.9827 med nan few nan) lr 3.4549e-03 elapsed 0:12:31 eta 0:07:39\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/782] time 0.308 (0.310) data 0.000 (0.001) loss 2.0669 (1.9400) acc 45.3125 (52.5022) (mean 52.9827 many 52.9827 med nan few nan) lr 3.4549e-03 elapsed 0:12:31 eta 0:07:39\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/782] time 0.315 (0.310) data 0.000 (0.001) loss 1.9729 (1.9785) acc 43.7500 (51.9504) (mean 53.3100 many 53.3100 med nan few nan) lr 3.4549e-03 elapsed 0:12:37 eta 0:07:33\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/782] time 0.315 (0.310) data 0.000 (0.001) loss 1.9729 (1.9785) acc 43.7500 (51.9504) (mean 53.3100 many 53.3100 med nan few nan) lr 3.4549e-03 elapsed 0:12:37 eta 0:07:33\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/782] time 0.312 (0.310) data 0.000 (0.001) loss 1.9744 (1.9057) acc 50.0000 (53.8131) (mean 54.7450 many 54.7450 med nan few nan) lr 3.4549e-03 elapsed 0:12:43 eta 0:07:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/782] time 0.312 (0.310) data 0.000 (0.001) loss 1.9744 (1.9057) acc 50.0000 (53.8131) (mean 54.7450 many 54.7450 med nan few nan) lr 3.4549e-03 elapsed 0:12:43 eta 0:07:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/782] time 0.305 (0.310) data 0.000 (0.001) loss 1.6935 (1.8367) acc 59.3750 (55.8632) (mean 55.3734 many 55.3734 med nan few nan) lr 3.4549e-03 elapsed 0:12:49 eta 0:07:20\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/782] time 0.305 (0.310) data 0.000 (0.001) loss 1.6935 (1.8367) acc 59.3750 (55.8632) (mean 55.3734 many 55.3734 med nan few nan) lr 3.4549e-03 elapsed 0:12:49 eta 0:07:20\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/782] time 0.303 (0.310) data 0.000 (0.001) loss 1.8472 (1.7667) acc 60.9375 (58.7661) (mean 57.2669 many 57.2669 med nan few nan) lr 3.4549e-03 elapsed 0:12:55 eta 0:07:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/782] time 0.303 (0.310) data 0.000 (0.001) loss 1.8472 (1.7667) acc 60.9375 (58.7661) (mean 57.2669 many 57.2669 med nan few nan) lr 3.4549e-03 elapsed 0:12:55 eta 0:07:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8795 (1.9367) acc 51.5625 (52.5006) (mean 53.2013 many 53.2013 med nan few nan) lr 3.4549e-03 elapsed 0:13:02 eta 0:07:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8795 (1.9367) acc 51.5625 (52.5006) (mean 53.2013 many 53.2013 med nan few nan) lr 3.4549e-03 elapsed 0:13:02 eta 0:07:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.8979 (1.9195) acc 57.8125 (54.5812) (mean 55.0868 many 55.0868 med nan few nan) lr 3.4549e-03 elapsed 0:13:08 eta 0:07:02\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.8979 (1.9195) acc 57.8125 (54.5812) (mean 55.0868 many 55.0868 med nan few nan) lr 3.4549e-03 elapsed 0:13:08 eta 0:07:02\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.7322 (1.8703) acc 57.8125 (55.6415) (mean 55.2105 many 55.2105 med nan few nan) lr 3.4549e-03 elapsed 0:13:14 eta 0:06:55\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.7322 (1.8703) acc 57.8125 (55.6415) (mean 55.2105 many 55.2105 med nan few nan) lr 3.4549e-03 elapsed 0:13:14 eta 0:06:55\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/782] time 0.304 (0.309) data 0.000 (0.001) loss 1.8314 (1.8503) acc 57.8125 (55.8618) (mean 55.0753 many 55.0753 med nan few nan) lr 3.4549e-03 elapsed 0:13:20 eta 0:06:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/782] time 0.304 (0.309) data 0.000 (0.001) loss 1.8314 (1.8503) acc 57.8125 (55.8618) (mean 55.0753 many 55.0753 med nan few nan) lr 3.4549e-03 elapsed 0:13:20 eta 0:06:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/782] time 0.308 (0.309) data 0.000 (0.001) loss 2.0862 (1.9050) acc 50.0000 (53.3843) (mean 53.8984 many 53.8984 med nan few nan) lr 3.4549e-03 elapsed 0:13:26 eta 0:06:43\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/782] time 0.308 (0.309) data 0.000 (0.001) loss 2.0862 (1.9050) acc 50.0000 (53.3843) (mean 53.8984 many 53.8984 med nan few nan) lr 3.4549e-03 elapsed 0:13:26 eta 0:06:43\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.5911 (1.9615) acc 59.3750 (52.5423) (mean 52.8427 many 52.8427 med nan few nan) lr 3.4549e-03 elapsed 0:13:32 eta 0:06:37\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.5911 (1.9615) acc 59.3750 (52.5423) (mean 52.8427 many 52.8427 med nan few nan) lr 3.4549e-03 elapsed 0:13:32 eta 0:06:37\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.3190 (1.9862) acc 45.3125 (52.4783) (mean 52.7051 many 52.7051 med nan few nan) lr 3.4549e-03 elapsed 0:13:38 eta 0:06:31\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/782] time 0.309 (0.309) data 0.000 (0.001) loss 2.3190 (1.9862) acc 45.3125 (52.4783) (mean 52.7051 many 52.7051 med nan few nan) lr 3.4549e-03 elapsed 0:13:38 eta 0:06:31\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.9640 (1.9416) acc 59.3750 (54.7924) (mean 55.0720 many 55.0720 med nan few nan) lr 3.4549e-03 elapsed 0:13:45 eta 0:06:24\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.9640 (1.9416) acc 59.3750 (54.7924) (mean 55.0720 many 55.0720 med nan few nan) lr 3.4549e-03 elapsed 0:13:45 eta 0:06:24\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/782] time 0.318 (0.309) data 0.000 (0.001) loss 1.9159 (1.9141) acc 53.1250 (53.2674) (mean 54.0253 many 54.0253 med nan few nan) lr 3.4549e-03 elapsed 0:13:51 eta 0:06:18\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/782] time 0.318 (0.309) data 0.000 (0.001) loss 1.9159 (1.9141) acc 53.1250 (53.2674) (mean 54.0253 many 54.0253 med nan few nan) lr 3.4549e-03 elapsed 0:13:51 eta 0:06:18\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/782] time 0.312 (0.309) data 0.000 (0.001) loss 2.3328 (2.0092) acc 45.3125 (51.8756) (mean 52.5991 many 52.5991 med nan few nan) lr 3.4549e-03 elapsed 0:13:57 eta 0:06:12\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/782] time 0.312 (0.309) data 0.000 (0.001) loss 2.3328 (2.0092) acc 45.3125 (51.8756) (mean 52.5991 many 52.5991 med nan few nan) lr 3.4549e-03 elapsed 0:13:57 eta 0:06:12\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/782] time 0.313 (0.309) data 0.002 (0.001) loss 2.1170 (1.9576) acc 48.4375 (52.3434) (mean 52.9480 many 52.9480 med nan few nan) lr 3.4549e-03 elapsed 0:14:03 eta 0:06:06\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/782] time 0.313 (0.309) data 0.002 (0.001) loss 2.1170 (1.9576) acc 48.4375 (52.3434) (mean 52.9480 many 52.9480 med nan few nan) lr 3.4549e-03 elapsed 0:14:03 eta 0:06:06\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9526 (1.9622) acc 51.5625 (53.7215) (mean 53.4057 many 53.4057 med nan few nan) lr 3.4549e-03 elapsed 0:14:10 eta 0:06:00\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.9526 (1.9622) acc 51.5625 (53.7215) (mean 53.4057 many 53.4057 med nan few nan) lr 3.4549e-03 elapsed 0:14:10 eta 0:06:00\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/782] time 0.310 (0.309) data 0.000 (0.001) loss 2.2535 (1.9896) acc 46.8750 (51.7141) (mean 52.0597 many 52.0597 med nan few nan) lr 3.4549e-03 elapsed 0:14:16 eta 0:05:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/782] time 0.310 (0.309) data 0.000 (0.001) loss 2.2535 (1.9896) acc 46.8750 (51.7141) (mean 52.0597 many 52.0597 med nan few nan) lr 3.4549e-03 elapsed 0:14:16 eta 0:05:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.8018 (1.8894) acc 62.5000 (54.9777) (mean 54.4117 many 54.4117 med nan few nan) lr 3.4549e-03 elapsed 0:14:22 eta 0:05:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/782] time 0.306 (0.309) data 0.000 (0.001) loss 1.8018 (1.8894) acc 62.5000 (54.9777) (mean 54.4117 many 54.4117 med nan few nan) lr 3.4549e-03 elapsed 0:14:22 eta 0:05:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/782] time 0.310 (0.309) data 0.000 (0.001) loss 2.1175 (1.9193) acc 48.4375 (53.3676) (mean 53.7197 many 53.7197 med nan few nan) lr 3.4549e-03 elapsed 0:14:28 eta 0:05:41\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/782] time 0.310 (0.309) data 0.000 (0.001) loss 2.1175 (1.9193) acc 48.4375 (53.3676) (mean 53.7197 many 53.7197 med nan few nan) lr 3.4549e-03 elapsed 0:14:28 eta 0:05:41\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.9370 (1.8969) acc 64.0625 (55.5355) (mean 55.1255 many 55.1255 med nan few nan) lr 3.4549e-03 elapsed 0:14:34 eta 0:05:35\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.9370 (1.8969) acc 64.0625 (55.5355) (mean 55.1255 many 55.1255 med nan few nan) lr 3.4549e-03 elapsed 0:14:34 eta 0:05:35\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.6698 (1.9367) acc 57.8125 (54.8816) (mean 55.1155 many 55.1155 med nan few nan) lr 3.4549e-03 elapsed 0:14:41 eta 0:05:29\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.6698 (1.9367) acc 57.8125 (54.8816) (mean 55.1155 many 55.1155 med nan few nan) lr 3.4549e-03 elapsed 0:14:41 eta 0:05:29\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/782] time 0.305 (0.309) data 0.001 (0.001) loss 2.0233 (1.9339) acc 50.0000 (53.8881) (mean 55.1295 many 55.1295 med nan few nan) lr 3.4549e-03 elapsed 0:14:47 eta 0:05:23\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/782] time 0.305 (0.309) data 0.001 (0.001) loss 2.0233 (1.9339) acc 50.0000 (53.8881) (mean 55.1295 many 55.1295 med nan few nan) lr 3.4549e-03 elapsed 0:14:47 eta 0:05:23\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/782] time 0.310 (0.309) data 0.000 (0.001) loss 1.7581 (1.9680) acc 53.1250 (52.0990) (mean 52.7831 many 52.7831 med nan few nan) lr 3.4549e-03 elapsed 0:14:53 eta 0:05:16\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/782] time 0.310 (0.309) data 0.000 (0.001) loss 1.7581 (1.9680) acc 53.1250 (52.0990) (mean 52.7831 many 52.7831 med nan few nan) lr 3.4549e-03 elapsed 0:14:53 eta 0:05:16\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/782] time 0.304 (0.309) data 0.001 (0.001) loss 2.3114 (1.9658) acc 46.8750 (53.1484) (mean 53.5424 many 53.5424 med nan few nan) lr 3.4549e-03 elapsed 0:14:59 eta 0:05:10\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/782] time 0.304 (0.309) data 0.001 (0.001) loss 2.3114 (1.9658) acc 46.8750 (53.1484) (mean 53.5424 many 53.5424 med nan few nan) lr 3.4549e-03 elapsed 0:14:59 eta 0:05:10\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/782] time 0.301 (0.309) data 0.000 (0.001) loss 1.9495 (1.8848) acc 56.2500 (55.6379) (mean 55.3333 many 55.3333 med nan few nan) lr 3.4549e-03 elapsed 0:15:05 eta 0:05:04\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/782] time 0.301 (0.309) data 0.000 (0.001) loss 1.9495 (1.8848) acc 56.2500 (55.6379) (mean 55.3333 many 55.3333 med nan few nan) lr 3.4549e-03 elapsed 0:15:05 eta 0:05:04\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/782] time 0.315 (0.309) data 0.000 (0.001) loss 1.7988 (1.9439) acc 54.6875 (53.4729) (mean 54.1241 many 54.1241 med nan few nan) lr 3.4549e-03 elapsed 0:15:11 eta 0:04:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/782] time 0.315 (0.309) data 0.000 (0.001) loss 1.7988 (1.9439) acc 54.6875 (53.4729) (mean 54.1241 many 54.1241 med nan few nan) lr 3.4549e-03 elapsed 0:15:11 eta 0:04:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.8422 (1.9120) acc 53.1250 (52.4974) (mean 53.0302 many 53.0302 med nan few nan) lr 3.4549e-03 elapsed 0:15:17 eta 0:04:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.8422 (1.9120) acc 53.1250 (52.4974) (mean 53.0302 many 53.0302 med nan few nan) lr 3.4549e-03 elapsed 0:15:17 eta 0:04:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [640/782] time 0.314 (0.309) data 0.000 (0.001) loss 1.8707 (1.8628) acc 56.2500 (54.5371) (mean 54.3615 many 54.3615 med nan few nan) lr 3.4549e-03 elapsed 0:15:24 eta 0:04:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [640/782] time 0.314 (0.309) data 0.000 (0.001) loss 1.8707 (1.8628) acc 56.2500 (54.5371) (mean 54.3615 many 54.3615 med nan few nan) lr 3.4549e-03 elapsed 0:15:24 eta 0:04:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [660/782] time 0.310 (0.309) data 0.000 (0.001) loss 1.9184 (1.8937) acc 45.3125 (53.5090) (mean 53.8715 many 53.8715 med nan few nan) lr 3.4549e-03 elapsed 0:15:30 eta 0:04:39\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [660/782] time 0.310 (0.309) data 0.000 (0.001) loss 1.9184 (1.8937) acc 45.3125 (53.5090) (mean 53.8715 many 53.8715 med nan few nan) lr 3.4549e-03 elapsed 0:15:30 eta 0:04:39\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [680/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.7241 (1.8640) acc 64.0625 (56.0585) (mean 55.9865 many 55.9865 med nan few nan) lr 3.4549e-03 elapsed 0:15:36 eta 0:04:33\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [680/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.7241 (1.8640) acc 64.0625 (56.0585) (mean 55.9865 many 55.9865 med nan few nan) lr 3.4549e-03 elapsed 0:15:36 eta 0:04:33\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [700/782] time 0.313 (0.309) data 0.000 (0.001) loss 1.8092 (1.9198) acc 59.3750 (55.0547) (mean 55.5042 many 55.5042 med nan few nan) lr 3.4549e-03 elapsed 0:15:42 eta 0:04:27\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [700/782] time 0.313 (0.309) data 0.000 (0.001) loss 1.8092 (1.9198) acc 59.3750 (55.0547) (mean 55.5042 many 55.5042 med nan few nan) lr 3.4549e-03 elapsed 0:15:42 eta 0:04:27\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [720/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.8160 (1.8863) acc 59.3750 (55.4063) (mean 55.7977 many 55.7977 med nan few nan) lr 3.4549e-03 elapsed 0:15:48 eta 0:04:21\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [720/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.8160 (1.8863) acc 59.3750 (55.4063) (mean 55.7977 many 55.7977 med nan few nan) lr 3.4549e-03 elapsed 0:15:48 eta 0:04:21\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [740/782] time 0.306 (0.309) data 0.000 (0.001) loss 2.0121 (1.9196) acc 53.1250 (53.1841) (mean 54.3169 many 54.3169 med nan few nan) lr 3.4549e-03 elapsed 0:15:55 eta 0:04:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [740/782] time 0.306 (0.309) data 0.000 (0.001) loss 2.0121 (1.9196) acc 53.1250 (53.1841) (mean 54.3169 many 54.3169 med nan few nan) lr 3.4549e-03 elapsed 0:15:55 eta 0:04:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [760/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9982 (1.9012) acc 53.1250 (54.3566) (mean 54.5016 many 54.5016 med nan few nan) lr 3.4549e-03 elapsed 0:16:01 eta 0:04:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [760/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.9982 (1.9012) acc 53.1250 (54.3566) (mean 54.5016 many 54.5016 med nan few nan) lr 3.4549e-03 elapsed 0:16:01 eta 0:04:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [780/782] time 0.312 (0.309) data 0.000 (0.001) loss 2.1456 (1.9343) acc 40.6250 (53.2123) (mean 53.4923 many 53.4923 med nan few nan) lr 3.4549e-03 elapsed 0:16:07 eta 0:04:02\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [780/782] time 0.312 (0.309) data 0.000 (0.001) loss 2.1456 (1.9343) acc 40.6250 (53.2123) (mean 53.4923 many 53.4923 med nan few nan) lr 3.4549e-03 elapsed 0:16:07 eta 0:04:02\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [782/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.2286 (1.8843) acc 68.7500 (54.0551) (mean 53.1965 many 53.1965 med nan few nan) lr 3.4549e-03 elapsed 0:16:07 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [782/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.2286 (1.8843) acc 68.7500 (54.0551) (mean 53.1965 many 53.1965 med nan few nan) lr 3.4549e-03 elapsed 0:16:07 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/782] time 0.305 (0.309) data 0.000 (0.001) loss 2.1515 (1.9355) acc 46.8750 (53.2996) (mean 53.4535 many 53.4535 med nan few nan) lr 9.5492e-04 elapsed 0:16:14 eta 0:03:55\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/782] time 0.305 (0.309) data 0.000 (0.001) loss 2.1515 (1.9355) acc 46.8750 (53.2996) (mean 53.4535 many 53.4535 med nan few nan) lr 9.5492e-04 elapsed 0:16:14 eta 0:03:55\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.8151 (1.8250) acc 54.6875 (56.1719) (mean 54.8692 many 54.8692 med nan few nan) lr 9.5492e-04 elapsed 0:16:20 eta 0:03:49\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/782] time 0.307 (0.309) data 0.000 (0.001) loss 1.8151 (1.8250) acc 54.6875 (56.1719) (mean 54.8692 many 54.8692 med nan few nan) lr 9.5492e-04 elapsed 0:16:20 eta 0:03:49\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.8918 (1.8330) acc 54.6875 (57.4398) (mean 56.7031 many 56.7031 med nan few nan) lr 9.5492e-04 elapsed 0:16:26 eta 0:03:43\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/782] time 0.311 (0.309) data 0.000 (0.001) loss 1.8918 (1.8330) acc 54.6875 (57.4398) (mean 56.7031 many 56.7031 med nan few nan) lr 9.5492e-04 elapsed 0:16:26 eta 0:03:43\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.7163 (1.8937) acc 51.5625 (52.7743) (mean 54.0501 many 54.0501 med nan few nan) lr 9.5492e-04 elapsed 0:16:33 eta 0:03:37\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.7163 (1.8937) acc 51.5625 (52.7743) (mean 54.0501 many 54.0501 med nan few nan) lr 9.5492e-04 elapsed 0:16:33 eta 0:03:37\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.0668 (1.9500) acc 54.6875 (54.4923) (mean 54.5032 many 54.5032 med nan few nan) lr 9.5492e-04 elapsed 0:16:39 eta 0:03:31\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/782] time 0.307 (0.309) data 0.000 (0.001) loss 2.0668 (1.9500) acc 54.6875 (54.4923) (mean 54.5032 many 54.5032 med nan few nan) lr 9.5492e-04 elapsed 0:16:39 eta 0:03:31\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.7776 (1.8379) acc 62.5000 (56.0355) (mean 55.6847 many 55.6847 med nan few nan) lr 9.5492e-04 elapsed 0:16:45 eta 0:03:24\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/782] time 0.305 (0.309) data 0.000 (0.001) loss 1.7776 (1.8379) acc 62.5000 (56.0355) (mean 55.6847 many 55.6847 med nan few nan) lr 9.5492e-04 elapsed 0:16:45 eta 0:03:24\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.7331 (1.8355) acc 57.8125 (56.1488) (mean 55.9127 many 55.9127 med nan few nan) lr 9.5492e-04 elapsed 0:16:51 eta 0:03:18\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.7331 (1.8355) acc 57.8125 (56.1488) (mean 55.9127 many 55.9127 med nan few nan) lr 9.5492e-04 elapsed 0:16:51 eta 0:03:18\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/782] time 0.301 (0.309) data 0.000 (0.001) loss 2.0036 (1.8926) acc 48.4375 (53.4378) (mean 54.0170 many 54.0170 med nan few nan) lr 9.5492e-04 elapsed 0:16:57 eta 0:03:12\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/782] time 0.301 (0.309) data 0.000 (0.001) loss 2.0036 (1.8926) acc 48.4375 (53.4378) (mean 54.0170 many 54.0170 med nan few nan) lr 9.5492e-04 elapsed 0:16:57 eta 0:03:12\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.8939 (1.8449) acc 51.5625 (55.2057) (mean 54.4609 many 54.4609 med nan few nan) lr 9.5492e-04 elapsed 0:17:04 eta 0:03:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.8939 (1.8449) acc 51.5625 (55.2057) (mean 54.4609 many 54.4609 med nan few nan) lr 9.5492e-04 elapsed 0:17:04 eta 0:03:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/782] time 0.312 (0.309) data 0.001 (0.001) loss 1.9909 (1.8707) acc 54.6875 (55.8316) (mean 56.1819 many 56.1819 med nan few nan) lr 9.5492e-04 elapsed 0:17:10 eta 0:03:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/782] time 0.312 (0.309) data 0.001 (0.001) loss 1.9909 (1.8707) acc 54.6875 (55.8316) (mean 56.1819 many 56.1819 med nan few nan) lr 9.5492e-04 elapsed 0:17:10 eta 0:03:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.8768 (1.8083) acc 56.2500 (56.9136) (mean 55.6287 many 55.6287 med nan few nan) lr 9.5492e-04 elapsed 0:17:16 eta 0:02:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.8768 (1.8083) acc 56.2500 (56.9136) (mean 55.6287 many 55.6287 med nan few nan) lr 9.5492e-04 elapsed 0:17:16 eta 0:02:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [240/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.7799 (1.8999) acc 60.9375 (55.7668) (mean 55.3954 many 55.3954 med nan few nan) lr 9.5492e-04 elapsed 0:17:22 eta 0:02:47\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [240/782] time 0.308 (0.309) data 0.000 (0.001) loss 1.7799 (1.8999) acc 60.9375 (55.7668) (mean 55.3954 many 55.3954 med nan few nan) lr 9.5492e-04 elapsed 0:17:22 eta 0:02:47\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [260/782] time 0.305 (0.309) data 0.000 (0.001) loss 2.0015 (1.9020) acc 53.1250 (54.3977) (mean 54.4029 many 54.4029 med nan few nan) lr 9.5492e-04 elapsed 0:17:28 eta 0:02:41\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [260/782] time 0.305 (0.309) data 0.000 (0.001) loss 2.0015 (1.9020) acc 53.1250 (54.3977) (mean 54.4029 many 54.4029 med nan few nan) lr 9.5492e-04 elapsed 0:17:28 eta 0:02:41\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [280/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.6546 (1.8443) acc 56.2500 (55.5280) (mean 55.0213 many 55.0213 med nan few nan) lr 9.5492e-04 elapsed 0:17:35 eta 0:02:35\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [280/782] time 0.312 (0.309) data 0.000 (0.001) loss 1.6546 (1.8443) acc 56.2500 (55.5280) (mean 55.0213 many 55.0213 med nan few nan) lr 9.5492e-04 elapsed 0:17:35 eta 0:02:35\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [300/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.7747 (1.8517) acc 62.5000 (54.9883) (mean 54.4718 many 54.4718 med nan few nan) lr 9.5492e-04 elapsed 0:17:41 eta 0:02:29\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [300/782] time 0.309 (0.309) data 0.000 (0.001) loss 1.7747 (1.8517) acc 62.5000 (54.9883) (mean 54.4718 many 54.4718 med nan few nan) lr 9.5492e-04 elapsed 0:17:41 eta 0:02:29\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [320/782] time 0.311 (0.310) data 0.001 (0.001) loss 2.0372 (1.9046) acc 57.8125 (55.3999) (mean 55.5435 many 55.5435 med nan few nan) lr 9.5492e-04 elapsed 0:17:47 eta 0:02:22\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [320/782] time 0.311 (0.310) data 0.001 (0.001) loss 2.0372 (1.9046) acc 57.8125 (55.3999) (mean 55.5435 many 55.5435 med nan few nan) lr 9.5492e-04 elapsed 0:17:47 eta 0:02:22\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [340/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.5702 (1.8196) acc 65.6250 (56.9808) (mean 56.1776 many 56.1776 med nan few nan) lr 9.5492e-04 elapsed 0:17:53 eta 0:02:16\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [340/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.5702 (1.8196) acc 65.6250 (56.9808) (mean 56.1776 many 56.1776 med nan few nan) lr 9.5492e-04 elapsed 0:17:53 eta 0:02:16\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [360/782] time 0.313 (0.310) data 0.000 (0.001) loss 2.1013 (1.9084) acc 45.3125 (55.0714) (mean 55.3525 many 55.3525 med nan few nan) lr 9.5492e-04 elapsed 0:17:59 eta 0:02:10\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [360/782] time 0.313 (0.310) data 0.000 (0.001) loss 2.1013 (1.9084) acc 45.3125 (55.0714) (mean 55.3525 many 55.3525 med nan few nan) lr 9.5492e-04 elapsed 0:17:59 eta 0:02:10\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [380/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.7109 (1.8668) acc 64.0625 (55.6544) (mean 56.3075 many 56.3075 med nan few nan) lr 9.5492e-04 elapsed 0:18:06 eta 0:02:04\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [380/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.7109 (1.8668) acc 64.0625 (55.6544) (mean 56.3075 many 56.3075 med nan few nan) lr 9.5492e-04 elapsed 0:18:06 eta 0:02:04\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [400/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8750 (1.8230) acc 53.1250 (55.8004) (mean 55.5167 many 55.5167 med nan few nan) lr 9.5492e-04 elapsed 0:18:12 eta 0:01:58\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [400/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8750 (1.8230) acc 53.1250 (55.8004) (mean 55.5167 many 55.5167 med nan few nan) lr 9.5492e-04 elapsed 0:18:12 eta 0:01:58\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [420/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8551 (1.9067) acc 51.5625 (53.5334) (mean 53.3156 many 53.3156 med nan few nan) lr 9.5492e-04 elapsed 0:18:18 eta 0:01:52\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [420/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.8551 (1.9067) acc 51.5625 (53.5334) (mean 53.3156 many 53.3156 med nan few nan) lr 9.5492e-04 elapsed 0:18:18 eta 0:01:52\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [440/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.6904 (1.9023) acc 59.3750 (52.9970) (mean 53.2670 many 53.2670 med nan few nan) lr 9.5492e-04 elapsed 0:18:24 eta 0:01:45\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [440/782] time 0.310 (0.310) data 0.000 (0.001) loss 1.6904 (1.9023) acc 59.3750 (52.9970) (mean 53.2670 many 53.2670 med nan few nan) lr 9.5492e-04 elapsed 0:18:24 eta 0:01:45\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [460/782] time 0.313 (0.310) data 0.000 (0.001) loss 1.8157 (1.8415) acc 64.0625 (57.6379) (mean 55.9256 many 55.9256 med nan few nan) lr 9.5492e-04 elapsed 0:18:30 eta 0:01:39\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [460/782] time 0.313 (0.310) data 0.000 (0.001) loss 1.8157 (1.8415) acc 64.0625 (57.6379) (mean 55.9256 many 55.9256 med nan few nan) lr 9.5492e-04 elapsed 0:18:30 eta 0:01:39\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [480/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.5524 (1.8417) acc 62.5000 (58.3828) (mean 57.8612 many 57.8612 med nan few nan) lr 9.5492e-04 elapsed 0:18:37 eta 0:01:33\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [480/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.5524 (1.8417) acc 62.5000 (58.3828) (mean 57.8612 many 57.8612 med nan few nan) lr 9.5492e-04 elapsed 0:18:37 eta 0:01:33\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [500/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.7479 (1.8408) acc 57.8125 (57.6165) (mean 57.0057 many 57.0057 med nan few nan) lr 9.5492e-04 elapsed 0:18:43 eta 0:01:27\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [500/782] time 0.307 (0.310) data 0.000 (0.001) loss 1.7479 (1.8408) acc 57.8125 (57.6165) (mean 57.0057 many 57.0057 med nan few nan) lr 9.5492e-04 elapsed 0:18:43 eta 0:01:27\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [520/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.6821 (1.8322) acc 62.5000 (57.3968) (mean 56.6579 many 56.6579 med nan few nan) lr 9.5492e-04 elapsed 0:18:49 eta 0:01:21\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [520/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.6821 (1.8322) acc 62.5000 (57.3968) (mean 56.6579 many 56.6579 med nan few nan) lr 9.5492e-04 elapsed 0:18:49 eta 0:01:21\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [540/782] time 0.307 (0.310) data 0.005 (0.001) loss 1.4814 (1.8342) acc 67.1875 (56.2314) (mean 56.0189 many 56.0189 med nan few nan) lr 9.5492e-04 elapsed 0:18:55 eta 0:01:14\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [540/782] time 0.307 (0.310) data 0.005 (0.001) loss 1.4814 (1.8342) acc 67.1875 (56.2314) (mean 56.0189 many 56.0189 med nan few nan) lr 9.5492e-04 elapsed 0:18:55 eta 0:01:14\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [560/782] time 0.312 (0.310) data 0.000 (0.001) loss 1.7661 (1.8355) acc 56.2500 (56.2643) (mean 56.6768 many 56.6768 med nan few nan) lr 9.5492e-04 elapsed 0:19:01 eta 0:01:08\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [560/782] time 0.312 (0.310) data 0.000 (0.001) loss 1.7661 (1.8355) acc 56.2500 (56.2643) (mean 56.6768 many 56.6768 med nan few nan) lr 9.5492e-04 elapsed 0:19:01 eta 0:01:08\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [580/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.0578 (1.8889) acc 54.6875 (55.0127) (mean 54.6903 many 54.6903 med nan few nan) lr 9.5492e-04 elapsed 0:19:08 eta 0:01:02\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [580/782] time 0.314 (0.310) data 0.000 (0.001) loss 2.0578 (1.8889) acc 54.6875 (55.0127) (mean 54.6903 many 54.6903 med nan few nan) lr 9.5492e-04 elapsed 0:19:08 eta 0:01:02\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [600/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0365 (1.8970) acc 53.1250 (55.6115) (mean 54.8991 many 54.8991 med nan few nan) lr 9.5492e-04 elapsed 0:19:14 eta 0:00:56\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [600/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0365 (1.8970) acc 53.1250 (55.6115) (mean 54.8991 many 54.8991 med nan few nan) lr 9.5492e-04 elapsed 0:19:14 eta 0:00:56\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [620/782] time 0.314 (0.310) data 0.003 (0.001) loss 2.0967 (1.9270) acc 59.3750 (56.3580) (mean 55.4937 many 55.4937 med nan few nan) lr 9.5492e-04 elapsed 0:19:20 eta 0:00:50\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [620/782] time 0.314 (0.310) data 0.003 (0.001) loss 2.0967 (1.9270) acc 59.3750 (56.3580) (mean 55.4937 many 55.4937 med nan few nan) lr 9.5492e-04 elapsed 0:19:20 eta 0:00:50\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [640/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.5322 (1.8736) acc 67.1875 (55.0993) (mean 55.2187 many 55.2187 med nan few nan) lr 9.5492e-04 elapsed 0:19:26 eta 0:00:43\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [640/782] time 0.309 (0.310) data 0.000 (0.001) loss 1.5322 (1.8736) acc 67.1875 (55.0993) (mean 55.2187 many 55.2187 med nan few nan) lr 9.5492e-04 elapsed 0:19:26 eta 0:00:43\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [660/782] time 0.324 (0.310) data 0.000 (0.001) loss 1.8982 (1.9118) acc 50.0000 (52.4326) (mean 53.4521 many 53.4521 med nan few nan) lr 9.5492e-04 elapsed 0:19:33 eta 0:00:37\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [660/782] time 0.324 (0.310) data 0.000 (0.001) loss 1.8982 (1.9118) acc 50.0000 (52.4326) (mean 53.4521 many 53.4521 med nan few nan) lr 9.5492e-04 elapsed 0:19:33 eta 0:00:37\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [680/782] time 0.311 (0.310) data 0.000 (0.001) loss 2.0763 (1.9280) acc 46.8750 (52.5961) (mean 53.3865 many 53.3865 med nan few nan) lr 9.5492e-04 elapsed 0:19:39 eta 0:00:31\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [680/782] time 0.311 (0.310) data 0.000 (0.001) loss 2.0763 (1.9280) acc 46.8750 (52.5961) (mean 53.3865 many 53.3865 med nan few nan) lr 9.5492e-04 elapsed 0:19:39 eta 0:00:31\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [700/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.7719 (1.8453) acc 57.8125 (56.7005) (mean 56.7943 many 56.7943 med nan few nan) lr 9.5492e-04 elapsed 0:19:45 eta 0:00:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [700/782] time 0.314 (0.310) data 0.000 (0.001) loss 1.7719 (1.8453) acc 57.8125 (56.7005) (mean 56.7943 many 56.7943 med nan few nan) lr 9.5492e-04 elapsed 0:19:45 eta 0:00:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [720/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.9581 (1.8948) acc 56.2500 (54.6377) (mean 55.0110 many 55.0110 med nan few nan) lr 9.5492e-04 elapsed 0:19:51 eta 0:00:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [720/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.9581 (1.8948) acc 56.2500 (54.6377) (mean 55.0110 many 55.0110 med nan few nan) lr 9.5492e-04 elapsed 0:19:51 eta 0:00:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [740/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8175 (1.8707) acc 54.6875 (54.9377) (mean 55.1201 many 55.1201 med nan few nan) lr 9.5492e-04 elapsed 0:19:57 eta 0:00:13\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [740/782] time 0.311 (0.310) data 0.000 (0.001) loss 1.8175 (1.8707) acc 54.6875 (54.9377) (mean 55.1201 many 55.1201 med nan few nan) lr 9.5492e-04 elapsed 0:19:57 eta 0:00:13\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [760/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.2167 (1.8654) acc 50.0000 (55.8600) (mean 55.7745 many 55.7745 med nan few nan) lr 9.5492e-04 elapsed 0:20:04 eta 0:00:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [760/782] time 0.309 (0.310) data 0.000 (0.001) loss 2.2167 (1.8654) acc 50.0000 (55.8600) (mean 55.7745 many 55.7745 med nan few nan) lr 9.5492e-04 elapsed 0:20:04 eta 0:00:06\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [780/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8028 (1.8562) acc 56.2500 (54.6679) (mean 55.0133 many 55.0133 med nan few nan) lr 9.5492e-04 elapsed 0:20:10 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [780/782] time 0.308 (0.310) data 0.000 (0.001) loss 1.8028 (1.8562) acc 56.2500 (54.6679) (mean 55.0133 many 55.0133 med nan few nan) lr 9.5492e-04 elapsed 0:20:10 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [782/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0159 (1.8718) acc 43.7500 (53.5779) (mean 54.9577 many 54.9577 med nan few nan) lr 9.5492e-04 elapsed 0:20:10 eta 0:00:00\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [782/782] time 0.312 (0.310) data 0.000 (0.001) loss 2.0159 (1.8718) acc 43.7500 (53.5779) (mean 54.9577 many 54.9577 med nan few nan) lr 9.5492e-04 elapsed 0:20:10 eta 0:00:00\u001b[0m\n",
      "\u001b[37mFinished training\u001b[0m\n",
      "\u001b[37mNote: Printed training accuracy is approximate. Use test_train=True for precise evaluation.\u001b[0m\n",
      "\u001b[37mTotal training time: 0:20:10\u001b[0m\n",
      "\u001b[37mFinished training\u001b[0m\n",
      "\u001b[37mNote: Printed training accuracy is approximate. Use test_train=True for precise evaluation.\u001b[0m\n",
      "\u001b[37mTotal training time: 0:20:10\u001b[0m\n",
      "\u001b[37mCheckpoint saved to /content/metalora/output/notebooks/cifar100_baseline_svhn/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mCheckpoint saved to /content/metalora/output/notebooks/cifar100_baseline_svhn/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      " 10%|9         | 15/157 [00:13<02:04]\u001b[0m\n",
      " 10%|9         | 15/157 [00:13<02:04]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:49]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:49]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:38<01:35]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:38<01:35]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:22]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:22]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:04<01:09]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:04<01:09]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:29<00:44]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:29<00:44]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:42<00:31]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:42<00:31]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:07<00:05]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:07<00:05]\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,545\n",
      "* accuracy: 75.5%\n",
      "* error: 24.5%\n",
      "* macro_f1: 75.2%\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,545\n",
      "* accuracy: 75.5%\n",
      "* error: 24.5%\n",
      "* macro_f1: 75.2%\u001b[0m\n",
      "\u001b[37m* class acc: [92. 89. 86. 73. 39. 70. 79. 78. 87. 97. 58. 73. 85. 80. 85. 85. 80. 94.\n",
      " 65. 86. 89. 86. 85. 79. 83. 67. 59. 48. 83. 75. 67. 77. 59. 61. 74. 70.\n",
      " 79. 87. 67. 89. 72. 87. 54. 85. 66. 49. 88. 58. 96. 87. 54. 76. 77. 96.\n",
      " 83. 31. 90. 85. 93. 59. 89. 78. 78. 53. 70. 83. 66. 52. 98. 87. 86. 91.\n",
      " 58. 69. 37. 77. 90. 75. 67. 76. 62. 65. 94. 79. 66. 85. 80. 92. 65. 92.\n",
      " 85. 80. 72. 57. 82. 67. 46. 71. 88. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 31.0%\n",
      "* hmean_acc: 71.9%\n",
      "* gmean_acc: 73.9%\u001b[0m\n",
      "\u001b[37m* class acc: [92. 89. 86. 73. 39. 70. 79. 78. 87. 97. 58. 73. 85. 80. 85. 85. 80. 94.\n",
      " 65. 86. 89. 86. 85. 79. 83. 67. 59. 48. 83. 75. 67. 77. 59. 61. 74. 70.\n",
      " 79. 87. 67. 89. 72. 87. 54. 85. 66. 49. 88. 58. 96. 87. 54. 76. 77. 96.\n",
      " 83. 31. 90. 85. 93. 59. 89. 78. 78. 53. 70. 83. 66. 52. 98. 87. 86. 91.\n",
      " 58. 69. 37. 77. 90. 75. 67. 76. 62. 65. 94. 79. 66. 85. 80. 92. 65. 92.\n",
      " 85. 80. 72. 57. 82. 67. 46. 71. 88. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 31.0%\n",
      "* hmean_acc: 71.9%\n",
      "* gmean_acc: 73.9%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.5%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.45, 'error_rate': 24.549999999999997, 'macro_f1': 75.17204335517434, 'worst_case_acc': 31.0, 'hmean_acc': np.float64(71.9393458807257), 'gmean_acc': np.float64(73.85612287779), 'many_acc': np.float64(75.45), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.45)})\u001b[0m\n",
      "\u001b[37m* many: 75.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.5%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.45, 'error_rate': 24.549999999999997, 'macro_f1': 75.17204335517434, 'worst_case_acc': 31.0, 'hmean_acc': np.float64(71.9393458807257), 'gmean_acc': np.float64(73.85612287779), 'many_acc': np.float64(75.45), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.45)})\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RUN_TRAINING = {\n",
    "    \"balanced\": True,\n",
    "    \"ir10\": True,\n",
    "    \"ir50\": True,\n",
    "    \"ir100\": True,\n",
    "}\n",
    "\n",
    "for tag, trainer_group in trainers.items():\n",
    "    run_flag = RUN_TRAINING.get(tag, False)\n",
    "    for method_name, trainer in trainer_group.items():\n",
    "        if run_flag:\n",
    "            logger.section(f\"Training: {tag} | {method_name}\")\n",
    "            trainer.train()\n",
    "        else:\n",
    "            print(\n",
    "                f\"Skipping training for {tag} | {method_name}; expecting checkpoints under {trainer.cfg.output_dir}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      " 10%|9         | 15/157 [00:12<02:00]\u001b[0m\n",
      " 10%|9         | 15/157 [00:12<02:00]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:48]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:48]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:39<01:37]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:39<01:37]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:24]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:24]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:04<01:09]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:04<01:09]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:28<00:43]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:28<00:43]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:41<00:30]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:41<00:30]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:07<00:05]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:07<00:05]\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,529\n",
      "* accuracy: 75.3%\n",
      "* error: 24.7%\n",
      "* macro_f1: 75.0%\u001b[0m\n",
      "\u001b[37m* class acc: [94. 89. 88. 72. 34. 74. 73. 74. 85. 95. 60. 69. 87. 81. 86. 85. 80. 94.\n",
      " 69. 85. 88. 85. 85. 77. 85. 67. 64. 46. 85. 76. 72. 81. 57. 63. 76. 72.\n",
      " 77. 86. 67. 89. 68. 84. 58. 85. 63. 33. 90. 55. 96. 88. 56. 77. 83. 98.\n",
      " 84. 38. 88. 83. 93. 56. 89. 78. 77. 52. 63. 83. 67. 55. 98. 87. 84. 92.\n",
      " 55. 66. 32. 78. 89. 76. 66. 80. 62. 69. 94. 78. 72. 82. 81. 92. 65. 93.\n",
      " 80. 76. 75. 57. 84. 65. 44. 74. 84. 87.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 32.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 73.5%\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,529\n",
      "* accuracy: 75.3%\n",
      "* error: 24.7%\n",
      "* macro_f1: 75.0%\u001b[0m\n",
      "\u001b[37m* class acc: [94. 89. 88. 72. 34. 74. 73. 74. 85. 95. 60. 69. 87. 81. 86. 85. 80. 94.\n",
      " 69. 85. 88. 85. 85. 77. 85. 67. 64. 46. 85. 76. 72. 81. 57. 63. 76. 72.\n",
      " 77. 86. 67. 89. 68. 84. 58. 85. 63. 33. 90. 55. 96. 88. 56. 77. 83. 98.\n",
      " 84. 38. 88. 83. 93. 56. 89. 78. 77. 52. 63. 83. 67. 55. 98. 87. 84. 92.\n",
      " 55. 66. 32. 78. 89. 76. 66. 80. 62. 69. 94. 78. 72. 82. 81. 92. 65. 93.\n",
      " 80. 76. 75. 57. 84. 65. 44. 74. 84. 87.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 32.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 73.5%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.3%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.3%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.29, 'error_rate': 24.709999999999994, 'macro_f1': 74.96132337155717, 'worst_case_acc': 32.0, 'hmean_acc': np.float64(71.32235795533218), 'gmean_acc': np.float64(73.52552456954989), 'many_acc': np.float64(75.29), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.29)})\u001b[0m\n",
      "\u001b[37m* many: 75.3%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.3%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.29, 'error_rate': 24.709999999999994, 'macro_f1': 74.96132337155717, 'worst_case_acc': 32.0, 'hmean_acc': np.float64(71.32235795533218), 'gmean_acc': np.float64(73.52552456954989), 'many_acc': np.float64(75.29), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.29)})\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,529\n",
      "* accuracy: 75.3%\n",
      "* error: 24.7%\n",
      "* macro_f1: 75.0%\u001b[0m\n",
      "\u001b[37m* class acc: [94. 89. 88. 72. 34. 74. 73. 74. 85. 95. 60. 69. 87. 81. 86. 85. 80. 94.\n",
      " 69. 85. 88. 85. 85. 77. 85. 67. 64. 46. 85. 76. 72. 81. 57. 63. 76. 72.\n",
      " 77. 86. 67. 89. 68. 84. 58. 85. 63. 33. 90. 55. 96. 88. 56. 77. 83. 98.\n",
      " 84. 38. 88. 83. 93. 56. 89. 78. 77. 52. 63. 83. 67. 55. 98. 87. 84. 92.\n",
      " 55. 66. 32. 78. 89. 76. 66. 80. 62. 69. 94. 78. 72. 82. 81. 92. 65. 93.\n",
      " 80. 76. 75. 57. 84. 65. 44. 74. 84. 87.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 32.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 73.5%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,529\n",
      "* accuracy: 75.3%\n",
      "* error: 24.7%\n",
      "* macro_f1: 75.0%\u001b[0m\n",
      "\u001b[37m* class acc: [94. 89. 88. 72. 34. 74. 73. 74. 85. 95. 60. 69. 87. 81. 86. 85. 80. 94.\n",
      " 69. 85. 88. 85. 85. 77. 85. 67. 64. 46. 85. 76. 72. 81. 57. 63. 76. 72.\n",
      " 77. 86. 67. 89. 68. 84. 58. 85. 63. 33. 90. 55. 96. 88. 56. 77. 83. 98.\n",
      " 84. 38. 88. 83. 93. 56. 89. 78. 77. 52. 63. 83. 67. 55. 98. 87. 84. 92.\n",
      " 55. 66. 32. 78. 89. 76. 66. 80. 62. 69. 94. 78. 72. 82. 81. 92. 65. 93.\n",
      " 80. 76. 75. 57. 84. 65. 44. 74. 84. 87.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 32.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 73.5%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.3%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.3%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.3%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.3%\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-2454144094.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-2454144094.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/157 [00:00<?]\u001b[0m\n",
      " 10%|9         | 15/157 [00:13<02:03]\u001b[0m\n",
      " 10%|9         | 15/157 [00:13<02:03]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:48]\u001b[0m\n",
      " 19%|#9        | 30/157 [00:25<01:48]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:38<01:35]\u001b[0m\n",
      " 29%|##8       | 45/157 [00:38<01:35]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:22]\u001b[0m\n",
      " 38%|###8      | 60/157 [00:51<01:22]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:03<01:09]\u001b[0m\n",
      " 48%|####7     | 75/157 [01:03<01:09]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 57%|#####7    | 90/157 [01:16<00:56]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:28<00:43]\u001b[0m\n",
      " 67%|######6   | 105/157 [01:28<00:43]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:41<00:31]\u001b[0m\n",
      " 76%|#######6  | 120/157 [01:41<00:31]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 86%|########5 | 135/157 [01:54<00:18]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:06<00:05]\u001b[0m\n",
      " 96%|#########5| 150/157 [02:06<00:05]\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "100%|##########| 157/157 [02:12<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,545\n",
      "* accuracy: 75.5%\n",
      "* error: 24.5%\n",
      "* macro_f1: 75.2%\u001b[0m\n",
      "\u001b[37m* class acc: [92. 89. 86. 73. 39. 70. 79. 78. 87. 97. 58. 73. 85. 80. 85. 85. 80. 94.\n",
      " 65. 86. 89. 86. 85. 79. 83. 67. 59. 48. 83. 75. 67. 77. 59. 61. 74. 70.\n",
      " 79. 87. 67. 89. 72. 87. 54. 85. 66. 49. 88. 58. 96. 87. 54. 76. 77. 96.\n",
      " 83. 31. 90. 85. 93. 59. 89. 78. 78. 53. 70. 83. 66. 52. 98. 87. 86. 91.\n",
      " 58. 69. 37. 77. 90. 75. 67. 76. 62. 65. 94. 79. 66. 85. 80. 92. 65. 92.\n",
      " 85. 80. 72. 57. 82. 67. 46. 71. 88. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 31.0%\n",
      "* hmean_acc: 71.9%\n",
      "* gmean_acc: 73.9%\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,545\n",
      "* accuracy: 75.5%\n",
      "* error: 24.5%\n",
      "* macro_f1: 75.2%\u001b[0m\n",
      "\u001b[37m* class acc: [92. 89. 86. 73. 39. 70. 79. 78. 87. 97. 58. 73. 85. 80. 85. 85. 80. 94.\n",
      " 65. 86. 89. 86. 85. 79. 83. 67. 59. 48. 83. 75. 67. 77. 59. 61. 74. 70.\n",
      " 79. 87. 67. 89. 72. 87. 54. 85. 66. 49. 88. 58. 96. 87. 54. 76. 77. 96.\n",
      " 83. 31. 90. 85. 93. 59. 89. 78. 78. 53. 70. 83. 66. 52. 98. 87. 86. 91.\n",
      " 58. 69. 37. 77. 90. 75. 67. 76. 62. 65. 94. 79. 66. 85. 80. 92. 65. 92.\n",
      " 85. 80. 72. 57. 82. 67. 46. 71. 88. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 31.0%\n",
      "* hmean_acc: 71.9%\n",
      "* gmean_acc: 73.9%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.5%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.45, 'error_rate': 24.549999999999997, 'macro_f1': 75.17204335517434, 'worst_case_acc': 31.0, 'hmean_acc': np.float64(71.9393458807257), 'gmean_acc': np.float64(73.85612287779), 'many_acc': np.float64(75.45), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.45)})\u001b[0m\n",
      "\u001b[37m* many: 75.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.5%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 75.45, 'error_rate': 24.549999999999997, 'macro_f1': 75.17204335517434, 'worst_case_acc': 31.0, 'hmean_acc': np.float64(71.9393458807257), 'gmean_acc': np.float64(73.85612287779), 'many_acc': np.float64(75.45), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(75.45)})\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,545\n",
      "* accuracy: 75.5%\n",
      "* error: 24.5%\n",
      "* macro_f1: 75.2%\u001b[0m\n",
      "\u001b[37m* class acc: [92. 89. 86. 73. 39. 70. 79. 78. 87. 97. 58. 73. 85. 80. 85. 85. 80. 94.\n",
      " 65. 86. 89. 86. 85. 79. 83. 67. 59. 48. 83. 75. 67. 77. 59. 61. 74. 70.\n",
      " 79. 87. 67. 89. 72. 87. 54. 85. 66. 49. 88. 58. 96. 87. 54. 76. 77. 96.\n",
      " 83. 31. 90. 85. 93. 59. 89. 78. 78. 53. 70. 83. 66. 52. 98. 87. 86. 91.\n",
      " 58. 69. 37. 77. 90. 75. 67. 76. 62. 65. 94. 79. 66. 85. 80. 92. 65. 92.\n",
      " 85. 80. 72. 57. 82. 67. 46. 71. 88. 86.]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 10,000\n",
      "* correct: 7,545\n",
      "* accuracy: 75.5%\n",
      "* error: 24.5%\n",
      "* macro_f1: 75.2%\u001b[0m\n",
      "\u001b[37m* class acc: [92. 89. 86. 73. 39. 70. 79. 78. 87. 97. 58. 73. 85. 80. 85. 85. 80. 94.\n",
      " 65. 86. 89. 86. 85. 79. 83. 67. 59. 48. 83. 75. 67. 77. 59. 61. 74. 70.\n",
      " 79. 87. 67. 89. 72. 87. 54. 85. 66. 49. 88. 58. 96. 87. 54. 76. 77. 96.\n",
      " 83. 31. 90. 85. 93. 59. 89. 78. 78. 53. 70. 83. 66. 52. 98. 87. 86. 91.\n",
      " 58. 69. 37. 77. 90. 75. 67. 76. 62. 65. 94. 79. 66. 85. 80. 92. 65. 92.\n",
      " 85. 80. 72. 57. 82. 67. 46. 71. 88. 86.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 31.0%\n",
      "* hmean_acc: 71.9%\n",
      "* gmean_acc: 73.9%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 31.0%\n",
      "* hmean_acc: 71.9%\n",
      "* gmean_acc: 73.9%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 75.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.5%\u001b[0m\n",
      "\u001b[37m* many: 75.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 75.5%\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-2454144094.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-2454144094.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[37m\n",
      "Class-Aware metrics:\u001b[0m\n",
      "\u001b[37m\n",
      "Class-Aware metrics:\u001b[0m\n",
      "\u001b[37m  id_accuracy: 75.2900\u001b[0m\n",
      "\u001b[37m  many_acc: 75.2900\u001b[0m\n",
      "\u001b[37m  med_acc: nan\u001b[0m\n",
      "\u001b[37m  few_acc: nan\u001b[0m\n",
      "\u001b[37m  auroc: 0.9123\u001b[0m\n",
      "\u001b[37m  aupr: 0.8521\u001b[0m\n",
      "\u001b[37m  fpr@95tpr: 0.4695\u001b[0m\n",
      "\u001b[37m  threshold@95tpr: 0.1411\u001b[0m\n",
      "\u001b[37m  id_mean: 0.6032\u001b[0m\n",
      "\u001b[37m  id_std: 0.2817\u001b[0m\n",
      "\u001b[37m  ood_mean: 0.1686\u001b[0m\n",
      "\u001b[37m  ood_std: 0.1309\u001b[0m\n",
      "\u001b[37m\n",
      "Baseline metrics:\u001b[0m\n",
      "\u001b[37m  id_accuracy: 75.4500\u001b[0m\n",
      "\u001b[37m  id_accuracy: 75.2900\u001b[0m\n",
      "\u001b[37m  many_acc: 75.2900\u001b[0m\n",
      "\u001b[37m  med_acc: nan\u001b[0m\n",
      "\u001b[37m  few_acc: nan\u001b[0m\n",
      "\u001b[37m  auroc: 0.9123\u001b[0m\n",
      "\u001b[37m  aupr: 0.8521\u001b[0m\n",
      "\u001b[37m  fpr@95tpr: 0.4695\u001b[0m\n",
      "\u001b[37m  threshold@95tpr: 0.1411\u001b[0m\n",
      "\u001b[37m  id_mean: 0.6032\u001b[0m\n",
      "\u001b[37m  id_std: 0.2817\u001b[0m\n",
      "\u001b[37m  ood_mean: 0.1686\u001b[0m\n",
      "\u001b[37m  ood_std: 0.1309\u001b[0m\n",
      "\u001b[37m\n",
      "Baseline metrics:\u001b[0m\n",
      "\u001b[37m  id_accuracy: 75.4500\u001b[0m\n",
      "\u001b[37m  many_acc: 75.4500\u001b[0m\n",
      "\u001b[37m  med_acc: nan\u001b[0m\n",
      "\u001b[37m  few_acc: nan\u001b[0m\n",
      "\u001b[37m  auroc: 0.9028\u001b[0m\n",
      "\u001b[37m  many_acc: 75.4500\u001b[0m\n",
      "\u001b[37m  med_acc: nan\u001b[0m\n",
      "\u001b[37m  few_acc: nan\u001b[0m\n",
      "\u001b[37m  auroc: 0.9028\u001b[0m\n",
      "\u001b[37m  aupr: 0.8343\u001b[0m\n",
      "\u001b[37m  fpr@95tpr: 0.4848\u001b[0m\n",
      "\u001b[37m  threshold@95tpr: 0.1400\u001b[0m\n",
      "\u001b[37m  id_mean: 0.6013\u001b[0m\n",
      "\u001b[37m  id_std: 0.2819\u001b[0m\n",
      "\u001b[37m  ood_mean: 0.1784\u001b[0m\n",
      "\u001b[37m  aupr: 0.8343\u001b[0m\n",
      "\u001b[37m  fpr@95tpr: 0.4848\u001b[0m\n",
      "\u001b[37m  threshold@95tpr: 0.1400\u001b[0m\n",
      "\u001b[37m  id_mean: 0.6013\u001b[0m\n",
      "\u001b[37m  id_std: 0.2819\u001b[0m\n",
      "\u001b[37m  ood_mean: 0.1784\u001b[0m\n",
      "\u001b[37m  ood_std: 0.1419\u001b[0m\n",
      "\u001b[37m  ood_std: 0.1419\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def evaluate_run(trainer):\n",
    "    id_acc_scalar = float(trainer.test())\n",
    "    id_metrics = trainer.evaluator.evaluate()\n",
    "    ood_metrics = trainer.evaluate_ood()\n",
    "    summary = {\n",
    "        \"id_accuracy\": id_acc_scalar,\n",
    "        \"many_acc\": id_metrics.get(\"many_acc\"),\n",
    "        \"med_acc\": id_metrics.get(\"med_acc\"),\n",
    "        \"few_acc\": id_metrics.get(\"few_acc\"),\n",
    "    }\n",
    "    summary.update(ood_metrics)\n",
    "    return summary\n",
    "\n",
    "summaries = {}\n",
    "for tag, trainer_group in trainers.items():\n",
    "    summaries[tag] = {}\n",
    "    for method_name, trainer in trainer_group.items():\n",
    "        print(f\"\\nEvaluating {tag} | {method_name}\")\n",
    "        summaries[tag][method_name] = evaluate_run(trainer)\n",
    "\n",
    "for tag in summaries:\n",
    "    print(f\"\\n=== {tag.upper()} ===\")\n",
    "    for method_name in [\"Class-Aware\", \"Baseline\"]:\n",
    "        summary = summaries[tag].get(method_name, {})\n",
    "        print(f\"{method_name} metrics:\")\n",
    "        for key, value in summary.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "            elif value is None:\n",
    "                print(f\"  {key}: N/A\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ceefd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def safe_value(summary, key):\n",
    "    value = summary.get(key) if summary else None\n",
    "    if isinstance(value, (int, float, np.floating)):\n",
    "        return float(value)\n",
    "    return np.nan\n",
    "\n",
    "metrics_groups = {\n",
    "    \"ID Accuracy\": [\"id_accuracy\", \"many_acc\", \"med_acc\", \"few_acc\"],\n",
    "    \"OOD Detection\": [\"auroc\", \"aupr\", \"fpr@95tpr\"],\n",
    "}\n",
    "\n",
    "scenario_names = list(summaries.keys())\n",
    "methods = [\"Class-Aware\", \"Baseline\"]\n",
    "bar_width = 0.35\n",
    "\n",
    "for group_name, metric_keys in metrics_groups.items():\n",
    "    fig, axes = plt.subplots(\n",
    "        1,\n",
    "        len(metric_keys),\n",
    "        figsize=(4 * len(metric_keys), 4),\n",
    "        sharey=False,\n",
    "    )\n",
    "    if len(metric_keys) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, metric_key in zip(axes, metric_keys):\n",
    "        positions = np.arange(len(scenario_names))\n",
    "        for idx, method_name in enumerate(methods):\n",
    "            values = [\n",
    "                safe_value(summaries.get(scenario, {}).get(method_name, {}), metric_key)\n",
    "                for scenario in scenario_names\n",
    "            ]\n",
    "            offset = (idx - (len(methods) - 1) / 2) * bar_width\n",
    "            ax.bar(positions + offset, values, bar_width, label=method_name)\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(scenario_names, rotation=20)\n",
    "        ax.set_title(f\"{group_name}: {metric_key}\")\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "        ax.set_ylabel(metric_key)\n",
    "        ax.legend()\n",
    "    fig.suptitle(f\"Performance Comparison ({group_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {\n",
    "    tag: {\n",
    "        method_name: trainer.training_history\n",
    "        for method_name, trainer in trainer_group.items()\n",
    "    }\n",
    "    for tag, trainer_group in trainers.items()\n",
    "}\n",
    "\n",
    "if not any(\n",
    "    history\n",
    "    for scenario_histories in histories.values()\n",
    "    for history in scenario_histories.values()\n",
    "):\n",
    "    raise RuntimeError(\"Run training before plotting the training curves.\")\n",
    "\n",
    "def compute_epoch_positions(history):\n",
    "    positions = []\n",
    "    for entry in history:\n",
    "        total = entry.get(\"total_batches\", 1)\n",
    "        total = total if total > 0 else 1\n",
    "        frac = entry[\"batch\"] / total\n",
    "        positions.append(entry[\"epoch\"] - 1 + frac)\n",
    "    return np.array(positions, dtype=np.float32)\n",
    "\n",
    "for tag, scenario_histories in histories.items():\n",
    "    if not any(scenario_histories.values()):\n",
    "        print(f\"No training history available for {tag}; skipping plot.\")\n",
    "        continue\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "    metric_specs = [\n",
    "        (\"Training Loss\", \"loss\", \"Loss\"),\n",
    "        (\"Training Accuracy\", \"acc\", \"Accuracy (%)\"),\n",
    "    ]\n",
    "    for ax, (title, key, ylabel) in zip(axes, metric_specs):\n",
    "        for method_name, history in scenario_histories.items():\n",
    "            if not history:\n",
    "                continue\n",
    "            steps = compute_epoch_positions(history)\n",
    "            values = np.array([entry.get(key, np.nan) for entry in history], dtype=np.float32)\n",
    "            ax.plot(steps, values, label=method_name)\n",
    "        ax.set_title(f\"{title} ({tag})\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend()\n",
    "    fig.suptitle(f\"Training Curves: {tag}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb96b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mAUPR (ID positive): 0.8521\u001b[0m\n",
      "\u001b[37mAUPR (OOD positive): 0.9600\u001b[0m\n",
      "\u001b[37mAUPR (OOD positive): 0.9600\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "TARGET_SCENARIO = \"balanced\"\n",
    "TARGET_METHOD = \"Class-Aware\"\n",
    "\n",
    "target_trainer = trainers[TARGET_SCENARIO][TARGET_METHOD]\n",
    "\n",
    "if target_trainer.last_ood_scores is None:\n",
    "    raise RuntimeError(\n",
    "        \"Run evaluate_run() for the selected trainer before computing precision curves.\"\n",
    ")\n",
    "\n",
    "id_scores = np.asarray(target_trainer.last_ood_scores[\"id\"], dtype=np.float32)\n",
    "ood_scores = np.asarray(target_trainer.last_ood_scores[\"ood\"], dtype=np.float32)\n",
    "labels_id = np.concatenate([np.ones_like(id_scores), np.zeros_like(ood_scores)])\n",
    "scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "prec_id, rec_id, _ = precision_recall_curve(labels_id, scores)\n",
    "aupr_id = auc(rec_id, prec_id)\n",
    "\n",
    "labels_ood = 1 - labels_id\n",
    "prec_ood, rec_ood, _ = precision_recall_curve(labels_ood, -scores)\n",
    "aupr_ood = auc(rec_ood, prec_ood)\n",
    "\n",
    "print(f\"AUPR (ID positive): {aupr_id:.4f}\")\n",
    "print(f\"AUPR (OOD positive): {aupr_ood:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
